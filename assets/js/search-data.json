{
  
    
        "post0": {
            "title": "Reinforcement Learning. Deep Q-networks",
            "content": "Overview . In this post, we will introduce Deep Q-networks (DQN). . Deep Q-networks . Online Q-learning can experience instabilities during training. This is because by using experience sampled sequentially from the environment leads to highly correlated gradient steps. Deep Q-networks (DQN) made deep reinforcement learning a viable approach to complex sequential control problems. In this section, we introduce the vanilla DQN algorithm. . Deep q-networks were introduced in the seminal work of Mnih et al. in [2]. They demonstrated that a single DQN can achive human level performance in many Atari games without any feature engineerign. A DQN modifies online Q-learing in two ways [1, 2] . Introducing experience replay | Introducing target network | . Both these two features greatly stabilize the learning. In particular, a DQN stores the experience tuples $(s_i, alpha_i, r_i, s_{i, NEW})$ in an experience buffer. During training, the samples are drawn from the buffer uniformly. This approach eliminates the correlations between the samples used in training the neural network and gives i.i.d. samples. . The second feature that a DQN introduces is the target network. When bootstraping with function approximations, in a sense we create a moving target to learn from. Attempting to train a neural network via such a route is more likely bound to fail. The key idea is to create a copy of the neural network that is only used to generate the Q-value estimates used in sampled Bellman updates. That is the target value for sample $i$ is obtained as . $$y_i = r_i + gamma max_{ alpha_i} Q_{ theta_{TN}}(s_i, alpha_i)$$ . Note that in the update rule above, $ theta_{TN}$ denotes the parameters of a target network. These are updated every $C$ steps by setting them equal to the parameters $ theta$ i.e. $ theta_{TN} = theta$. Such an update rule, creates a lag in updating the target network which may make the action-value estimates that it generates a bit stale compared to the original network. What we gain, however, is that the target values become stable and the original network is trainable. . Loss function . In the DQN algorithm, we, typically, use the following loss function . $$L = begin{cases} left( Q(s, alpha) - (r + gamma max_{ alpha in mathbb{A}} hat{Q}(s_{NEW}, alpha) right)^2, text{if step is not at the end of the episode} left( Q(s, alpha) - r right)^2, text{otherwise} end{cases}$$ . The following section summarizes the DQN algorithm. . DQN algorithm . Let&#39;s now walk over the steps of the DQN algorithm as these are described in [1]. These are: . Initialize $ theta$ and the replay buffer with a fixed capacity. Set $ theta_{TN}= theta$ | Set the policy $ pi$ to be an $ epsilon-$greedy with respect to $q_{ theta}$ | Until some condition is met do . 3.1 Sample an action $ alpha$ from the policy $ pi$ . 3.2 Take the action $ alpha$ and observe $r$ and $s_{NEW}$. Add the transition $(s, alpha, r, s_{ NEW})$ in the replay buffer. If $|D|&gt;M$ eject the oldest transition from the buffer . 3.3 If the experience buffer has reached the indicated capacity, unfiromly sample a random minibatch of $N$ transitions from $D$ else return to 3.1 above. . 3.4 Obtain the target values $y_i = r_i + gamma max_{ alpha_i} q_{ theta_{TN}}(s_i, alpha_i)$. . 3.5 Take the gradient step to update $ theta$ . 3.6 Every $C$ steps update the target network parameters . | References . Enes Bilgin, Mastering Reinforcement Learning with Python. Build next-generation, self-learning models using reinforcement learning techniques and best practices. | Mnih V. et al. Human level control through deep reinforcement learning, Nature, v. 518, pp. 529-533, 2015 | Maxim Lapan, Deep Reinforcement Learning Hands-on, Packt |",
            "url": "https://pockerman.github.io/qubit_opus/reinforcement-learning/deep-reinforcement-learning/dqn/python/pytorch/2021/10/18/rl-dqn.html",
            "relUrl": "/reinforcement-learning/deep-reinforcement-learning/dqn/python/pytorch/2021/10/18/rl-dqn.html",
            "date": " • Oct 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "C++ Programming. Using the auto keyword",
            "content": "Overview . The auto keyword changed its semantics starting from the C++11 standard. In this notebook, we will review its new semantics and examine its new flavors. . Using the auto keyword . The auto keyword changed its semantics starting from the C++11 standard. From C++11 onwards, the semantic of the keyword is automatic type deduction. In fact, from C++11 onwards, we have the following different flavors of the keyword [1]: . auto | const auto&amp; | auto&amp; | auto&amp;&amp; | . Furthermore, we have decltype(auto). This post is a short guide on how to use auto and its various flavors. . Automatic type deduction . The first thing to note about auto is that it ise used for automatic type deduction. This means that we can write code like the following: . ... // before c++11 int x = 5; //from c++11 auto x = 5; ... . Thus, using auto may help us to write cleaner and less cluttered code. This is emphasized particularly when we consider function signatures. Compare the code snippet below (example taken from [1]): . class Foo { int value()const{..} const int&amp; cref_value()const{...} int&amp; ref_value(){...} }; . with the following code snippet . class Foo { auto value()const{..} auto&amp; cref_value()const{...} auto&amp; ref_value(){...} }; . The latter API is obviously cleaner and simpler.The type returned is deduced by the compiler for us. . . Remark . Althgough to a large extent using auto simplifies our code, overusing it can have the opposite result. . . One other advantage of using auto is that we cannot leave the variable uninitialized. That is the following fails to compile. . ... auto x; ... . This is reasonable as the compiler uses the right hand side value to deduce the type and hence the memory size it has to allocate. If there isn&#39;t a value there is nothing to deduce from. Thus, uninitialized variables are not allowed when using auto and the good news are that the compilers let us know the exact line number in our code that this occurs. . const reference . const auto&amp; has the ability to bind to anything [1]. The original object cannot be mutated via such a reference. Note that if the const reference is bound to a temporary object, the lifetime of the temporary will be extended to the lifetime of the reference [1]. . Although we may be using auto&amp; , it is possible that we end up with a const reference. For example . auto foo = Foo{}; auto&amp; cref = cref_value(); . We should however strive to be more explicit and for such cases simply use const auto&amp; and use auto&amp; to only denote mutable references [1]. . Forwarding reference . Similar to auto&amp;, auto&amp;&amp; can bind to anything. auto&amp;&amp; is called a forwardin or universal reference [1]. Similar to const auto&amp;, auto&amp;&amp; extends the lifetime of a temporary. However, in contrast to const auto&amp;, auto&amp;&amp; allows us to mutate the objects it references including the temporaries. . . Remark . Note that auto&amp;&amp; and T&amp;&amp; are interpreted as forwarding references only when used in a function template where T is a template parameter of that function. However, using &amp;&amp; with an explicit type e.g. std::vector&lt;double&gt;&amp;&amp; denotes an rvalue reference and does not have the properties of a forwarding reference [1]. . . References . Bjorn Andrist, Viktor Sehr, C++ High Performance, 2nd Edition, Packt Publishing |",
            "url": "https://pockerman.github.io/qubit_opus/programming/c++/2021/10/17/cpp-program-using-auto.html",
            "relUrl": "/programming/c++/2021/10/17/cpp-program-using-auto.html",
            "date": " • Oct 17, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "PyTorch with C++. Linear regression model",
            "content": "Overview . Linear regression model . #include &lt;torch/torch.h&gt; #include &lt;iostream&gt; #include &lt;vector&gt; namespace example { typedef std::size_t uint_t; class Net: public torch::nn::Module { public: // Net(uint_t input_size, uint_t output_size); // forward torch::Tensor forward(torch::Tensor input); private: torch::nn::Linear linear; torch::Tensor bias_; }; Net::Net(uint_t input_size, uint_t output_size) : linear(register_module(&quot;linear&quot;, torch::nn::Linear(input_size, output_size))), bias_() { bias_ = register_parameter(&quot;b&quot;, torch::randn(output_size)); } torch::Tensor Net::forward(torch::Tensor input){ return linear(input) + bias_; } } int main() { using namespace example; if(torch::cuda::is_available()){ std::cout&lt;&lt;&quot;CUDA is available on this machine&quot;&lt;&lt;std::endl; } else{ std::cout&lt;&lt;&quot;CUDA is not available on this machine&quot;&lt;&lt;std::endl; } // create data std::vector&lt;double&gt; x_train(11, 0.0); std::vector&lt;double&gt; y_train(11, 0.0); for(uint_t i=0; i&lt;x_train.size(); ++i){ x_train[i] = static_cast&lt;double&gt;(i); y_train[i] = 2*static_cast&lt;double&gt;(i) + 1; } auto x_tensor = torch::from_blob(x_train.data(), {int(y_train.size()), int(x_train.size()/y_train.size())}); auto y_tensor = torch::from_blob(y_train.data(), {int(y_train.size()), 1}); Net net(1, 1); for (const auto&amp; p : net.parameters()) { std::cout &lt;&lt; p &lt;&lt; std::endl; } torch::nn::MSELoss mse; torch::optim::SGD sgd(net.parameters(), 0.01); for(uint_t e=0; e&lt;100; ++e){ sgd.zero_grad(); auto outputs = net.forward(x_tensor); auto loss = mse(outputs, y_tensor); // get gradients w.r.t to parameters loss.backward(); // update parameters sgd.step(); std::cout&lt;&lt;&quot;Epoch=&quot;&lt;&lt;e&lt;&lt;&quot; loss=&quot;&lt;&lt;loss&lt;&lt;std::endl; } return 0; } . CUDA is not available on this machine 0.01 * 1.7510 [ CPUFloatType{1} ] -0.4187 [ CPUFloatType{1,1} ] 0.01 * -1.6082 [ CPUFloatType{1} ] Epoch=0 loss=4.09091 [ CPUFloatType{} ] Epoch=1 loss=3.72361 [ CPUFloatType{} ] Epoch=2 loss=3.39557 [ CPUFloatType{} ] Epoch=3 loss=3.10247 [ CPUFloatType{} ] Epoch=4 loss=2.84047 [ CPUFloatType{} ] Epoch=5 loss=2.60615 [ CPUFloatType{} ] Epoch=6 loss=2.39647 [ CPUFloatType{} ] Epoch=7 loss=2.20874 [ CPUFloatType{} ] Epoch=8 loss=2.04054 [ CPUFloatType{} ] Epoch=9 loss=1.88976 [ CPUFloatType{} ] Epoch=10 loss=1.75448 [ CPUFloatType{} ] Epoch=11 loss=1.63302 [ CPUFloatType{} ] Epoch=12 loss=1.52387 [ CPUFloatType{} ] Epoch=13 loss=1.42571 [ CPUFloatType{} ] ... . References .",
            "url": "https://pockerman.github.io/qubit_opus/pytorch/c++/api/machine-learning/linear-regression/2021/10/15/pytorch_cpp_linear_regression.html",
            "relUrl": "/pytorch/c++/api/machine-learning/linear-regression/2021/10/15/pytorch_cpp_linear_regression.html",
            "date": " • Oct 15, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Apache Spark. Create a  DataFrame with Scala",
            "content": "Overview . In this post, we will review how to create a DataFrame in Spark using the Scala API. . Create a DataFrame with Scala . The Spark DataFrame is inspired by the equivalent pandas DataFrame. A DataFrame is Spark is like a distributed in-memory table with named columns and schemas [1]. Similar to an RDD, a DataFrame is also immutable and Spark keeps a lineage of all the transformations. Thus, when we add or change the names and types of the columns, a new DataFrame is actually created. . package train.spark import org.apache.spark.sql.SparkSession object CreateDataFrame { def main(args: Array[String]) { val spark = SparkSession .builder() .appName(&quot;Spark DataFrame Demo&quot;) .getOrCreate() val csvFile = &quot;/home/alex/qi3/learn_scala/scripts/spark/data/train.csv&quot; val df = spark.read.csv(csvFile) // print the schema df.printSchema() } } . The output is . 21/10/15 15:22:10 WARN Utils: Your hostname, LT-2R0620-101 resolves to a loopback address: 127.0.1.1; using 192.168.0.71 instead (on interface wlp58s0) 21/10/15 15:22:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/alex/MySoftware/spark-3.0.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int) WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release 21/10/15 15:22:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable root |-- _c0: string (nullable = true) |-- _c1: string (nullable = true) |-- _c2: string (nullable = true) |-- _c3: string (nullable = true) |-- _c4: string (nullable = true) . Specify a schema . package train.spark import org.apache.spark.sql.SparkSession import org.apache.spark.sql.SQLContext import org.apache.spark.sql.types._ import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf object CreateDataFrame { def main(args: Array[String]) { val csvFile = &quot;/home/alex/qi3/learn_scala/scripts/spark/data/train.csv&quot; val appName: String = &quot;Spark DataFrame Demo&quot; val conf = new SparkConf().setAppName(appName) val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val customSchema = StructType(Array( StructField(&quot;mu-1&quot;, DoubleType, false), StructField(&quot;mu-2&quot;, DoubleType, false), StructField(&quot;label&quot;, IntegerType, false))) // specify a schema val df_schema = sqlContext.read.format(&quot;csv&quot;) .option(&quot;delimiter&quot;,&quot;,&quot;) .schema(customSchema) .load(csvFile) df_schema.printSchema() df_schema.groupBy(&quot;label&quot;).count().show() df_schema.show(5) } } . root |-- mu-1: double (nullable = true) |-- mu-2: double (nullable = true) |-- label: integer (nullable = true) +--+--+ |label|count| +--+--+ | null| 1| | 1| 185| | 3| 185| | 4| 185| | 2| 185| | 0| 185| +--+--+ +--+--+--+ | mu-1| mu-2|label| +--+--+--+ | null| null| null| |22.91|28.54| 0| |17.26|30.72| 0| |17.05|31.08| 0| |24.05|26.27| 0| +--+--+--+ only showing top 5 rows . References . Jules S. Damji, Brooke Wenig, Tathagata Das, Denny Lee, Learning Spark. Lighting-fast data analytics, O&#39;Reilly, 2nd Edition. |",
            "url": "https://pockerman.github.io/qubit_opus/spark/scala/big-data/data-engineering/data-analysis/2021/10/15/create-dataframe-spark-scala.html",
            "relUrl": "/spark/scala/big-data/data-engineering/data-analysis/2021/10/15/create-dataframe-spark-scala.html",
            "date": " • Oct 15, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Apache Spark. RDD operations",
            "content": "Overview . In this post, we will review the allowed operations on an RDD. . Acknowledgements . The content of this notebooks is to a large extent edited from [1]. . RDD operations with Scala . A Spark RDD provides two types of operations . Transformations | Actions | . Transformations . A transformation operation creates a new RDD from an existing RDD. Moreover, we can apply a chain of transformations once the data is loaded into memory. Below are some examples of transformations we can apply on an RDD. . map(function): It returns a new data set by operating on each element of the source RDD. | flatMap(function): Similar to map, but each item can be mapped to zero, one, or more items. | mapPartitions(function): Similar to map, but works on the partition level. | mapPartitionsWithIndex(function): Similar to mapPartitions, but provides a function with an Int value to indicate the index position of the partition. . | filter(function): It returns a new RDD that contains only elements that satisfy the predicate. . | union(otherDataset): It returns a new data set that contains the elements of the source RDD and the otherDataset RDD. Note that the participating RDDs should be of the same data type. . | intersection(otherDataset): It returns a new data set that contains the intersection of elements from the source RDD and the argument RDD. . | . Spark transformations are lazy evaluated. What this means that a transformation is applied only when an action is called. Examples of actions are collect and count . Actions . Transformations in Spark are lazy evaluated. What this means that a transformation is applied only when an action is called. Let&#39;s see some examples of actions. . collect(): Returns all the elements of the data set are returned as an array to the driver program. | count(): Returns the number of elements in the data set. | reduce(function): It returns a data set by aggregating the elements of the RDD it is applied on. The aggregation is done by using the user provided function argument. The function should take two arguments and returns a single argument. Moreover it should be commutative and associative so that it can be operated in parallel. . | first(): Returns the first element in the data set. . | take(n): Returns the first n elements in the data set as an array. | takeOrdered(n, [ordering]): Return the first n elements of the RDD using either their natural order or a custom comparator. | takeSample(withReplacement, num, [seed]): Returns an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed. | saveAsTextFile(path): Write the elements of the RDD as a text file in the local file system, HDFS, or any another supported storage system. | foreach(function): Applies the function argument on each element in the RDD. | . References . Subhashini Chellappan, Dharanitharan Ganesan, Practical Apache Spark. Using the Scala API, Apress |",
            "url": "https://pockerman.github.io/qubit_opus/spark/scala/api/data-analysis/big-data/2021/10/01/rdd-ops-scala.html",
            "relUrl": "/spark/scala/api/data-analysis/big-data/2021/10/01/rdd-ops-scala.html",
            "date": " • Oct 1, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Machine learning notes. Hyperparameter tuning",
            "content": "Overview . Machine learning models typically involve a parameter set that it is learnt during the training process. However, machine learning models also contain parameters that are not learnable and must be specified before the training process begins. In this notebook, we will review some commonly used methods to establish good hyperparameters values. In particular, we will review the following approaches . Grid search | Random search | Informed search | . Hyperparameter tuning . Machine learning models typically consist of a set of parameters that define their learnability. It is these parameters that somehow we try to estimate during training. However, most machine learning models also contain parameters that we need to establish before training begins. These parameters affect the learning process but they are not learnable themselves. We call these parameters as hyperparameters. . Hyperparameters are typically set before the modeling process begins. For example the number of clusters is a hyperparameter that the application needs to establish before running the algorithm. Thus, the crucial elemet that distinguishes parameters from hyperparameters is that the former are learnt by the model whilst the latter are set by the application. . In the sequel, we will differentiate hyperparameters into two categories. Namely, parameters that affect the model performance and parameters that do not. an example of the latter category is the number of cpu cores that we want to use when training the model. Although, this impacts the training time, it does not impact how the model performs on unseen data or in other words the model&#39;s performance. . Setting and hyperparameter values . Now that we have the needed definitions out of the way, we turn our attention to the main topic of this notebook. Namely, how do we set the optimal values for the model hyperparameters. Unfortunately, there is not a clear answer to this question. Hyperparameters are specific to each algorithm. However, there are available some general guidelines and tips that we can follow. Let&#39;s review some of the top tips. . First, we need to identify which hyperparameters values are in conflict. For example, if we are using sklearn&#39;s logistic regression model, the solver and penalty parameters have options that may be in conflict; the elasticnet penalty is only supported by the saga solver. . Another point to be aware of is that some hyperparameter values are simply silly. For example, setting the number of clusters equal to one when performing K-means clustering or equal to the number of points in the dataset, does not sound very meaningful. Similarly, setting the number of neighbors in a kNN algorithm equal to one is not very wise. . Below, we will review the following methods for hyperparamter tuning . Grid search | Random search | Informed search | . Let&#39;s start with grid search. . Grid search . Grid search performs an exhaustive search over a specified parameter values for an estimator. We can visualize this as a two dimensional grid. At each point of the grid, a different combination of parameters is examined. For example, consider an artificial model with three hyperparamters $a, b$ and $c$. Each of these parameters has the following values sets; $a in [a_1, a_2, a_3], b in [b_1, b_2], c in [c_1, c_2, c_3]$. Grid search performs exhaustive search by forming all the possible triplets and fitting the model using the identified values. Let&#39;s see how to perform grid search in sklearn. Overall the steps of using grid search in sklearn are as follows . Choose the algorithm to tune the hyperparameters (estimator) | Define which hyperparameters to tune (param_grid) | Define the range of values for each hyperparameter | Decide of the cross-validation scheme to use (cv) | Define the score function to be used when deciding which model is the best (scoring) | . The following example, taken from here, shows how to use grid search . from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.metrics import classification_report from sklearn.svm import SVC . digits = datasets.load_digits() # To apply an classifier on this data, we need to flatten the image, to # turn the data in a (samples, feature) matrix: n_samples = len(digits.images) X = digits.images.reshape((n_samples, -1)) y = digits.target # Split the dataset in two equal parts X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.5, random_state=0) . tuned_parameters = [{&#39;kernel&#39;: [&#39;rbf&#39;], &#39;gamma&#39;: [1e-3, 1e-4], &#39;C&#39;: [1, 10, 100, 1000]}, {&#39;kernel&#39;: [&#39;linear&#39;], &#39;C&#39;: [1, 10, 100, 1000]},] . clf = GridSearchCV(estimator=SVC(), param_grid=tuned_parameters, scoring=&#39;precision_macro&#39;) clf.fit(X_train, y_train) . GridSearchCV(estimator=SVC(), param_grid=[{&#39;C&#39;: [1, 10, 100, 1000], &#39;gamma&#39;: [0.001, 0.0001], &#39;kernel&#39;: [&#39;rbf&#39;]}, {&#39;C&#39;: [1, 10, 100, 1000], &#39;kernel&#39;: [&#39;linear&#39;]}], scoring=&#39;precision_macro&#39;) . print(&quot;Best parameters set found on development set:&quot;) print() print(clf.best_params_) print() print(&quot;Grid scores on development set:&quot;) print() means = clf.cv_results_[&#39;mean_test_score&#39;] stds = clf.cv_results_[&#39;std_test_score&#39;] for mean, std, params in zip(means, stds, clf.cv_results_[&#39;params&#39;]): print(&quot;%0.3f (+/-%0.03f) for %r&quot; % (mean, std * 2, params)) print() print(&quot;Detailed classification report:&quot;) print() print(&quot;The model is trained on the full development set.&quot;) print(&quot;The scores are computed on the full evaluation set.&quot;) print() y_true, y_pred = y_test, clf.predict(X_test) print(classification_report(y_true, y_pred)) print() . Best parameters set found on development set: {&#39;C&#39;: 10, &#39;gamma&#39;: 0.001, &#39;kernel&#39;: &#39;rbf&#39;} Grid scores on development set: 0.986 (+/-0.016) for {&#39;C&#39;: 1, &#39;gamma&#39;: 0.001, &#39;kernel&#39;: &#39;rbf&#39;} 0.959 (+/-0.028) for {&#39;C&#39;: 1, &#39;gamma&#39;: 0.0001, &#39;kernel&#39;: &#39;rbf&#39;} 0.988 (+/-0.017) for {&#39;C&#39;: 10, &#39;gamma&#39;: 0.001, &#39;kernel&#39;: &#39;rbf&#39;} 0.982 (+/-0.026) for {&#39;C&#39;: 10, &#39;gamma&#39;: 0.0001, &#39;kernel&#39;: &#39;rbf&#39;} 0.988 (+/-0.017) for {&#39;C&#39;: 100, &#39;gamma&#39;: 0.001, &#39;kernel&#39;: &#39;rbf&#39;} 0.983 (+/-0.026) for {&#39;C&#39;: 100, &#39;gamma&#39;: 0.0001, &#39;kernel&#39;: &#39;rbf&#39;} 0.988 (+/-0.017) for {&#39;C&#39;: 1000, &#39;gamma&#39;: 0.001, &#39;kernel&#39;: &#39;rbf&#39;} 0.983 (+/-0.026) for {&#39;C&#39;: 1000, &#39;gamma&#39;: 0.0001, &#39;kernel&#39;: &#39;rbf&#39;} 0.974 (+/-0.012) for {&#39;C&#39;: 1, &#39;kernel&#39;: &#39;linear&#39;} 0.974 (+/-0.012) for {&#39;C&#39;: 10, &#39;kernel&#39;: &#39;linear&#39;} 0.974 (+/-0.012) for {&#39;C&#39;: 100, &#39;kernel&#39;: &#39;linear&#39;} 0.974 (+/-0.012) for {&#39;C&#39;: 1000, &#39;kernel&#39;: &#39;linear&#39;} Detailed classification report: The model is trained on the full development set. The scores are computed on the full evaluation set. precision recall f1-score support 0 1.00 1.00 1.00 89 1 0.97 1.00 0.98 90 2 0.99 0.98 0.98 92 3 1.00 0.99 0.99 93 4 1.00 1.00 1.00 76 5 0.99 0.98 0.99 108 6 0.99 1.00 0.99 89 7 0.99 1.00 0.99 78 8 1.00 0.98 0.99 92 9 0.99 0.99 0.99 92 accuracy 0.99 899 macro avg 0.99 0.99 0.99 899 weighted avg 0.99 0.99 0.99 899 . The output from GridSearchCV can be categorized into three different groups . Results log: cv_results_ | Best results: best_index_, best_params_ and best_score_ | Other extra information such as refit_time_, and scorer_ | . Random search . The next method we will review is random search. According to wikipedia, random search (RS) is a family of numerical optimization methods that do not require the gradient of the problem to be optimized, and RS can hence be used on functions that are not continuous or differentiable. Such optimization methods are also known as direct-search, derivative-free, or black-box methods. . In general, random search is similar to the grid search approach we reviewed above. In particular, . We have to define an estimator | The parameters to be tuned and their range of values | Establish a cross-validation scheme | Establish a scoring functon | . We won&#39;t go into the details of why random search works. Instead let&#39;s see how to perform random search with scikit-learn. The following example, is a copy verbatim from scikit-learn . from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.model_selection import RandomizedSearchCV from scipy.stats import uniform . iris = load_iris() logistic = LogisticRegression(solver=&#39;saga&#39;, tol=1e-2, max_iter=2, random_state=0) distributions = dict(C=uniform(loc=0, scale=4), penalty=[&#39;l2&#39;, &#39;l1&#39;]) clf = RandomizedSearchCV(logistic, distributions, random_state=0) search = clf.fit(iris.data, iris.target) . /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; . search.best_params_ . {&#39;C&#39;: 2.195254015709299, &#39;penalty&#39;: &#39;l1&#39;} . Grid search performs exhaustive search by trying out all possible combinations. On the other hand, random search selects a subset of combinations. Given this, it requires that we establish a sampling methodology. Grid search is more computationally expensive than random search. However, it is guaranteed to find the best score in the sample space. Random search is not guaranteed to find the best score, but it is likely to find a good one faster than grid search. . Informed search . Both grid and random search algorithms are uninformed search algorithms. What this means is that algorithms do not use any form of information in order to improve the searching. In this section, we will review some informed search algorithms . Coarse to fine tuning . In this approach, we start with a rough estimate and iteratively we refine our search.This approach utilizes both grid and random search. Here are the general steps we can follow towards this direction. . Start with random search | Find the areas in the sampling space that look promising | Do a grid search in these smaller areas | Continue until a good or optimal score is achieved | Other approaches to informed search include Bayesian statistics and genetic algorithms. However, we won&#39;t go into these directions as they can easily form the subject of whole books on their own. You can check the article A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning to get a conceptual view about the subject. . References . Hyperparameter tuning in Python course from Datacamp | Hyperparameter optimization | Random search |",
            "url": "https://pockerman.github.io/qubit_opus/machine-learning/hyperparameters/sklearn/grid-search/random-search/informed-search/2021/09/20/mln-hyperparameter-tuning.html",
            "relUrl": "/machine-learning/hyperparameters/sklearn/grid-search/random-search/informed-search/2021/09/20/mln-hyperparameter-tuning.html",
            "date": " • Sep 20, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "PyTorch with C++ 1",
            "content": "Overview . PyTorch is one of the well established libraries for modeling deep neural networks. The exposed Python API is the most commonly used one. However, the library also exposes bindings for C++. In this series of notebooks, I will try to demonstrate how to use the latter. I will be following to a large extent the documentation for the C++ frontend. . PyTorch with C++ 1 . I will start by performing some basic manipulations with the Tensor class. Instances of this class are used around the library to perform numerics. Whilst at this I will also show how to link against the C++ bindings. . Install and link against libtorch . To start with, download the binaries from here: https://pytorch.org/get-started/locally/. I use the pre-build binaries. Also make sure that you download the C++11 ABI. . Let&#39;s create now the first C++ PyTorch program. The following program is taken from the official documentation. . #include &lt;torch/torch.h&gt; #include &lt;iostream&gt; int main() { torch::Tensor tensor = torch::eye(3); std::cout &lt;&lt; tensor &lt;&lt; std::endl; return 0; } . I use the following CMakeLists.txt file to generate the needed Makefile . cmake_minimum_required(VERSION 3.0 FATAL_ERROR) PROJECT(example_1 VERSION 1.0.0 LANGUAGES CXX) SET(SOURCE example_1.cpp) SET(EXECUTABLE example_1) # default optionsSET(BUILD_SHARED_LIBS ON) SET(CMAKE_BUILD_TYPE &quot;Debug&quot;) SET(CMAKE_CXX_COMPILER g++) SET(CMAKE_CXX_STANDARD 20) SET(CMAKE_CXX_STANDARD_REQUIRED True) SET(CMAKE_C_COMPILER gcc) SET(CMAKE_LINKER_FLAGS &quot;-pthread&quot;) LIST(APPEND CMAKE_PREFIX_PATH /home/alex/MySoftware/libtorch) FIND_PACKAGE(Torch REQUIRED CONFIG) MESSAGE(STATUS &quot;TORCH Include directory ${TORCH_INCLUDE_DIRS}&quot;) MESSAGE(STATUS &quot;Build type: ${CMAKE_BUILD_TYPE}&quot;) MESSAGE(STATUS &quot;C++ Compiler: ${CMAKE_CXX_COMPILER}&quot;) MESSAGE(STATUS &quot;C Compiler: ${CMAKE_C_COMPILER}&quot;) INCLUDE_DIRECTORIES(${TORCH_INCLUDE_DIRS}) ADD_EXECUTABLE(${EXECUTABLE} ${SOURCE}) TARGET_LINK_LIBRARIES(${EXECUTABLE} ${TORCH_LIBRARIES}) . The program above produces the following output when built and executed: . 1 0 0 0 1 0 0 0 1 [ CPUFloatType{3,3} ] . I will now extend the example above to check on the provided API. Still, this is very elementary. Here is the updated example . #include &lt;torch/torch.h&gt; #include &lt;iostream&gt; #include &lt;vector&gt; int main() { if(torch::cuda::is_available()){ std::cout&lt;&lt;&quot;CUDA is available on this machine&quot;&lt;&lt;std::endl; } else{ std::cout&lt;&lt;&quot;CUDA is not available on this machine&quot;&lt;&lt;std::endl; } torch::Tensor tensor = torch::eye(3); std::cout &lt;&lt; tensor &lt;&lt; std::endl; std::vector&lt;double&gt; data(3, 2.0); auto tensor_from_data_1 = torch::tensor(data); std::cout &lt;&lt; tensor_from_data_1 &lt;&lt; std::endl; data[0] = data[1] = data[2] = 1.0; auto tensor_from_data_2 = torch::tensor(data); std::cout &lt;&lt; tensor_from_data_2 &lt;&lt; std::endl; auto sum = tensor_from_data_2 + tensor_from_data_1; std::cout &lt;&lt; sum &lt;&lt; std::endl; if(torch::cuda::is_available()){ // create a tensor and send it to the GPU auto cuda_tensor = torch::tensor({1.0, 2.0, 3.0}).to(&quot;cuda&quot;); } // compute element-wise product auto tensor1 = torch::tensor({1.0, 2.0, 3.0}); auto product = tensor1 * tensor1; std::cout &lt;&lt; product &lt;&lt; std::endl; return 0; } . The output of the program above is shown below . CUDA is not available on this machine 1 0 0 0 1 0 0 0 1 [ CPUFloatType{3,3} ] 2 2 2 [ CPUFloatType{3} ] 1 1 1 [ CPUFloatType{3} ] 3 3 3 [ CPUFloatType{3} ] 1 4 9 [ CPUFloatType{3} ] . The driver code above can be found in this github repository. . References . PyTorch | Using the PyTorch C++ Frontend | .",
            "url": "https://pockerman.github.io/qubit_opus/pytorch/deep-neural-networks/api/c++/numerics/2021/08/29/pytorch-with-cpp-1.html",
            "relUrl": "/pytorch/deep-neural-networks/api/c++/numerics/2021/08/29/pytorch-with-cpp-1.html",
            "date": " • Aug 29, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Machine learning with Scala Spark linear regression",
            "content": "Overview . In a previous post I developed a trivial Scala application that performs linear regression with only one feature. In this post, I want to go a bit further, I want to use Spark&#39;s MLlib to develop a linear regression model using two features this time. . Machine learning with Scala Spark linear regression . The first thing I need to do in order to use MLlib in my Scala application is to update the dependencies in the build.sbt script. These should now look as . libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-core_2.12&quot; % &quot;3.0.1&quot; libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-sql_2.12&quot; % &quot;3.0.1&quot; libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-mllib_2.12&quot; % &quot;3.0.1&quot; . package train.spark import org.apache.spark.ml.regression.LinearRegression import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.apache.spark.ml.feature.VectorAssembler import org.apache.spark.ml.linalg.Vectors import org.apache.spark.sql.types.DoubleType object LinearRegressionApp { def main(args: Array[String]) { val conf = new SparkConf().setAppName(&quot;Linear regression Spark&quot;) val sc = new SparkContext(conf) val session = SparkSession.builder().appName(&quot;Linear regression Spark&quot;).master(&quot;local[4]&quot;).getOrCreate() // Should be some file on your system val csvFile = &quot;/home/alex/qi3/spark_scala/data/spark_regression.csv&quot; val inputTrainigSet = session.read.format(&quot;csv&quot;).load(csvFile) println(&quot;Number of Partitions: &quot;+inputTrainigSet.rdd.getNumPartitions) println(&quot;Action: First element: &quot;+inputTrainigSet.rdd.first()) val analysisData = inputTrainigSet.withColumn(&quot;x1&quot;, inputTrainigSet(&quot;_c0&quot;).cast(DoubleType)) .withColumn(&quot;x2&quot;, inputTrainigSet(&quot;_c1&quot;).cast(DoubleType)) .withColumn(&quot;y&quot;, inputTrainigSet(&quot;_c2&quot;).cast(DoubleType)) .drop(&quot;_c0&quot;) .drop(&quot;_c1&quot;) .drop(&quot;_c2&quot;) //creating features column val assembler = new VectorAssembler() .setInputCols(Array(&quot;x1&quot;,&quot;x2&quot;)) .setOutputCol(&quot;features&quot;) // create the model val lr = new LinearRegression() .setMaxIter(10) .setRegParam(0.3) .setElasticNetParam(0.8) .setFeaturesCol(&quot;features&quot;) .setLabelCol(&quot;y&quot;) val trainigSet = assembler.transform(analysisData) // Fit the model val lrModel = lr.fit(trainigSet) // Print the coefficients and intercept for linear regression println(s&quot;Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}&quot;) // Summarize the model over the training set and print out some metrics val trainingSummary = lrModel.summary println(s&quot;numIterations: ${trainingSummary.totalIterations}&quot;) // there is sth wrong with my scala/spark version and this // throws an excpetion //println(s&quot;objectiveHistory: [${trainingSummary.objectiveHistory.mkString(&quot;,&quot;)}]&quot;) trainingSummary.residuals.show() println(s&quot;RMSE: ${trainingSummary.rootMeanSquaredError}&quot;) println(s&quot;r2: ${trainingSummary.r2}&quot;) } } . 21/08/25 12:36:15 WARN Utils: Your hostname, LT-2R0620-101 resolves to a loopback address: 127.0.1.1; using 192.168.0.71 instead (on interface wlp58s0) 21/08/25 12:36:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/alex/MySoftware/spark-3.0.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int) WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release 21/08/25 12:36:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 21/08/25 12:36:17 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect. Number of Partitions: 1 Action: First element: [0.0,4.0,4.357400305044133] 21/08/25 12:36:22 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS 21/08/25 12:36:22 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS Coefficients: [1.2545846367230242,0.7527507820338242] Intercept: 1.305736977601481 numIterations: 3 +--+ | residuals| +--+ | 0.04066019930735543| | -0.6631570819021908| | 0.8844468485401586| |-0.27725408848247746| | 1.523792089069631| | 0.9081058052618962| | 0.6154843963633212| | -1.5426210882366824| | -1.116750516169644| | -0.5438006575317718| |-0.41191237820348237| |-0.10423573938951769| | -0.7720329729420263| | -0.5175509972153742| | 0.5066514385552212| | 0.28386941829179424| | -1.7266735995448794| | -0.7963013580643907| | -0.8306208671329927| | -0.7913153349720496| +--+ only showing top 20 rows RMSE: 1.0241722775198268 r2: 0.8486882566011 .",
            "url": "https://pockerman.github.io/qubit_opus/spark/scala/api/data-analysis/machine-learning/2021/08/25/ml-scala-spark-linear-regression.html",
            "relUrl": "/spark/scala/api/data-analysis/machine-learning/2021/08/25/ml-scala-spark-linear-regression.html",
            "date": " • Aug 25, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Apache Spark. Create an RDD with Scala",
            "content": "Overview . In this post I want to review how to create Spark RDDs within a Scala applications. There are various methods to do so. The following example is taken for Spark by {Examples}. . Create Spark RDD with Scala . A Resilient Distributed Datasets, or RDD for short, is the fundamental data structure of Spark. An RDD is an immutable collection of objects that can be distributed across a cluster of computers. An RDD collection is divided into a number of partitions so that each node on a Spark cluster can independently perform computations. Note that modern Spark applications will most likely be using DataFrame and DataSet objects which are data structures on top of RDD. This means that RDDs are rather low level and if possible we should avoid it. . Anyway, there are two main methods available in Spark to create an RDD: . SparkContext.parallelize method | Read from a file | . The first method is illustrated in the example below . package train.spark import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf object CreateRDD { def main(args: Array[String]) { val conf = new SparkConf().setAppName(&quot;Hello Scala Spark&quot;) val sc = new SparkContext(conf) val data = Array(1,2,3,4,5,6,7,8,9,10) val rdd = sc.parallelize(data) rdd.foreach(println) println(&quot;Number of Partitions: &quot;+rdd.getNumPartitions) println(&quot;Action: First element: &quot;+rdd.first()) } } . Running the application produces something like the following . 3 6 1 8 9 2 7 4 5 10 Number of Partitions: 4 Action: First element: 1 . Note the the output may be different as it depends on which thread is accessing the standard output first. . The second method is to read a file from disk. This is also shown in the snippet below. . package train.spark import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf object CreateRDDFile { def main(args: Array[String]) { val conf = new SparkConf().setAppName(&quot;Hello Scala Spark&quot;) val sc = new SparkContext(conf) // Should be some file on your system val csvFile = &quot;/home/alex/qi3/learn_scala/scripts/spark/data/train.csv&quot; val csvRDD = sc.textFile(csvFile) println(&quot;Number of Partitions: &quot;+csvRDD.getNumPartitions) println(&quot;Action: First element: &quot;+csvRDD.first()) } } . Upon executing this code, I get . Number of Partitions: 2 Action: First element: #Duplicate: 0, Delete: 1, Normal-1: 2, TUF: 3, Normal-2: 4 . However, I am interested in converting the contents of the file into floating point numbers so that I can feed them to a machine learning algorithm. I can do this as follows. I can use the map() function to convert the RDD[String] into an RDD[Array[Double]] . val doubleRDD = csvRDD.map(line =&gt; {line.split(&quot;,&quot;)}) .map( arrString =&gt; {Try(Array(arrString(0).toDouble, arrString(1).toDouble, arrString(2).toDouble))}) .map(_ match {case Success(res) =&gt; res case Failure(res) =&gt; Array(-100, -100, -100)}) . We can use a schema in order to let Spark know the type of the data but this requires that we use a DataFrame instead and not an RDD. . Note also that Spark divides by default data into two partitions and distributes them across a cluster. The number of partitions can be specified while creating an RDD as shown below. . // Should be some file on your system val csvFile = &quot;/home/alex/qi3/learn_scala/scripts/spark/data/train.csv&quot; // use four partitions for the data val csvRDD = sc.textFile(csvFile, 4) . References .",
            "url": "https://pockerman.github.io/qubit_opus/spark/scala/big-data/data-engineering/data-analysis/2021/08/19/spark-rdd-scala.html",
            "relUrl": "/spark/scala/big-data/data-engineering/data-analysis/2021/08/19/spark-rdd-scala.html",
            "date": " • Aug 19, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Apache Spark with Scala",
            "content": "Overview . In this post, I describe the steps you need to take in order to submit a Scala application to be executed from Spark. The official documentation can be found here. . Apache Spark with Scala . I will assume that the environment is already set. Meaning, Scala is already installed, in this post I use version 2.13.3, Spark is also installed, for this post I use version 3.0.1. Finally, I am using SBT for building and packaging the application. . The Scala application simply informs us about the version of Spark we are using, the name of the master node and whether we run in local or distributed mode. It is shown below . //HelloSpark.scala package spark import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf object HelloSpark { def main(args: Array[String]) { val conf = new SparkConf().setAppName(&quot;Hello Scala Spark&quot;) val sc = new SparkContext(conf) println(&quot;Spark version: &quot; + sc.version) println(&quot;Spark master: &quot; + sc.master) println(&quot;Spark running &#39;locally&#39;?: &quot; + sc.isLocal) } } . Notice, that I placed the application under the spark package. This will be used when submitting the class to Spark. For the moment we need to structure properly our code for SBT to work. In particular, we need a file structure as follows . build.sbt +src/ +main/ +scala/ +spark/HelloSpark.scala . The build.sbt script is shown below . name := &quot;Hello Spark&quot; version := &quot;0.0.1&quot; scalaVersion := &quot;2.13.3&quot; libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-core_2.12&quot; % &quot;3.0.1&quot; libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-sql_2.12&quot; % &quot;3.0.1&quot; . As a side not, observe that I don&#39;t use double percentages. The reason why is explained here. . In order to compile our code, call sbt at the level where the build.sbt script is located. This will bring up the sbt console. Once in the console, type compile to build the project. When the compilation finishes and still in the sbt console, type package to create the application .jar file. The whole process for this application should not take long. Once finished, we can submit our application to Spark for execution. I use the following bash shell script for convenience. . /home/alex/MySoftware/spark-3.0.1-bin-hadoop2.7/bin/spark-submit --class &quot;spark.HelloSpark&quot; --master local[4] target/scala-2.13/hello-spark_2.13-0.0.1.jar . Notice how I specify the class to execute by prefixing it with the package name it belongs to. Upon execution of the script you should see something similar to what follows . Spark version: 3.0.1 Spark master: local[4] Spark running &#39;locally&#39;?: true . According to the official documentation, applications should define a main() method instead of extending scala.App, as the latter may not work correctly. . Summary . In this post, I described the steps I need to take in order to build and submit a Scala application to Spark. .",
            "url": "https://pockerman.github.io/qubit_opus/spark/scala/api/data-analysis/2021/08/06/apache-spark-scala.html",
            "relUrl": "/spark/scala/api/data-analysis/2021/08/06/apache-spark-scala.html",
            "date": " • Aug 6, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Django with Docker",
            "content": "Overview . Deploying applications is never ease regardless of the provisioning one may take. Containers solve many of the problems of application deployment. In this post, I want to describe how to containerize a minimal django application with docker. I will assume that docker is already installed on the machine. If not, checkout the official docker documentation. The code for this notebook can be found at this repository. . Acknowledgements . This post is basically edited from the testdriven.io; Dockerizing Django with Postgres, Gunicorn, and Nginx post. You should check this article out if you want more details on what is happening. . Django with docker . Assuming that docker is already installed on the host machine, I can check the version of docker and docker-compose by typing in the terminal. . docker --version docker-compose --version . The application won&#39;t do something really great as my goal here is to understand how to make these components work together. Thus, the application I will be looking at has two main components in terms of infrastructure. Namely, . It uses Django to support HTTP requests/responses | It uses MySQL for persistence | . Django project . Creating a simple Django project is fairly easy. Checkout how to do so here. Let&#39;s create a hello_world_django project. I will have the project files in the app directory. So . mkdir app &amp;&amp; cd app django-admin startproject hello_world_django . . The above creates the app directory and within that directory it creates the hello_world_django. . ├── hello_world_django │ ├── __init__.py │ ├── asgi.py │ ├── settings.py │ ├── urls.py │ └── wsgi.py ├── manage.py . Let&#39;s create a requirements.txt file in the app directory with the following contents . Django==3.0.7 . In order to containerize the hello_world_django project, we need to have a Dockerfile. A Dockerfile specifies overall how our application is to be built. So in the app directory, create a Dockerfile with the following contents . # pull official base image FROM python:3.8.3-alpine # set work directory WORKDIR /usr/src/app # set environment variables ENV PYTHONDONTWRITEBYTECODE 1 ENV PYTHONUNBUFFERED 1 # install dependencies RUN pip install --upgrade pip COPY ./requirements.txt . RUN pip install -r requirements.txt # copy project COPY . . . The Dockerfile above, starts with an Alpine-based Docker image for Python 3.8.3. It then sets a working directory along with two environment variables: . PYTHONDONTWRITEBYTECODE: Prevents Python from writing pyc files to disc (equivalent to python -B option) | PYTHONUNBUFFERED: Prevents Python from buffering stdout and stderr (equivalent to python -u option) | . Finally, it updates pip, copies over the requirements.txt file, installed the dependencies, and copied over the Django project itself. Although we can use docker build to build our image, I will use docker-compose to do so. In the source directory, create a file called docker-compose.yml with the following contents . version: &#39;3.8&#39; services: web: build: ./app command: python manage.py runserver 0.0.0.0:8000 volumes: - ./app/:/usr/src/app/ ports: - 8000:8000 env_file: - ./.env.dev . We also need one more file, namely the .env.dev file that contains the following . DEBUG=1 SECRET_KEY=foo DJANGO_ALLOWED_HOSTS=localhost 127.0.0.1 0.0.0.0 [::1] . The file should also be placed at the root directory where the docker-compose.yml is located. We also need to update the settings.py file so that we can retrieve these from the environment under which the application is running. . SECRET_KEY = os.environ.get(&quot;SECRET_KEY&quot;) DEBUG = int(os.environ.get(&quot;DEBUG&quot;, default=0)) # &#39;DJANGO_ALLOWED_HOSTS&#39; should be a single string of hosts with a space between each. # For example: &#39;DJANGO_ALLOWED_HOSTS=localhost 127.0.0.1 [::1]&#39; ALLOWED_HOSTS = os.environ.get(&quot;DJANGO_ALLOWED_HOSTS&quot;).split(&quot; &quot;) . Let&#39;s now build the image and check if everything works as described above. We can do so . docker-compose build . Start the container by using . docker-compose up -d . We can view the application at http://0.0.0.0:8000/. This should display django&#39;s default landing page. So far so good. Let&#39;s now try to integrate MySQL into the mix. . Configure MySQL . Adding MySQL into the mix, we just need to add a new service into docker-compose.yml. This is shown below . version: &#39;3.7&#39; services: web: build: ./app command: python manage.py runserver 0.0.0.0:8000 volumes: - ./app/:/usr/src/app/ ports: - 8000:8000 env_file: - ./.env.dev db: image: mysql:5.7 container_name: mysql_my_django_app ports: - &#39;3306:3306&#39; environment: MYSQL_DATABASE: &#39;django_app_demo&#39; MYSQL_PASSWORD: &#39;password&#39; MYSQL_ROOT_PASSWORD: &#39;password&#39; volumes: mysql_data: . To persist the data beyond the life of the container we configured a volume. This config will bind mysql_data to the &quot;/var/lib/mysql/data/&quot; directory in the container. . Note that since the default database in django is sqlite3, we need to update the DATABASES entry in the settings.py file according to . DATABASES = { &#39;default&#39;: { &#39;ENGINE&#39;: os.environ.get(&quot;SQL_ENGINE&quot;, &quot;django.db.backends.sqlite3&quot;), #&#39;django.db.backends.mysql&#39;, &#39;NAME&#39;: os.environ.get(&quot;SQL_DATABASE&quot;, BASE_DIR / &quot;db.sqlite3&quot;), #&#39;django_app_demo&#39;, &#39;USER&#39;: os.environ.get(&quot;SQL_USER&quot;, &quot;user&quot;), #&#39;root&#39;, &#39;PASSWORD&#39;: os.environ.get(&quot;SQL_PASSWORD&quot;, &quot;password&quot;), #&#39;password&#39;, &#39;HOST&#39;: os.environ.get(&quot;SQL_HOST&quot;, &quot;localhost&quot;), #&#39;db&#39;, &#39;PORT&#39;: os.environ.get(&quot;SQL_PORT&quot;, &quot;3306&quot;), #3306, } } . Similarly, we update the .env.dev file now looking like . DEBUG=1 SECRET_KEY=foo DJANGO_ALLOWED_HOSTS=localhost 127.0.0.1 0.0.0.0 [::1] SQL_ENGINE=django.db.backends.mysql SQL_DATABASE=django_app_demo SQL_USER=root SQL_PASSWORD=password SQL_HOST=db SQL_PORT=3306 . We also need to to install mysqlclient otherwise we get a django exception django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module. We add this in the requirements file. So the requirements.txt is now as follows . Django==3.0.7 mysqlclient==2.0.3 . We now have two containers. Let&#39;s build the new image and spin the two containers . docker-compose up -d --build . Once again we can access the application at http://0.0.0.0:8000/. . Migrations . In order to be able to persist data, we need to create the database tables. Django uses the notion of migrations in order to build and monitor the database tables. Let&#39;s instruct Django to run any migrations. Typically, we don&#39;t want to do that every time we fire up the container, so I just use a manual approach . docker-compose exec django_app python manage.py migrate . Summary . In this post, I described how to containerize a minimal Django-based web application. Specifically, I used the following docker commands . References . Dockerizing a Python Django Web Application | Django Development with Docker Compose and Machine | Dockerize a Flask, Celery, and Redis Application with Docker Compose | Dockerizing Django with Postgres, Gunicorn, and Nginx |",
            "url": "https://pockerman.github.io/qubit_opus/programming/django/docker/containers/python/2021/07/29/django-with-docker.html",
            "relUrl": "/programming/django/docker/containers/python/2021/07/29/django-with-docker.html",
            "date": " • Jul 29, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Use OpenAI Gym environments from C++",
            "content": "Overview . OpenAI-Gym is one of the most commonly used of Python packages used when developing reinforcement learning algorithms. In this post, we use the boost::python library to interact with an OpenAI-Gym environment from a C++ program. . Use OpenAI Gym environments from C++ . In this post we use the boost::python library in order to interact with an OpenAI-Gym environment. Specifically, we will interact with the FrozenLake-v0 environment. The exposition here is meant to be minimal rather than exhaustive. . In order to use boost::python we need to include it. . #include &lt;boost/python.hpp&gt; . Before starting the interaction with the Python interpreter, we need to initialize it. This is done by calling . Py_Initialize() . We next import the module of interest that is gym. We obtain a namespace so that we can use it to obtain the created environment in the interpreter. This is done in the following line . auto gym_namespace = gym_module.attr(&quot;__dict__&quot;); . We can extract various attributes. For instance, the module name as shown below . std::cout&lt;&lt;&quot;Module name &quot;&lt;&lt;boost::python::extract&lt;const char*&gt;(gym_namespace[&quot;__name__&quot;])&lt;&lt;std::endl; . The line of interest is where we create the environment in the interpreter and get a pointer back in the C++ program . // create an environment auto ignored = boost::python::exec(&quot;import gym n&quot; &quot;world = gym.make(&#39;FrozenLake-v0&#39;, is_slippery=True) n&quot; &quot;world = world.unwrapped&quot;, gym_namespace); // get the created world auto world = boost::python::extract&lt;boost::python::api::object&gt;(gym_namespace[&quot;world&quot;]); . Observe how we access the environment as an entry in the gym_namespace we created above. Once we have an instance of the environment, we can query it as shown below . auto world_dict = boost::python::extract&lt;boost::python::dict&gt;(world().attr(&quot;__dict__&quot;)); auto observation_space = boost::python::extract&lt;boost::python::api::object&gt;(world_dict()[&quot;observation_space&quot;]); std::cout&lt;&lt;&quot;Number of states &quot;&lt;&lt;boost::python::extract&lt;int&gt;(observation_space().attr(&quot;__dict__&quot;)[&quot;n&quot;]) &lt;&lt;std::endl; auto action_space = boost::python::extract&lt;boost::python::api::object&gt;(world_dict()[&quot;action_space&quot;]); std::cout&lt;&lt;&quot;Number of actions &quot;&lt;&lt;boost::python::extract&lt;int&gt;(action_space().attr(&quot;__dict__&quot;)[&quot;n&quot;])&lt;&lt;std::endl; . or execute an action . // create an environment boost::python::exec(&quot;observation = world.reset()&quot;, gym_namespace); // the observation auto observation = boost::python::extract&lt;int&gt;(gym_namespace[&quot;observation&quot;]); std::cout&lt;&lt;&quot;Observation after reset=&quot;&lt;&lt;observation&lt;&lt;std::endl; . The full driver program is shown below. . #include &lt;boost/python.hpp&gt; #include &lt;iostream&gt; int main(){ try { std::cout&lt;&lt;&quot;Starting the interpreter...&quot;&lt;&lt;std::endl; Py_Initialize(); std::cout&lt;&lt;&quot;Importing module...&quot;&lt;&lt;std::endl; auto gym_module = boost::python::import(&quot;gym&quot;); auto gym_namespace = gym_module.attr(&quot;__dict__&quot;); std::cout&lt;&lt;&quot;Module name &quot;&lt;&lt;boost::python::extract&lt;const char*&gt;(gym_namespace[&quot;__name__&quot;])&lt;&lt;std::endl; std::cout&lt;&lt;&quot;Creating the environment...&quot;&lt;&lt;std::endl; // create an environment auto ignored = boost::python::exec(&quot;import gym n&quot; &quot;world = gym.make(&#39;FrozenLake-v0&#39;, is_slippery=True) n&quot; &quot;world = world.unwrapped&quot;, gym_namespace); // get the created world auto world = boost::python::extract&lt;boost::python::api::object&gt;(gym_namespace[&quot;world&quot;]); auto world_dict = boost::python::extract&lt;boost::python::dict&gt;(world().attr(&quot;__dict__&quot;)); // uncomment this to see the attributes /*auto keys = boost::python::list(world_dict().keys()); for(auto i=0; i&lt;boost::python::len(keys); ++i){ std::cout&lt;&lt;boost::python::extract&lt;std::string&gt;(boost::python::object(keys[i]))()&lt;&lt;std::endl;; }*/ auto observation_space = boost::python::extract&lt;boost::python::api::object&gt;(world_dict()[&quot;observation_space&quot;]); std::cout&lt;&lt;&quot;Number of states &quot;&lt;&lt;boost::python::extract&lt;int&gt;(observation_space().attr(&quot;__dict__&quot;)[&quot;n&quot;])&lt;&lt;std::endl; auto action_space = boost::python::extract&lt;boost::python::api::object&gt;(world_dict()[&quot;action_space&quot;]); std::cout&lt;&lt;&quot;Number of actions &quot;&lt;&lt;boost::python::extract&lt;int&gt;(action_space().attr(&quot;__dict__&quot;)[&quot;n&quot;])&lt;&lt;std::endl; // create an environment boost::python::exec(&quot;observation = world.reset()&quot;, gym_namespace); // the observation auto observation = boost::python::extract&lt;int&gt;(gym_namespace[&quot;observation&quot;]); std::cout&lt;&lt;&quot;Observation after reset=&quot;&lt;&lt;observation&lt;&lt;std::endl; } catch(boost::python::error_already_set const &amp;) { PyErr_Print(); } std::cout&lt;&lt;&quot;Finilize...&quot;&lt;&lt;std::endl; return 0; } . Running the program gives the following output . Starting the interpreter... Importing module... Module name gym Creating the environment... Number of states 16 Number of actions 4 Observation after reset=0 Finilize... . Although boost::python handles a lot of the low level details needed for interacting with Python, the above program is rather dense and for more complicated scenarios, e.g. implementing A2C on an Atari environment, things will definitely get more complicated. One way to handle this is to write own wrappers that hide most of the boilerplate code. . Buidling the program with CMake . As an aside here is the CMakeLists.txt to use in order to build the program above . CMAKE_MINIMUM_REQUIRED(VERSION 3.6) SET(SOURCE example_1.cpp) SET(EXECUTABLE example_1) # find Boost FIND_PACKAGE(Boost 1.65.0 REQUIRED COMPONENTS python system) if(Boost_FOUND) if(Boost_LIBRARY_DIR) MESSAGE( STATUS &quot;Boost_LIBRARY_DIR not empty using it: ${Boost_LIBRARY_DIR}&quot; ) elseif(BOOST_LIBRARYDIR) MESSAGE( STATUS &quot;Boost_LIBRARY_DIR empty, but BOOST_LIBRARYDIR is set. Setting Boost_LIBRARY_DIR to: ${BOOST_LIBRARYDIR}&quot; ) set(Boost_LIBRARY_DIR ${BOOST_LIBRARYDIR}) elseif(Boost_LIBRARY_DIRS) MESSAGE( STATUS &quot;Boost_LIBRARY_DIR empty, but Boost_LIBRARY_DIRS is set. Setting Boost_LIBRARY_DIR to: ${Boost_LIBRARY_DIRS}&quot; ) set(Boost_LIBRARY_DIR ${Boost_LIBRARY_DIRS}) elseif(Boost_LIBRARY_DIR_RELEASE) MESSAGE( STATUS &quot;Boost_LIBRARY_DIR empty, but Boost_LIBRARY_DIR_RELEASE is set. Setting Boost_LIBRARY_DIR to: ${Boost_LIBRARY_DIR_RELEASE}&quot; ) set(Boost_LIBRARY_DIR ${Boost_LIBRARY_DIR_RELEASE}) elseif(Boost_LIBRARY_DIR_DEBUG) MESSAGE( STATUS &quot;Boost_LIBRARY_DIR empty, but Boost_LIBRARY_DIR_DEBUG is set. Setting Boost_LIBRARY_DIR to: ${Boost_LIBRARY_DIR_RELEASE}&quot; ) set(Boost_LIBRARY_DIR ${Boost_LIBRARY_DIR_DEBUG}) else() MESSAGE( WARNING &quot;Boost_LIBRARY_DIR empty, BOOST_LIBRARYDIR empty, Boost_LIBRARY_DIRS empty: might miss libraries at linking&quot; ) endif() else() MESSAGE( FATAL_ERROR &quot;Boost was not found!&quot;) endif() INCLUDE_DIRECTORIES(${Boost_INCLUDE_DIRS}) # use c++20 standard SET(CMAKE_CXX_COMPILER /usr/bin/g++-10) SET(CMAKE_C_COMPILER /usr/bin/gcc-10) SET(CMAKE_CXX_STANDARD 20) SET(CMAKE_CXX_STANDARD_REQUIRED True) SET(CMAKE_CXX_FLAGS &quot;-g -Wall -Wextra&quot;) SET(CMAKE_LINKER_FLAGS &quot;-pthread&quot;) # use the Boost link directories LINK_DIRECTORIES(${Boost_LIBRARY_DIR}) # this may be different... LINK_DIRECTORIES(/usr/lib/python3.8/config-3.8-x86_64-linux-gnu/) ADD_EXECUTABLE(${EXECUTABLE} ${SOURCE}) TARGET_LINK_LIBRARIES(${EXECUTABLE} python3.8) TARGET_LINK_LIBRARIES(${EXECUTABLE} boost_python38) TARGET_LINK_LIBRARIES(${EXECUTABLE} boost_system) .",
            "url": "https://pockerman.github.io/qubit_opus/programming/openai-gym/reinforcement-learning/c++/python/boost-python/2021/07/21/use-openai-gym-cpp.html",
            "relUrl": "/programming/openai-gym/reinforcement-learning/c++/python/boost-python/2021/07/21/use-openai-gym-cpp.html",
            "date": " • Jul 21, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Use Django with Apache",
            "content": "Overview . Recently, I had to serve an application developed with Django on a LAMP infrastructure. I will describe in this post the steps I followed in order to do so. You can find the application here. . Use Django with Apache . To start with, the Django offcial documentation has most of the information you need here. The suggested way is by using mod_wsgi. The problem that I had with that was due to a problem with SQLite that the project was initially using. I had to create a virtual environment on the server and install everything under the virtual environment. However, mod_wsgi only works with the version of Python it was compiled against. So if this is the case you may have to install the package in your environment. . In the latter scenario, you need to configure the modules loaded by Apache such that it points to your installation and not the system-wide one. You can run a find command on the directory you have your virtual environments: . find /path/to/your/envs/ -name &quot;mod_wsgi*.so&quot; . Then you need to update loadmodule.conf (which is typically located at /local/apache2/etc/) to point to the path given by find. . You then need to update the httpd.conf file according to your needs. You will need to provide as a minimum the following . Alias /robots.txt /path/to/mysite.com/static/robots.txt Alias /favicon.ico /path/to/mysite.com/static/favicon.ico Alias /media/ /path/to/mysite.com/media/ Alias /static/ /path/to/mysite.com/static/ &lt;Directory /path/to/mysite.com/static&gt; Require all granted &lt;/Directory&gt; &lt;Directory /path/to/mysite.com/media&gt; Require all granted &lt;/Directory&gt; WSGIDaemonProcess django_app_name python-home=/path/to/virtual/env/ python-path=/path/to/django/app/ WSGIProcessGroup django_app_name WSGIScriptAlias / /path/to/django/app/wsgi.py process-group=django_app_name WSGIApplicationGroup %{GLOBAL} &lt;Directory /path/to/mysite.com/mysite&gt; &lt;Files wsgi.py&gt; Require all granted &lt;/Files&gt; &lt;/Directory&gt; . It turns out that the process is not overly complicated but it may take some time to figure out some things. .",
            "url": "https://pockerman.github.io/qubit_opus/programming/django/python/apache/web-development/2021/07/15/django-apache.html",
            "relUrl": "/programming/django/python/apache/web-development/2021/07/15/django-apache.html",
            "date": " • Jul 15, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "MPI P2P Communication Modes",
            "content": "Overview . In the previous post, we saw the standard communication mode that is used under the hoods with MPI_Send. Here, we describe a few more communication modes supported by the MPI standard. . MPI P2P communication modes . MPI has three additional modes for P2P communication [1]: . Buffered | Synchronous | Ready | . In the buffered mode, the sending operation is always locally blocking and just like with standard communication mode, it will return as soon as the message is copied to a buffer. The difference here is that the buffer is user-provided [1]. . The synchronous mode is a globally blocking operation [1]. In this mode, the sending operation will return only when the retrival of the message has been initiated by the receiving process. However, the message receiving may not be complete [1]. . . Remark . The buffered and synchronous modes constitute two symmetrical endpoints. In the buffered mode we trade the waiting with memory whilst in the synchronous mode we don&#39;t mind o wait for the message to reach the destination. . . In the ready mode, the send operation will succeed only if a matching receive operation has been initiated already [1]. Otherwise, the function returns with an error code. The purpose of this mode is to reduce the overhead of handshaking operations [1]. . So how can we distinguish between these different commnunication modes? This is done by prefixing the initial letter of each mode before the Send [1]. Thus, we have . MPI_Bsend | MPI_Ssend | MPI_Rsend | . The resr of the functions signatures is the same as that of MPI_Send [1] . int [ MPI_Bsend | MPI_Ssend | MPI_Rsend ] (void∗ buf , int count , MPI_Datatype datatype , int dest , int tag , MPI_Comm comm ) ; . . Remark . Bear in mind that blocking sends can be matched with non blocking receives, and vice versa [1]. However, the tuple (communicator, rank, message tag) should match in order to do so. . . Summary . In this post, we introduced three more communication modes supported by MPI for P2P message exchange. The fact that we have in our disposal different means for P2P communucation means that we can adjust the application to better suit the hardware it is running on. The interafces of the supplied functions are the same with that of MPI_Send. This greatly facilitates development. We can, for example, create an array of function pointers so that we group these functions in one place and call the specified function based on some given configuration parameter. . References . Gerassimos Barlas, Multicore and GPU Programming An Integrated Approach, Morgan Kaufmann |",
            "url": "https://pockerman.github.io/qubit_opus/programming/mpi/parallel-computing/c++/distributed-computing/2021/07/08/mpi-comm-modes.html",
            "relUrl": "/programming/mpi/parallel-computing/c++/distributed-computing/2021/07/08/mpi-comm-modes.html",
            "date": " • Jul 8, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "MPI Basic Point-to-Point Communication",
            "content": "Overview . When two processes communicate with each other, we call this communication pattern as point-to-point communication [3]. MPI allows for easy information exchange between processes or nodes although the resulting interfaces may be quite overwhelming. In this notebook, we introduce the two most basic point-to-point communication functions in MPI namely MPI_Send (doc) and MPI_Recv (doc). . Basic point-to-point Communication . MPI_Send performs a blocking send; that is the function call may block until the message is received by the destination process [1]. An MPI_Send must be matched with a receive operation. MPI_Recv (doc) performs a blocking receive [2]. . . Remark . Note that MPI_Send may return before the message is delivered. MPI_Send uses the so called standard communication mode [3]. Behind the scenes, MPI decides whether to block or not based on the size of the message. The blocking lasts until the the destination process collects the message. Thus, if the message is small MPI_Send returns as soon as the message is copied to a local MPI buffer [3]. This copy is needed in order to release the buffer used by the source process for subsequent operations, because with this form of send, there is no way for the sender process to know when the message has been delivered [3]. . . MPI_Send sends a buffer of data of a certain type to another process. It requires the following arguments. . A pointer to a data buffer | The datatype contained in the specified data buffer | How many elements are contained in the buffer | A message tag (sort of the id of the message) which should be a non-negative integer | The receiving process id wihin the communicator | The communicator used | . The datatype must correspond precisely to the data stored in the buffer. For this, MPI has predefined types that can be used. MPI has most of the usual C types. Furthermore, the standard has made provisions for creating and communicating user defined types as well. . Note also that MPI_Send returns an error value code. If this value is 0 (or the symbolic constant MPI_SUCCESS ), no error has occurred [3]. . . Remark . The default behaviour when a fatal error occurs in any of the participating processes is to abort the whole execution. In a sense, the default MPI behaviour when an error occurs is not fault tolerant. . . MPI_Recv has a very similar signature with MPI_Send. The exception is that there is no destination id parameter but the id of the process from the process receives. Note also that the buffer set aside must be at least as large as the number or elements expected to be received. . Specification of the sent/received datatype is required so that machines wiht different endianness or machines with different memory types (32-bit, 64-bit, 128-bit) to be able to communicate. . Simple example . Below is a simple example of how to use MPI_Send and MPI_Recv. You can also find the example here. . #include &lt;mpi.h&gt; #include &lt;iostream&gt; int main(int argc, char** argv){ int rank; int n_procs; // initialize MPI. No MPI calls // prior to this point should be made MPI_Init(&amp;argc, &amp;argv); // what&#39;s my rank MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); // how may procs MPI_Comm_size(MPI_COMM_WORLD, &amp;n_procs); MPI_Status status; if(rank == 0){ std::cout&lt;&lt;&quot;Hello from process &quot;&lt;&lt;rank&lt;&lt;&quot; of &quot;&lt;&lt;n_procs&lt;&lt;std::endl; int num = 2; // send a number to the worker MPI_Send(&amp;num, 1, MPI_INT, 1, 0, MPI_COMM_WORLD); // recv the answer int ans = -1; MPI_Recv(&amp;ans, 1, MPI_INT, 1, 1, MPI_COMM_WORLD, &amp;status); if(ans == 0){ std::cout&lt;&lt;&quot;Number &quot;&lt;&lt;num&lt;&lt;&quot; is odd&quot;&lt;&lt;std::endl; } else{ std::cout&lt;&lt;&quot;Number &quot;&lt;&lt;num&lt;&lt;&quot; is even&quot;&lt;&lt;std::endl; } } else if(rank == 1){ // receive int data = -1; MPI_Recv(&amp;data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &amp;status); if(data % 2 == 0){ data = 1; MPI_Send(&amp;data, 1, MPI_INT, 0, 1, MPI_COMM_WORLD); } else{ data = 0; MPI_Send(&amp;data, 1, MPI_INT, 0, 1, MPI_COMM_WORLD); } } MPI_Finalize(); // No MPI calls beyond this point return 0; } . Note the following . The tag can be any integer between 0-32767 | MPI Recv may use for the tag the wildcard MPI_ANY_TAG. This allows an MPI_Recv to receive from a send using any tag. | MPI_Send cannot use the wildcard MPI_ANY_TAG. A speciﬁc tag must be speciﬁed. | MPI_Recv may use for the source the wildcard MPI_ANY_SOURCE. This allows an MPI_Recv to receive from a send from any source. | MPI_Send must specify the process rank of the destination. No wildcard exists. | . Summary . In this section, we introduced the two most basic point-to-point communication functions available in MPI. Namly we saw, MPI_Send and MPI_Recv. Although these functions, and MPI in general, hide much of the boilerplate code needed so that two processes can communicate, still the resulting program is rather verbose. This is something that we would like to hide as much as possible both for application maintenance as well as for development and performance considerations. . References . MPI_Send | MPI_Recv | Gerassimos Barlas, Multicore and GPU Programming An Integrated Approach, Morgan Kaufmann |",
            "url": "https://pockerman.github.io/qubit_opus/programming/mpi/parallel-computing/c++/2021/07/07/mpi-basic-point-to-point-communication.html",
            "relUrl": "/programming/mpi/parallel-computing/c++/2021/07/07/mpi-basic-point-to-point-communication.html",
            "date": " • Jul 7, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Machine Learning with Scala Logistic Regression",
            "content": "Overview . In the post Machine Learning with Scala Linear Regression we saw how to develop a simple linear regressor with the aide of the Breeze library. In this post, we see how to develop a logistic regressor classifier for two class classification. . Machine Learning with Scala Logistic Regression . Logistic regression is a linear classifier that is the decision boundary is a line or a hyperplane. The logistic regression algorithm is to a large extent similar to linear regression with two notable differences . We filter the result of the linear regression so that it is mapped in the range $[0, 1]$. Thus, the immediate output of logistic regression can be interpreted as a probability | The loss function that we minimize is not the MSE | . Other than that the algorithm is the same. Hence, we use a linear model of the form . $$ hat{y}_i = a x_i + b$$ . and we filter it via function so that the ouput is mapped bewteen $[0, 1]$. The sigmoid function . $$ phi(x) = frac{1}{1 + e^{-x}}$$ . can be used for such a filtering. . The loss function has the following form . $$L( mathbf{w}) = sum_{i}^N -y_i log( hat{y}_i) + (1 - y_i)(1 - log( hat{y}_i))$$ . where $ mathbf{w}$ is the parameters coefficients with $ mathbf{w} = [a, b]$. . We first import some useful packages . import breeze.linalg.{DenseMatrix, DenseVector} import breeze.linalg._ import breeze.numerics.{exp, log1p, sigmoid} import breeze.optimize.{DiffFunction, minimize} . We wrap the loss function and its gradient calculation into an object class . object LogisticRegression{ def L(x: DenseMatrix[Double], y: DenseVector[Double], parameters: DenseVector[Double]): Double = { val xBeta = x * parameters val expXBeta = exp(xBeta) val targets_time = y *:* xBeta -sum(targets_time - log1p(expXBeta)) } def gradL(x: DenseMatrix[Double], y: DenseVector[Double], parameters: DenseVector[Double]): DenseVector[Double]={ val xBeta = x * parameters val probs = sigmoid(xBeta) x.t * (probs - y) } } . This is the class that wraps the linear regression model. . class LogisticRegression { // The model parameters var parameters: DenseVector[Double] = null // Flag indicating if the interception term is used var useIntecept: Boolean=true; // auxiliary constructor def this(numFeatures: Int, useIntercept: Boolean=true){ this() init(numFeatures = numFeatures, useIntercept = useIntercept) } // initialize the underlying data def init(numFeatures: Int, useIntercept: Boolean=true): Unit = { val totalFeatures = if(useIntercept) numFeatures + 1 else numFeatures this.parameters = DenseVector.zeros[Double](totalFeatures) this.useIntecept = useIntercept } // train the model def train(x: DenseMatrix[Double], y: DenseVector[Double])={ // set up the optimization val f = new DiffFunction[DenseVector[Double]] { def calculate(parameters: DenseVector[Double]) = (LogisticRegression.L(x, y, parameters=parameters), LogisticRegression.gradL(x, y, parameters = parameters)) } this.parameters = minimize(f, this.parameters) } // predict the class of the given point def predict(x: DenseVector[Double]): Double = { require(parameters != null) if(!useIntecept){ require(x.size == parameters.size) sum(parameters * x) } else{ require(x.size == parameters.size -1 ) sum(parameters.slice(0, x.size) * x) + parameters(0) } } } . Let&#39;s put this into action with a simple example. . import breeze.linalg._ import breeze.numerics._ import breeze.optimize._ import breeze.stats._ import engine.models.LogisticRegression import engine.utils.{CSVDataSetLoader, VectorUtils} import spire.algebra.NormedVectorSpace.InnerProductSpaceIsNormedVectorSpace import spire.implicits.rightModuleOps object LogisticRegression_Exe extends App{ println(s&quot;Starting application: ${LogisticRegression_Exe.getClass.getName}&quot;) // load the data val data = CSVDataSetLoader.loadRepHeightWeightsFullData val recaledHeights = VectorUtils.standardize(data.heights); val rescaledWeights = VectorUtils.standardize(data.weights); val rescaledHeightsAsMatrix = recaledHeights.toDenseMatrix.t val rescaledWeightsAsMatrix = rescaledWeights.toDenseMatrix.t val featureMatrix = DenseMatrix.horzcat(DenseMatrix.ones[Double](rescaledHeightsAsMatrix.rows, 1), rescaledHeightsAsMatrix, rescaledWeightsAsMatrix) println(s&quot;Feature matrix shape (${featureMatrix.rows}, ${featureMatrix.cols})&quot;) val targets = data.genders.values.map{gender =&gt; if(gender == &#39;M&#39;) 1.0 else 0.0} println(s&quot;Targets vector shape (${targets.size}, )&quot;) // logistic regression model val lr = new LogisticRegression; // initialize the model lr.init(numFeatures=2) lr.train(x=featureMatrix, y=targets) val optimalParams = lr.parameters println(s&quot;Optimal parameters ${optimalParams}&quot;) println(&quot;Done...&quot;) } . You can find the complete example in this repo. . Summary . In this post we looked into how to develop a simple linear regression model with Scala. The Scala numerics library Breeze greatly simplifies the development. . References . Logistic regression | Pascal Bugnion, Patric R. Nicolas, Alex Kozlov, Scala: Applied Machine Learning |",
            "url": "https://pockerman.github.io/qubit_opus/machine-learning/scala/logistic-regression/2021/06/28/ml-with-scala-logistic-regression.html",
            "relUrl": "/machine-learning/scala/logistic-regression/2021/06/28/ml-with-scala-logistic-regression.html",
            "date": " • Jun 28, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Machine Learning with Scala Linear Regression",
            "content": "Overview . Python at the time of writing is the defacto language for prototyping and developing machine learning algorithms. In this post, we will be using Scala to develop a simple linear regressor model. We will do this with the help of the Scala numerics library Breeze. . Machine Learning with Scala Linear Regression . As it is well known, the linear regression model assumes the following functional form for the predictor $ hat{y}$ . $$ hat{y}_i = a x_i + b$$ . The loss function has the following form . $$L( mathbf{w}) = sum_{i}^N (y_i - hat{y}_i)^2 = sum_{i}^N (y_i - (a x_i + b))^2$$ . where $ mathbf{w}$ is the parameters coefficients with $ mathbf{w} = [a, b]$. The gradient of the loss function with respect to the parameters is as follows . $$ frac{ partial L}{ partial a} = -2 sum_{i}^N (y_i - hat{y}_i) x_i$$ . $$ frac{ partial L}{ partial b} = -2 sum_{i}^N (y_i - hat{y}_i)$$ . The term . $$SSE = sum_{i}^N (y_i - hat{y}_i)^2$$ . is called the sum of squared errors or SSE. If we divide with the number of training examples, $N$, then we get the so-called mean squared error or MSE . $$MSE = frac{1}{N} sum_{i}^N (y_i - hat{y}_i)^2$$ . We first import some useful packages . import breeze.linalg._ import breeze.optimize.{DiffFunction, minimize} . We wrap the loss function and its gradient calculation into an object class . object LinearRegression { def L(x: DenseMatrix[Double], y: DenseVector[Double], parameters: DenseVector[Double]): Double = { val yHat = x * parameters var value = 0.0 for( i &lt;- 0 until yHat.size){ val diff = y(i) - yHat(i) value += diff * diff } value } def gradL(x: DenseMatrix[Double], y: DenseVector[Double], parameters: DenseVector[Double]): DenseVector[Double]={ val yHat = x * parameters // we have as many components as columns val gradients = DenseVector.zeros[Double](x.cols) for( i &lt;- 0 until yHat.size){ var diff = y(i) - yHat(i) for( c &lt;- 0 until gradients.size){ diff *= x(i, c) gradients(c) += diff } } -2.0 * gradients } } . This is the class that wraps the linear regression model. . class LinearRegression{ // The model parameters var parameters: DenseVector[Double] = null // Flag indicating if the interception term is used var useIntecept: Boolean=true; // constructor def this(numFeatures: Int, useIntercept: Boolean=true){ this() init(numFeatures = numFeatures, useIntercept = useIntercept) } // train the model def train(x: DenseMatrix[Double], y: DenseVector[Double])={ // set up the optimization val f = new DiffFunction[DenseVector[Double]] { def calculate(parameters: DenseVector[Double]) = (LinearRegression.L(x, y, parameters=parameters), LinearRegression.gradL(x, y, parameters = parameters)) } this.parameters = minimize(f, this.parameters) } // the initialization function def init(numFeatures: Int, useIntercept: Boolean=true): Unit = { val totalFeatures = if(useIntercept) numFeatures + 1 else numFeatures this.parameters = DenseVector.zeros[Double](totalFeatures) this.useIntecept = useIntercept } } . Let&#39;s put this into action with a simple example. . object LinearRegressionExe_1 { def main(args: Array[String]):Unit={ // data set val x = LineDataSetLoader.lineSplit(0.0, 10.0, 100) System.out.println(&quot;Number of training examples: &quot; + x.size) val coeffs = Array[Double](1.0, 2.0) val poly = new Polynomial(coeffs) val y = poly.values(x) // the feature matrix val featureMatrix = DenseMatrix.horzcat(DenseMatrix.ones[Double](x.size, 1), x.toDenseMatrix.t) // model val model = new LinearRegression(numFeatures = 1, useIntercept = true) model.train(x=featureMatrix,y=y) println(s&quot;Polynomial coeffs ${poly.getCoeffsAsDenseVector}&quot;) println(s&quot;Linear regressor coeffs ${model.getParameters}&quot;) } } . You can find the complete example in this repo. . Summary . In this post we looked into how to develop a simple linear regression model with Scala. The Scala numerics library Breeze greatly simplifies the development. . References . Linear regression | Pascal Bugnion, Patric R. Nicolas, Alex Kozlov, Scala: Applied Machine Learning |",
            "url": "https://pockerman.github.io/qubit_opus/machine-learning/scala/linear-regression/2021/06/27/ml-with-scala-linear-regression.html",
            "relUrl": "/machine-learning/scala/linear-regression/2021/06/27/ml-with-scala-linear-regression.html",
            "date": " • Jun 27, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "MPI Object Communication 1",
            "content": "Overview . In object oriented code bases, data is typically organized into classes that wrap functionality, hide information and expose an API so that client code can utilize them. Thus, frequently, we end up in the situation where we have an object that we need to send across. MPI offers various posibilities to do so. In this post we will see MPI_Type_create_struct. . MPI Object Communication 1 . MPI communication functions such as MPI_Send/Recv need as an input the type of the data that is to be communicated [1]. When dealing with primitive types like integers and floats MPI has got us covered so there isn&#39;t much we should do. . However, frequently we want to communicate structures or objects. Sure, we can break up the structures that need to be communicated into individual elements or arrays of elements and send these in a series of send operations. However, this costly and rather counter productive; it breaks data encapsulation to start with. . Why it is costly, can be understood by considerin the so-called start-up latency [1]. This is the fixed cost we need to accept that includes the activation of multiple OS layers, the network interface, and so on [1]. The result is that although the actual over-the-wire times may be identical, the accumulation of the extra start-up latencies makes such an approach expensive to use. . MPI has two main mechanisms that we can use to communicate structures between heterogeneous machines [1] . MPI derived datatypes | Packing/unpacking data | . In this post, we will look into how to construct MPI derived datatypes using MPI_Type_create_struct and leave the second approach for another post. . Derived Datatypes . The memory layout of the same data structure differs from machine to machine. MPI, in order to successfully transfer and translate an instance of a structure from one machine to another, it requires the following information [1]: . The number and types of all the data members/fields. | The relative offset of the fields from the beginning of the structure (where to deposit data). | The total memory occupied by a structure, including any padding necessary to align it to specific boundaries. This is needed so that arrays of structures can be communicated. | . MPI provides utilities for describing the information above for a generatl datatype. Once a derived datatype is defined, a reference to this object can be used in any communication function that requires a datatype specification parameter [1]. . . Remark . Derived datatypes must be declared individually/locally in all the processes that will employ them [1]. . . Two of the most commonly used functions for creating derived datatypes are [1]: . MPI_Type_vector | MPI_Type_create_struct | . MPI_Type_vector is useful for extracting blocks of data from single or multidimensional arrays of a single datatype e.g. a vector. MPI_Type_create_struct is the most generic of the available functions, allowing the use of blocks made of different datatypes [1]. . Regardless of the approach used, each specification of a derived datatype must be followed by a call to the MPI_Type_commit function for having MPI store the specification internally. Once a datatype is committed, it can be used repeatedly in communication functions. MPI_Type_commit takes just a single parameter, which is a reference to the MPI_Datatype object [1]. . The following example shows how to use MPI_Type_create_struct. . Example . #include &lt;mpi.h&gt; #include &lt;iostream&gt; struct Point { unsigned int id; double x; double y; }; . As already mentioned MPI_Type_create_struct is rather involved so we group everything in the following function . void create_mpi_point( MPI_Datatype* t){ Point p; // the types the struct has MPI_Datatype types [3]; types[0] = MPI_UNSIGNED; types[1] = MPI_DOUBLE; types[2] = MPI_DOUBLE; // get the addresses MPI_Aint displ[3]; MPI_Aint off; MPI_Aint base; displ [0] = 0 ; MPI_Get_address (&amp;(p.id) , &amp;base ) ; MPI_Get_address (&amp;(p.x) , &amp;off ) ; displ [1] = off- base ; MPI_Get_address (&amp;(p.y) , &amp;off ) ; displ [2] = off - base; int blklen [3] = {1, 1, 1} ; // create the type MPI_Type_create_struct( 3 , blklen , displ , types , t); // commit it MPI_Type_commit ( t ) ; } . Here is the main function . int main(int argc, char** argv){ int rank; int n_procs; // initialize MPI. No MPI calls // prior to this point should be made MPI_Init(&amp;argc, &amp;argv); // what&#39;s my rank MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); // how may procs MPI_Comm_size(MPI_COMM_WORLD, &amp;n_procs); if(n_procs &gt; 2){ std::cout&lt;&lt;&quot;Application should be run with 2 processes.&quot;&lt;&lt;std::endl; MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE); } // status on the receive side MPI_Status status; // all processes must commit the Point type MPI_Datatype mpi_point_type; // create the mpi point create_mpi_point(&amp;mpi_point_type); if(rank == 0){ std::cout&lt;&lt;&quot;Hello from process &quot;&lt;&lt;rank&lt;&lt;&quot; of &quot;&lt;&lt;n_procs&lt;&lt;std::endl; Point p = {10, 0.5, 1.5}; std::cout&lt;&lt;&quot;Process &quot;&lt;&lt;rank&lt;&lt;&quot; sending point &quot; &lt;&lt;p.id &lt;&lt;&quot;, &quot; &lt;&lt;p.x &lt;&lt;&quot;, &quot; &lt;&lt;p.y &lt;&lt;std::endl; // send a number to the worker MPI_Send(&amp;p, 1, mpi_point_type, 1, 0, MPI_COMM_WORLD); } else if(rank == 1){ // receive Point p_recv; MPI_Recv(&amp;p_recv, 1, mpi_point_type, 0, 0, MPI_COMM_WORLD, &amp;status); std::cout&lt;&lt;&quot;Process &quot;&lt;&lt;rank&lt;&lt;&quot; received point &quot; &lt;&lt;p_recv.id &lt;&lt;&quot;, &quot; &lt;&lt;p_recv.x &lt;&lt;&quot;, &quot; &lt;&lt;p_recv.y &lt;&lt;std::endl; } MPI_Finalize(); // No MPI calls beyond this point return 0; } . Summary . In summary, this post breifly touched on the issued of communicating user defined datatypes with MPI. These are usually in the form of classes or structs. Although, we could align such types with primitive types and comunicate the ensuing arrays, this is not a viable approach due to start-up latency. Furthermore, it will certainly lead to an error prone and complex code base. . MPI provides various utilites in order to address such a situation. In this post, we saw MPI_Type_create_struct. This is the most generic of the available functions, allowing the use of blocks made of different datatypes [1]. . References . Gerassimos Barlas, Multicore and GPU Programming. An Integrated Approach. |",
            "url": "https://pockerman.github.io/qubit_opus/programming/mpi/parallel-computing/c++/2021/06/24/mpi-object-communication.html",
            "relUrl": "/programming/mpi/parallel-computing/c++/2021/06/24/mpi-object-communication.html",
            "date": " • Jun 24, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "MPI Hello World",
            "content": "Overview . The Message Passage Interface, or MPI for short, is perhaps the defacto standard used in nowadays scientific distributed computing. It provides interfaces for both point-to-point and collective communication. In this series, we will go over basic and intermediate usage of the MPI standard. . MPI Hello World . MPI is a well-established standard that includes [1]: . Process creation | Process management | Point-to-point communication | Collective communication | One-sided communication | External interfaces | Topologies | . Language bindings exist for C and Fortran. Newer MPI standards are trying to better support the scalability in future extreme-scale computing systems, because currently, the only feasible option for increasingthe computing power is to increase the number of cooperating processors [1]. . The following code snippet is the Hello World equivalent for MPI. It demonstrates basic usage of the standard. . Code . // example_1.cpp #include &lt;mpi.h&gt; #include &lt;iostream&gt; int main(int argc, char** argv){ int rank; int n_procs; // initialize MPI. No MPI calls // prior to this point should be made MPI_Init(&amp;argc, &amp;argv); // what&#39;s my rank MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); // how may procs MPI_Comm_size(MPI_COMM_WORLD, &amp;n_procs); std::cout&lt;&lt;&quot;Hello from process &quot;&lt;&lt;rank&lt;&lt;&quot; of &quot;&lt;&lt;n_procs&lt;&lt;std::endl; MPI_Finalize(); // No MPI calls beyond this point } . We can compile the code by using the following . mpicxx example_1.cpp -o example_1 . Executing using four processes . mpirun -np 4 example_1 . produces . Hello from process 0 of 4 Hello from process 1 of 4 Hello from process 3 of 4 Hello from process 2 of 4 . . Remark . The mpirun command is also known as mpiexec on some implementations. It makesl that copies of the executable can be run on every machine. . . A few things to notice are: . In order to access any MPI related functionality we need to inlude the header file mpi.h` | . MPI related functions start with the prefix MPI_ | . As commented in the code, all MPI related calls should occur within MPI_Init/MPI_Finalize. | . In the code above, every process executes the same instructions. We can use if/else statements to differentiate what each process will execute. The variable MPI_COMM_WORLD is a predefined intra-communicator, i.e., it serves communications taking place between processes belonging to its own group of processes [1]. . The code above also calculates the rank of the calling process via MPI_Comm_rank, and the total number of processes in the comunicator MPI_Comm_size. . References . Roman Trobec et al., Introduction to parallel computing. From algorithms to programming on state-of-the-art platforms, Springer. |",
            "url": "https://pockerman.github.io/qubit_opus/programming/mpi/parallel-computing/c++/2021/06/13/mpi-hello-world.html",
            "relUrl": "/programming/mpi/parallel-computing/c++/2021/06/13/mpi-hello-world.html",
            "date": " • Jun 13, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Value Iteration With Scala 2",
            "content": "Overview . In a previous post we saw how to implement the value iteration algorithm in Scala. This is particularly easy algorithm to implement. In that post, we used a dummy environment to check on the algorithm. In this post we extend our implementation by integrating ScalaPy in our implementation. This way we can interact with OpenAI Gym library which is written in Python. However, this way we have access to various environments to test our reinforcement learning algorithms. . Value iteration . Recall that one drawback of policy iteration is that each of its iterations involves a policy evaluation. This, however, may itself be an iterative computation; therefore requiring multiple sweeps through the state set [1]. Furthermore, if the evaluation is done iteratively, then convergence to $V_{ pi}$ occurs only in the limit [1]. . Luckily, the policy evaluation step of policy iteration can truncated without loosing the convergence gurantees of the method. Moreover, this can be done in several different ways [1]. . In particular, when policy evaluation is stopped after just one update of each state, the algorithm is called value iteration. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps [1] . $$V_{k+1}(s) = max_{ alpha} sum_{s^*, r}p(s^*, r | s, alpha) left[r + gamma V_{k}(s^*) right], ~~ forall s in mathbb{S}$$ . Figure 1: Value iteration algorithm. Image from [1]. . The value iteration is obtained simply by turning the Bellman optimality equation into an update rule [1]. It requires the maximum to be taken over all the actions. Furthermore, the algorithm terminates by checking the amount of change of the value function. . Code . OpenAI Gym provides a large collection of environments to test on reinforcement learning algorithms. The library is written in Python. Hence, we cannot use here as is. Fortunately, ScalaPy allows us to use Python libraries from Scala code. Check the ScalaPy how to install it on your machine. . Let&#39;s see how to implement a simple wrapper over the FrozenLake-v0 environment. . import scala.collection.mutable.ArrayBuffer import scala.util.control.Breaks._ import scala.math.max . import scala.collection.mutable.ArrayBuffer import scala.util.control.Breaks._ import scala.math.max . package engine.worlds import me.shadaj.scalapy.py class FrozenLake(val version: String) { // load the gym module private val gym = py.module(&quot;gym&quot;) private var env: me.shadaj.scalapy.py.Dynamic = null // Make the environment def make: Unit = this.env = gym.make(this.name) // Reset the environment def reset: Int = { val state = this.env.reset() state.as[Int] } // Returns the name of the environment def name: String = {&quot;FrozenLake-&quot; + version} // Returns the number of states def nStates: Int = this.env.observation_space.n.as[Int] // Returns the number of actions def nActions: Int = this.env.action_space.n.as[Int]; // def getDynamics(state: Int, action: Int): Seq[(Double, Int, Double, Boolean)] = { val P = py.Dynamic.global.dict(this.env.P) val dynamicsTuple = P.bracketAccess(state) val result = dynamicsTuple.bracketAccess(action).as[Seq[Tuple4[Double, Int,Double, Boolean]]] result } } . This is very simple wrapper suitable though for our needs. It hides all the boilerplate code we need to have in order to interface with OpenAI Gym. Let&#39;s also change the implementation of the ValueIteration class. The class has two implementation depending on the trainMode value defined below . package engine.rl object TrainMode extends Enumeration { val DEFAULT = Value(0, name = &quot;DEFAULT&quot;) val STOCHASTIC = Value(1, name = &quot;STOCHASTIC&quot;) } . The STOCHASTIC mode walks in the environment by randomly selecting an action. The DEFAULT implements the algorithm as we implemented in the previous related post. . package engine.rl import scala.collection.mutable import breeze.linalg.{DenseVector, max} import engine.worlds.DiscreteEnvironment import scala.util.control.Breaks.{break, breakable} class ValueIteration(env: DiscreteEnvironment, gamma: Double, maxIterations: Int, tolerance: Double, trainMode: TrainMode.Value=TrainMode.DEFAULT) { val rewards = new mutable.HashMap[Tuple3[Int, Int, Int], Double]() val transits = new mutable.HashMap[Tuple3[Int, Int, Int], Int] var stateValues = DenseVector.zeros[Double](env.nStates) var residual = 1.0 def train: Unit = { this.rewards.clear() this.transits.clear() this.stateValues = DenseVector.zeros[Double](env.nStates) breakable { for(itr &lt;- Range(0, maxIterations)){ println(&quot;&gt; Learning iteration &quot; + itr) println(&quot;&gt; Learning residual &quot; + residual) step if(residual &lt; tolerance) break; } } } def step: Unit ={ // stop condition var delta = 0.0 // update each state for( s &lt;- 0 until env.nStates){ // Do a one-step lookahead to find the best action val value = oneStepLookahead(state=s) val bestActionValue = breeze.linalg.max(value) delta = math.max(delta, math.abs(bestActionValue - stateValues(s))) stateValues(s) = bestActionValue } residual = delta } def defaultStep: Unit = { } def stochasticStep: Unit = { } protected def oneStepLookahead(state: Int): DenseVector[Double] = { val values = DenseVector.zeros[Double](env.nActions) for(action &lt;- 0 until env.nActions) { val dynamics = env.getDynamics(state = state, action = action) for(item &lt;- dynamics.indices){ val prob = dynamics(item)._1 val next_state = dynamics(item)._2 val reward = dynamics(item)._3 values(action) += prob * (reward + gamma * stateValues(next_state)) if(rewards.contains((state, action, next_state))){ rewards.update((state, action, next_state), reward) transits.update((state, action, next_state), 1) } else{ rewards.addOne((state, action, next_state), reward) transits.addOne((state, action, next_state), 1) } } } values } } . References . Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. |",
            "url": "https://pockerman.github.io/qubit_opus/scala/reinforcement-learning/algorithms/dynamic-programming/2021/06/06/scala-value-iteration-2.html",
            "relUrl": "/scala/reinforcement-learning/algorithms/dynamic-programming/2021/06/06/scala-value-iteration-2.html",
            "date": " • Jun 6, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "29 Μαίου 1453 Η άλωση της Κωνσταντινούπολης",
            "content": "29 &#924;&#945;&#943;&#959;&#965; 1453 &#919; &#940;&#955;&#969;&#963;&#951; &#964;&#951;&#962; &#922;&#969;&#957;&#963;&#964;&#945;&#957;&#964;&#953;&#957;&#959;&#973;&#960;&#959;&#955;&#951;&#962; . . .",
            "url": "https://pockerman.github.io/qubit_opus/%CE%B9%CF%83%CF%84%CE%BF%CF%81%CE%AF%CE%B1/%CE%BA%CF%89%CE%BD%CF%83%CF%84%CE%B1%CE%BD%CF%84%CE%B9%CE%BD%CE%BF%CF%8D%CF%80%CE%BF%CE%BB%CE%B7/%CE%AC%CE%BB%CF%89%CF%83%CE%B7/1453/2021/05/29/alosi-polis.html",
            "relUrl": "/%CE%B9%CF%83%CF%84%CE%BF%CF%81%CE%AF%CE%B1/%CE%BA%CF%89%CE%BD%CF%83%CF%84%CE%B1%CE%BD%CF%84%CE%B9%CE%BD%CE%BF%CF%8D%CF%80%CE%BF%CE%BB%CE%B7/%CE%AC%CE%BB%CF%89%CF%83%CE%B7/1453/2021/05/29/alosi-polis.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Scala Inheritance 2",
            "content": "Scala inheritance 2 . As in Java or C++, you can declare a ﬁeld or method as protected . Such a member is accessible from any subclass, but not from other locations. Unlike in Java, a protected member is not visible throughout the package to which the class belongs. (If you want this visibility, you can use a package modiﬁer). . class Element{ protected def speak = println(&quot;Hi from element&quot;) } . defined class Element . Recall that a class has one primary constructor and any number of auxiliary constructors, and that all auxiliary constructors must start with a call to a preceding auxiliary constructor or the primary constructor. As a consequence, an auxiliary constructor can never invoke a superclass constructor directly. . The auxiliary constructors of the subclass eventually call the primary constructor of the subclass. Only the primary constructor can call a superclass constructor. Recall that the primary constructor is intertwined with the class deﬁnition. The call to the superclass constructor is similarly intertwined. Here is an example . Overriding fields . A ﬁeld in Scala consists of a private ﬁeld and accessor/mutator methods. You can override a val (or a parameterless def ) with another val ﬁeld of the same name. The subclass has a private ﬁeld and a public getter, and the getter overrides the superclass getter (or method) . class Item(name: String){ override def toString = getClass.getName + &quot; &quot; + name } . defined class Item . class Milk extends Item(&quot;Milk&quot;) . defined class Milk . val milk = new Milk println(milk.toString) . ammonite.$sess.cmd2$Helper$Milk Milk . milk: Milk = ammonite.$sess.cmd2$Helper$Milk Milk . class Bullet extends Item(&quot;Bullet&quot;){ override val name =&quot;Flower&quot; } . cmd4.sc:2: value name overrides nothing override val name =&#34;Flower&#34; ^Compilation Failed . Compilation Failed . A def can only override another def . | A val can only override another val or a parameterless def . | A var can only override an abstract var | . Anonymous subclasses . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/inheritance/programming/oop/2021/05/24/scala-inheritance-2.html",
            "relUrl": "/scala/inheritance/programming/oop/2021/05/24/scala-inheritance-2.html",
            "date": " • May 24, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "Οι Μαορί στην Κρήτη",
            "content": "&#927;&#953; &#924;&#945;&#959;&#961;&#943; &#963;&#964;&#951;&#957; &#922;&#961;&#942;&#964;&#951; . Στις 20 Μαίου 1941 ξεκινάει η αεροπορική επιχείρηση κατάληψης της Κρήτης απο τον ναζιστικό στράτό. Η μάχη που ακολουθεί ειναι σφοδρότατη και διαρκεί μεχρι την 1η Ιουνίου 1941 και είναι γνωστή ως Η Μάχη της Κρήτης. . Εικονα 1. Γερμανοί αλεξιπτωτιστές και αεροπλάνα πάνω από την Κρήτη, Μάιος 1941. Πηγη Wikipedia. . Εικονα 2. Γερμανοί αλεξιπτωτιστές και αεροπλάνα πάνω από την Κρήτη, Μάιος 1941. Πηγη Wikipedia. . Εικονα 3. Μάχη σώμα με σώμα στην Κρήτη. . Εικονα 4. Γυναικόπαιδα αιχμάλωτοι των Γερμανών στην Κρήτη. . Την υπερασπιση το νησιου εχει αναλαβει στην κυριολεξια ενα μωσαικο πολεμιστων; Ελληνες, Βρεττανοι, Αυστραλοι και Νεοζηλανδοι. Αναμεσα σε αυτες τις δυναμεις υπαρχει και το 28ο ταγμα πεζικου της 2ης Νεοζηλανδικης μεραρχιας. Η συνθεση του ταγματος περιλαμβανει και 700 αυτοχθονες εθελοντες πολεμιστες απο τη Νεα Ζηλανδια τους λεγομενους Μαορι. . Εικονα 5. Μέλη του τάγματος των Μαορί που συμμετείχαν στην Μάχη της Κρήτης, εκτελώντας τον παραδοσιακό πολεμικό χορό χάκα στην Αίγυπτο. (Πηγή φωτογραφίας AΠΕ-ΜΠΕ: Wikimedia Commons). . Το ταγμα βρισκεται στην Ελλαδα απο τα τελη Μαρτιου 1941. Συμφωνα με τον κυριο Γεωργιο Λοη, οι Μαορί στην αρχη ηρθαν αντιμετωποι με τα ναζιστικα στρατευματα στην ηπειρωτικη Ελλαδα οταν και καλυπταν τον ελιγμο υποχωρησης της Βρετανικης Εκστρατευτικης Δυναμης. Στην συνεχεια μεταφερονται στην Κρητη μαζι με τον κυριο ογκο των υποχωρουντων συμμαχικων δυναμεων και θα λαβει μερος στην υπερασπιση του νησιου. . Κατα την διαρκεια της Μαχης της Κρήτης, το ταγμα θα λαβει μερος στις εκκαθαρισεις των θυλάκων που ειχαν δημιουργησει οι Γερμανοι αλεξιπτωτιστες στην περιοχη το Πλατανια Χανιων εων στις 22-23 Μαιου το ταγμα συμμετειχε την συμμαχικη αντεπιθεση ανακαταλυψης του αεροδρομιου του Μάλεμε. Οπως μας λεει ο κυριος Γεωργιος Λοης . »Οι Νεοζηλανδοί αυτόχθονες πολέμησαν με παραδειγματική γενναιότητα, αγωνιζόμενοι αρκετές φορές με πείσμα εκ του συστάδην και κραυγάζοντας τις παραδοσιακές πολεμικές ιαχές τους, προκαλώντας κυριολεκτικά τον τρόμο στις τάξεις του εχθρού. Αν και προσπάθησαν να ανταπεξέλθουν στην αποστολή τους με τον καλύτερο δυνατό τρόπο, ενεργώντας μέσα σε χαοτικές συνθήκες, εντούτοις διατάχθηκαν να υποχωρήσουν από το Μάλεμε, παρά τη θέλησή τους να συνεχίσουν την επιχείρηση κατάληψης του αεροδρομίου» . Επισης, οπως συνεχιζει ο κυριος Λόης . «Στις 27 Μαΐου το 28ο Τάγμα πρωτοστάτησε στη συγκλονιστική αναμέτρηση της 42ης Οδού (σημερινή οδός Τσικαλαριών), συμπαρασύροντας και τις γειτονικές Νεοζηλανδικές και Αυστραλιανές μονάδες. Απέναντι στην επιτιθέμενη εχθρική δύναμη, οι ενθουσιώδεις Μαορί στρατιώτες τοποθέτησαν με ψυχραιμία τις ξιφολόγχες στα τυφέκιά τους και διενέργησαν ορμητική έφοδο με απαράμιλλο θάρρος και με εκκωφαντικές ιαχές, διαλύοντας κυριολεκτικά τους Γερμανούς Ορεινούς Κυνηγούς. Μάλιστα, πριν την εφόρμηση, αρκετοί εξ’ αυτών σηκώθηκαν αγέρωχα από τις θέσεις μάχης τους και εκτέλεσαν τον πολεμικό χορό χάκα, κραδαίνοντας τον οπλισμό τους για λίγα δευτερόλεπτα, ενώ οι αντίπαλοί τους κοιτούσαν εμβρόντητοι. Αυτή η παροδική συμμαχική νίκη καθυστέρησε σημαντικά τη γερμανική προέλαση, παρέχοντας τον απαιτούμενο χρόνο στους σχηματισμούς της Βρετανικής Κοινοπολιτείας να αρχίσουν την απόσυρσή τους με σχετική ασφάλεια από την περιοχή της Σούδας προς τα νότια παράλια της Κρήτης» . «Τις επόμενες ημέρες, το κύριο σώμα των Νεοζηλανδών αυτοχθόνων ακολούθησε το καθορισμένο δρομολόγιο αποχώρησης, διασχίζοντας τα δύσβατα Λευκά Όρη απολύτως συντεταγμένα με τερματικό σταθμό τη Χώρα Σφακίων, όπου ανέμεναν τα βρετανικά πολεμικά πλοία εκκένωσης. Παράλληλα, ένα κλιμάκιο της μονάδας συγκρότησε το τμήμα οπισθοφυλακής των συμμαχικών στρατευμάτων, καλύπτοντας με επιτυχία την υποχωρητική ενέργειά τους. Οι ακατάβλητοι Μαορί αναχώρησαν σε δύο τμήματα την 31η Μαΐου και την 1η Ιουνίου 1941 για το λιμάνι της Αλεξάνδρειας της Αιγύπτου, αφήνοντας στην Κρήτη έναν βαρύ φόρο αίματος 84 νεκρών, ενώ άλλοι 71 άνδρες συνελήφθησαν αιχμάλωτοι». . Δεν υπάρχει αμφιβολία, οπως τονίζει ο κ. Γεώργιος Λόης, «οι πολεμιστές Μαορί επέδειξαν ύψιστο μαχητικό πνεύμα, υπερασπιζόμενοι με περισσή αυταπάρνηση την κρητική γη ενάντια στους Γερμανούς εισβολείς, ευρισκόμενοι χιλιάδες χιλιόμετρα μακριά από τις πατρογονικές εστίες τους. . &#917;&#965;&#967;&#945;&#961;&#953;&#963;&#964;&#943;&#949;&#962; . Το άρθρο αυτό είναι μια αναπαραγωγή του άρθρου 80 χρόνια από τη Μάχη της Κρήτης: Η ξεχασμένη ιστορία των Μαορί που πολέμησαν με ξιφολόγχες τους Γερμανούς αλεξιπτωτιστές από τον ιστότοπο HellasJournal. .",
            "url": "https://pockerman.github.io/qubit_opus/%CE%B9%CF%83%CF%84%CE%BF%CF%81%CE%AF%CE%B1/%CE%BC%CE%AC%CF%87%CE%B7-%CF%84%CE%B7%CF%82-%CE%BA%CF%81%CE%AE%CF%84%CE%B7%CF%82/%CE%BC%CE%B1%CE%BF%CF%81%CE%AF/%CE%B5%CE%BB%CE%BB%CE%AC%CE%B4%CE%B1/%CE%B2%CF%80%CF%80/2021/05/23/maori-crete-battle.html",
            "relUrl": "/%CE%B9%CF%83%CF%84%CE%BF%CF%81%CE%AF%CE%B1/%CE%BC%CE%AC%CF%87%CE%B7-%CF%84%CE%B7%CF%82-%CE%BA%CF%81%CE%AE%CF%84%CE%B7%CF%82/%CE%BC%CE%B1%CE%BF%CF%81%CE%AF/%CE%B5%CE%BB%CE%BB%CE%AC%CE%B4%CE%B1/%CE%B2%CF%80%CF%80/2021/05/23/maori-crete-battle.html",
            "date": " • May 23, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Εκπομπές για την γενοκτονία των Ελλήνων  του Πόντου",
            "content": "from IPython.display import YouTubeVideo YouTubeVideo(&#39;2C_O3hKpw-4&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;gjCqPYOpWb0&#39;, width=800, height=300) .",
            "url": "https://pockerman.github.io/qubit_opus/history/pontiac-genocide/turkish-atrocities/2021/05/19/pontiac-genocide.html",
            "relUrl": "/history/pontiac-genocide/turkish-atrocities/2021/05/19/pontiac-genocide.html",
            "date": " • May 19, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Scala Inheritance 1",
            "content": "Overview . Inheritance is a cornerstone of OOP and Scala has support for this concept. A Scala class can extend one and only one other class. This is similar to Java. We can do this using the extends keyword. . Inheritance 1 . Class inheritance is the common way for achiving polymorphism. A Scala class can extend one and only one other class. This is similar to Java. We can do this using the extends keyword. . class Element(id: Int){ } . defined class Element . class Triangle(id: Int) extends Element(id){ def elementType: String = &quot;TRI&quot; } . defined class Triangle . Similar to Java, we can declare a class final so that it cannot be extended. However, unlike Java, you can also declare individual methods or fields final so that they cannot be overridden [1]. . Overriding methods . Child classes can customize the behaviour of methods from the parent classes (provided of course these are not decaled final). This however should be told explicitly using the override modifier . class Element(id: Int){ def elementType: String = &quot;INVALID&quot;; } . defined class Element . class Triangle(id: Int) extends Element(id){ def elementType: String = &quot;TRI&quot; } . cmd3.sc:2: `override` modifier required to override concrete member: def elementType: String (defined in class Element) def elementType: String = &#34;TRI&#34; ^Compilation Failed . Compilation Failed . class Triangle(id: Int) extends Element(id){ override def elementType: String = &quot;TRI&quot; } . defined class Triangle . val tri = new Triangle(10) . tri: Triangle = ammonite.$sess.cmd3$Helper$Triangle@1266c3c3 . print(tri.elementType) . TRI . This requirement can potentially be very useful [1]: . When we misspell the name of the method that we are overriding | When we accidentally provide a wrong parameter type in the overriding method | When we introduce a new method in a superclass that clashes with a subclass method | . As a final note, we can invoke a superclass method in Scala works in the same way we do in Java that is using the keyword super . class TriQuad(id: Int) extends Triangle(id){ override def elementType: String = super.elementType + &quot;_QUAD&quot; } . defined class TriQuad . val quad = new TriQuad(10) . quad: TriQuad = ammonite.$sess.cmd6$Helper$TriQuad@69fd55e1 . print(quad.elementType) . TRI_QUAD . isInstanceOf . Frequently, we want to know is an object is an instance of a class. To do so we can use the isInstanceOf method [1] . val q1 = new TriQuad(1) . q1: TriQuad = ammonite.$sess.cmd6$Helper$TriQuad@31d67ba6 . if(q1.isInstanceOf[TriQuad]) println(&quot;This is TRI_QUAD&quot;) . This is TRI_QUAD . The q1.isInstanceOf[TriQuad] will succeed if q1 refers to an object of class TriQuad. Since TriQuad is a subclass of Element the following also succeeds . if(q1.isInstanceOf[Element]) println(&quot;This is TRI_QUAD&quot;) . This is TRI_QUAD . inst.isInstanceOf[Type] returns false if inst is null: . val q2 = null . q2: Null = null . if(q2.isInstanceOf[Element]) println(&quot;This is TRI_QUAD&quot;) else println(&quot;Object is null&quot;) . Object is null . We can further use the asInstanceOf to convert a reference to a subclass reference: . val q2 = q1.asInstanceOf[Element] . q2: Element = ammonite.$sess.cmd6$Helper$TriQuad@31d67ba6 . print(q2.elementType) . TRI_QUAD . If q2 is not an Element , then we get an exception . class SomeClass{} . defined class SomeClass . val q2 = new SomeClass . q2: SomeClass = ammonite.$sess.cmd16$Helper$SomeClass@3f85fd8 . val q3 = q2.asInstanceOf[Element] . java.lang.ClassCastException: class ammonite.$sess.cmd16$Helper$SomeClass cannot be cast to class ammonite.$sess.cmd2$Helper$Element (ammonite.$sess.cmd16$Helper$SomeClass and ammonite.$sess.cmd2$Helper$Element are in unnamed module of loader ammonite.runtime.SpecialClassLoader @7bd4937b) ammonite.$sess.cmd18$Helper.&lt;init&gt;(cmd18.sc:1) ammonite.$sess.cmd18$.&lt;clinit&gt;(cmd18.sc:7) . If we want to test whether an instance refers to a specific class, but not a subclass, we can use classOf [1] . if (p.getClass == classOf[Element]) . The classOf method is defined in the scala.Predef object that is always imported. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/inheritance/programming/oop/2021/05/17/scala-inheritance-1.html",
            "relUrl": "/scala/inheritance/programming/oop/2021/05/17/scala-inheritance-1.html",
            "date": " • May 17, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Prototypical Networks",
            "content": "",
            "url": "https://pockerman.github.io/qubit_opus/2021/05/17/prototypical-networks.html",
            "relUrl": "/2021/05/17/prototypical-networks.html",
            "date": " • May 17, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "Scala Enumerations",
            "content": "Overview . Enumerations can be very useful when we want to create discrete set of items. Often these items help us to differentiate a run time instances having the same base class. Scala provides an Enumeration helper class that we can use to create enumerations[1]. . Enumerations . Scala does not have enumerated types [1]. In contrast, it provides the Enumeration helper class to help us create enumerations. This is shown in the code snippet below . object Element extends Enumeration{ val QUAD, TRI, HEX, TET = Value } . defined object Element . Above we defined an enumerated type with four fields. The above initialization is equivalent to [1] . ... val QUAD = Value val TRI = Value val HEX = Value val TET = Value ... . println(Element.QUAD) println(Element.TRI) . QUAD TRI . Each call to the Value method returns a new instance of an inner class, also called Value [1]. We can also initialize the enumeration fields with ids, names or both as shown below . object Element_2 extends Enumeration{ val QUAD = Value(0, &quot;QUAD&quot;) val TRI = Value(1, &quot;TRI3&quot;) } . defined object Element_2 . println(Element_2.QUAD) println(Element_2.TRI) . QUAD TRI3 . If not specified, the id is one more than the previously assigned one, starting with zero and the default name is the field name [1]. . Note that the type of the enumeration is Element.Value and not just Element. The latter is just the type of the object holding the values. We can use aliases to disambiguate this [1] . object Element_3 extends Enumeration{ type Element_3 = Value val QUAD = Value(0, &quot;QUAD&quot;) val TRI = Value(1, &quot;TRI3&quot;) } . defined object Element_3 . Now the type of the enumeration is Element_3.Element_3 [1]. . for( e &lt;- Element_3.values) println(e.id + &quot;:&quot; + e) . 0:QUAD 1:TRI3 . Finally, you can look up an enumeration value by its id or name [1]. Both of the following yield the object Element.HEX : . println(Element(2)) println(Element.withName(&quot;HEX&quot;)) . HEX HEX . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/enumerations/programming/2021/05/04/scala-enumerations.html",
            "relUrl": "/scala/enumerations/programming/2021/05/04/scala-enumerations.html",
            "date": " • May 4, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "Testing for Controllability",
            "content": "Overview . We have introduced controllability and observability for linear time-invariant systems. We saw the conditions for such a system is controllalble and observable. Now we turn to the question how can we test is controllable and observable. . Testing for controllability . Recall that we deal with linear systems of the form . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . We can understand whether the linear system above is controllable or not by examing the controllability matrix $ mathbf{ cal{C}}$. In particular, the column space of that matrix. This matrix is defined as follows . $$ mathbf{ cal{C}} = begin{bmatrix} mathbf{B} &amp;&amp; mathbf{AB} &amp;&amp; mathbf{A}^2 mathbf{B} &amp;&amp; dots &amp;&amp; mathbf{A}^{n-1} mathbf{B} end{bmatrix}$$ . Where $n$ is the number of state variables. If the controllability matrix has $n$ linearly independent columns, then the system under consideration is controllable [1]. Note that this does not mean that the columns of $ mathbf{ cal{C}}$ should be linearly independent. All that we require, is that we can find $n$ linearly independent columns (see example 2 below). Let&#39;s see two examples taken from [1]. . The Popov-Belevich-Hautus or PBH is one of the most useful tests to determine whether or not a system is contollable [1]. The test says that the pair $( mathbf{A}, mathbf{B})$ is controllable if and only if the column rank of the matrix . $$ begin{bmatrix} mathbf{A} - lambda mathbf{I} &amp;&amp; mathbf{B} end{bmatrix}$$ . is equal to $n$ $ forall lambda in mathbb{C}$. Indeed the rank of $ mathbf{A} - lambda mathbf{I}$ is $n$ only when $ lambda$ is an eigenvalue of $ mathbf{A}$ [1]. Given that the $ mathbf{A} - lambda mathbf{I}$ is only rank deficient for the eigenvalues $ lambda$, then the kernel or null-space of $ mathbf{A} - lambda mathbf{I}$ is given by the span of the eigenvectors corresponding to $ lambda$ [1]. Therefore, for the matrix . $$ begin{bmatrix} mathbf{A} - lambda mathbf{I} &amp;&amp; mathbf{B} end{bmatrix}$$ . to have rank $n$, the columns in $ mathbf{B}$ must have some component in each of the eigenvector directions of $ mathbf{A}$ so that to complment the null space $ mathbf{A} - lambda mathbf{I}$ [1] . If $ mathbf{A}$ has $n$ distinct eigenvalues, then the system will be controllable with a single actuation input. In this call the matrix $ mathbf{A} - lambda mathbf{I}$ has at most one eigenvector direction in the null-space [1]. We can choose $ mathbf{B}$ as the sum of all $n$ linearly independent eigenvectors. This will guarantee to have some component in each direction. . Obviously, cases exist where we have degenerate eigenvalues with multiplicity greater than 2. The actuation input i.e. matrix $ mathbf{B}$ must then have multiple columns [1]. One more case where more it may be helpful to have multiple actuators is when we need better control of the system or when dealing with systes with large transient growth [1]. . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/robotics/autonomous-vehicles/controllability/dynamical-systems/controllability-test/2021/05/02/testing-for-controllability.html",
            "relUrl": "/robotics/autonomous-vehicles/controllability/dynamical-systems/controllability-test/2021/05/02/testing-for-controllability.html",
            "date": " • May 2, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "The Scala apply Method",
            "content": "Overview . The apply method is called in expression of the form Object(arg1,...,argN) [1]. . The apply method . We have seen that we can write both experssions . val arr1 = new Array[Int](10) . arr1: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) . val arr2 = Array(10) . arr2: Array[Int] = Array(10) . The first expression creates an Array of length 10. In this expression the constructor is called. In contrast, in the second expression, the apply method is called. And an Array instance is created having length one and value . println(arr2(0)) . 10 . Not having the new keyword is handy for nested expressions, like the one below . val arr3 = Array(Array(1, 7), Array(2, 9)) . arr3: Array[Array[Int]] = Array(Array(1, 7), Array(2, 9)) . In order to be able to use expressions like the above, the apply method must be defined [1]. We can do this as shown below [1] . class MyCls (val idx: Int, val value: Double){ } object MyCls{ def apply(idx: Int, value: Double) = new MyCls(idx, value) } . defined class MyCls defined object MyCls . val cls = MyCls(1, 20) . cls: MyCls = ammonite.$sess.cmd5$Helper$MyCls@5e0ad6c6 . println(cls.idx) println(cls.value) . 1 20.0 . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/apply-method/programming/2021/04/30/scala-apply-method.html",
            "relUrl": "/scala/classes/apply-method/programming/2021/04/30/scala-apply-method.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "More On Scala object",
            "content": "Overview . We have seen how we can create singletons and companion objects using object. In this notebook we introduce more things we can do with object. . More on Scala object . An object can extend one class. However, it can extend one or more traits [1]. This results in an object that has all of the features specified in the object definition [1]. One utilization of this pattern is to specify default objects as shown below. . abstract class Element(val idx: Int){ def nFaces: Int; def nVertices: Int; } . defined class Element . object DummyElement extends Element(-1){ override def nFaces: Int = -1 override def nVertices: Int = -1 } . defined object DummyElement . Whenever we want to use an Element that makes no sense but anyway it is needed we can use DummyElement. . Application objects . Just like Java and C++, a Scala application starts with a main method which has the following signature [1] . def main(args: Array[String]): Unit . We can wrap that in a companion object . class Hello{ def showMsg() = println(&quot;Hello...&quot;) } object Hello{ def main(args: Array[String]){ val msg = new Hello msg.showMsg() } } . defined class Hello defined object Hello . Note that we can also extend the App trait and place the program code into the constructor body [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/companion-object/programming/2021/04/29/scala-object.html",
            "relUrl": "/scala/classes/companion-object/programming/2021/04/29/scala-object.html",
            "date": " • Apr 29, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "Scala Companion Objects",
            "content": "Overview . Scala does not support static functions. We saw that object classes can be used to implement patterns like the singleton pattern. Moreover, often it make more sense that a function is a class function. We can do this using companion objects. . Companion objects . A companion object has the same name as the class it refers to. It is implemented using the object keyword . object MyClass{ private var currentIndex = 0 def getNewIndex : Int = {currentIndex +=1; currentIndex} } . defined object MyClass . class MyClass{ val id = MyClass.getNewIndex def getIdx: Int = id } . defined class MyClass . val cls = new MyClass println(cls.getIdx) . 1 . cls: MyClass = ammonite.$sess.cmd5$Helper$MyClass@38c61f45 . Both the class and its companion object can access each other’s private features. Furthermore, they must be located in the same source file [1]. . Note that the companion object of a class is accessible, but it is not in scope [1]. This means that in the example above we need to use MyClass.getNewIndex and not just getNewIndex to invoke the method of the companion object [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/companion-object/programming/2021/04/27/scala-companion-objects.html",
            "relUrl": "/scala/classes/companion-object/programming/2021/04/27/scala-companion-objects.html",
            "date": " • Apr 27, 2021"
        }
        
    
  
    
        ,"post31": {
            "title": "Scala Singletons",
            "content": "Overview . Often in software modeling we need to represent an entity that it does not make sense to have more than one instances throughout program execution. We call these objects singletons. Typically, singletons are modeled using static functions. Scala does not support static functions. Instead we use the object construct [1]. . Singletons . An object defines a single instance of a class with the features we want. For example . object Counter{ private var theCounter = 0 def getNewCounter : Int = {theCounter +=1; theCounter} } . defined object Counter . When the application requires a new counter, simply calls Counter.getNewCounter . println(&quot;New counter &quot; + Counter.getNewCounter) . New counter 1 . The constructor of an object is executed when the object is first used [1]. If an object is never used, its constructor is, obviously, not executed [1]. . An object can have all the features of a class, including extending other classes or traits [1]. However, an object cannot have a constructor with parameters. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/singletons/programming/2021/04/26/scala-singletons.html",
            "relUrl": "/scala/classes/singletons/programming/2021/04/26/scala-singletons.html",
            "date": " • Apr 26, 2021"
        }
        
    
  
    
        ,"post32": {
            "title": "Observability",
            "content": "Overview . Using full state feedback i.e. $ mathbf{u} = - mathbf{K} mathbf{x}$, we can modify the behavior of a controllable system. However, it is not always possible to have full-state measurements of the state vector $ mathbf{x}$. In this case, we have to estimat it. This is only possible when the system observable [1]. In this post we will have a brief view of observability. . Observability . Recall that we deal with linear systems of the form . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . In this case, observability is similar to controlability [1]. Briefly, a system is observable if it is possible to estimate any state $ boldsymbol{ xi} in mathbb{R}^n$ from a history of measurements $ mathbf{y}(t)$ [1]. The observability matrix $ mathbf{ cal{O}}$ allows us to determin entirely whether a system is observable or not [1]. It is defined as . $$ mathcal{ cal{O}} = begin{bmatrix} mathbf{C} mathbf{CA} mathbf{C} mathbf{A}^2 vdots mathbf{C} mathbf{A}^{n-1} end{bmatrix}$$ . Where $n$ is the number of state variables. Specifically, if the rows of the matrix span $ mathbb{R}^n$ then it is possible to estimate any full-dimensional state vector $ mathbf{x} in mathbb{R}^n$ from the time-history of $ mathbf{y}(t)$ [1]. . If a system is observable, then it is possible to design the eignevalues of the estimated dynamics to have properties such as noise attenuation and fast estimation [1]. Finally, note that observability matrix is the transpose of the controllability matrix $ mathbf{ cal{C}}$. . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/dynamical-systems/linear-systems/control/observability/2021/04/25/observability.html",
            "relUrl": "/dynamical-systems/linear-systems/control/observability/2021/04/25/observability.html",
            "date": " • Apr 25, 2021"
        }
        
    
  
    
        ,"post33": {
            "title": "Value Iteration With Scala",
            "content": "Overview . When policy evaluation is stopped after just one update of each state, the algorithm is called value iteration. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps [1]. This post looks at the value iteration algorithm. . Value iteration . One drawback of policy iteration is that each of its iterations involves a policy evaluation. This, however, may itself be an iterative computation; therefore requiring multiple sweeps through the state set [1]. Furthermore, if the evaluation is done iteratively, then convergence to $V_{ pi}$ occurs only in the limit [1]. . Given the above limitations of policy iterations, the question posed is whether we could we stop earlier? [1]. Luckily, the policy evaluation step of policy iteration can truncated without loosing the convergence gurantees of the method. Moreover, this can be done in several ways [1]. . In particular, when policy evaluation is stopped after just one update of each state, the algorithm is called value iteration. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps [1] . $$V_{k+1}(s) = max_{ alpha} sum_{s^*, r}p(s^*, r | s, alpha) left[r + gamma V_{k}(s^*) right], ~~ forall s in mathbb{S}$$ . Figure 1: Value iteration algorithm. Image from [1]. . The value iteration is obtained simply by turning the Bellman optimality equation into an update rule [1]. It requires the maximum to be taken over all the actions. Furthermore, the algorithm terminates by checking the amount of change of the value function. . Code . import scala.collection.mutable.ArrayBuffer import scala.util.control.Breaks._ import scala.math.max . import scala.collection.mutable.ArrayBuffer import scala.util.control.Breaks._ import scala.math.max . object Grid{ class State(val idx: Int){ val neigbors = new ArrayBuffer[Int]() for(i &lt;- 0 until 4){ neigbors += -1 } def addNeighbors(neighbors: Array[Int]): Unit = { require(neighbors.length == 4) for(n &lt;- 0 until neighbors.length){ addNeighbor(n, neighbors(n)) } } def addNeighbor(idx: Int, nIdx: Int): Unit = { require(idx &lt; 4) neigbors(idx) = nIdx } def getNeighbor(idx: Int): Int = { require(idx &lt; 4) return neigbors(idx) } } } . defined object Grid . class Grid{ val states = new ArrayBuffer[Grid.State]() def nStates : Int = states.length def nActions: Int = 4 def envDynamics(state: Grid.State, action: Int): (Double, Int, Double, Boolean) = { (0.25, states(state.idx).getNeighbor(action), 1.0, false) } def getState(idx: Int): Grid.State = { require(idx &lt; nStates) states(idx) } def create(): Unit = { // add a new state for(s &lt;- 0 until 9){ states += new Grid.State(s) if(s == 0){ states(s).addNeighbors(Array(0, 1, 3, 0)) } else if(s == 1){ states(s).addNeighbors(Array(1, 2, 4, 0)) } else if(s == 2){ states(s).addNeighbors(Array(2, 2, 5, 1)) } else if(s == 3){ states(s).addNeighbors(Array(0, 4, 6, 3)) } else if(s == 4){ states(s).addNeighbors(Array(1, 5, 7, 3)) } else if(s == 5){ states(s).addNeighbors(Array(2, 5, 8, 4)) } else if(s == 6){ states(s).addNeighbors(Array(3, 7, 6, 6)) } else if(s == 7){ states(s).addNeighbors(Array(4, 8, 7, 6)) } else if(s == 8){ states(s).addNeighbors(Array(5, 8, 8, 7)) } } } } . defined class Grid . class ValueIteration(val numIterations: Int, val tolerance: Double, val gamma: Double){ val valueF = new ArrayBuffer[Double]() var residual = 1.0 def train(grid: Grid): Unit = { valueF.clear() for(i &lt;- 0 until grid.nStates){ valueF += 0.0 } breakable { for(itr &lt;- Range(0, numIterations)){ println(&quot;&gt; Learning iteration &quot; + itr) println(&quot;&gt; Learning residual &quot; + residual) step(grid) if(residual &lt; tolerance) break; } } } def step(grid: Grid): Unit = { var delta: Double = 0.0 for(sIdx &lt;- 0 until grid.nStates){ // Do a one-step lookahead to find the best action val lookAheadVals = this.one_step_lookahead(grid, grid.getState(sIdx)) val maxActionValue = lookAheadVals.max delta = max(delta, (maxActionValue - valueF(sIdx).abs)) // # Update the value function. Ref: Sutton book eq. 4.10. valueF(sIdx) = maxActionValue } this.residual = delta } // Helper function to calculate the value for // all action in a given state. // Returns a vector of length grid.nActions containing // the expected value of each action. def one_step_lookahead(grid: Grid, state: Grid.State): ArrayBuffer[Double] = { val values = new ArrayBuffer[Double](grid.nActions) for(i &lt;- 0 until grid.nActions){ values += 0.0 } for(i &lt;- 0 until grid.nActions){ val (prob, next_state, reward, done) = grid.envDynamics(state, i) val oldVal = values(i) values(i) = oldVal + prob * (reward + this.gamma * valueF(next_state)) } values } } . defined class ValueIteration . val grid = new Grid grid.create() . grid: Grid = ammonite.$sess.cmd2$Helper$Grid@794b701a . val valueFunction = new ValueIteration(100, 1.0e-4, 1.0) . valueFunction: ValueIteration = ammonite.$sess.cmd29$Helper$ValueIteration@44c65bd3 . valueFunction.train(grid) . &gt; Learning iteration 0 &gt; Learning residual 1.0 &gt; Learning iteration 1 &gt; Learning residual 0.3330078125 &gt; Learning iteration 2 &gt; Learning residual 0.078125 &gt; Learning iteration 3 &gt; Learning residual 0.0048828125 &gt; Learning iteration 4 &gt; Learning residual 3.0517578125E-4 . References . Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. |",
            "url": "https://pockerman.github.io/qubit_opus/scala/reinforcement-learning/algorithms/dynamic-programming/2021/04/23/scala-value-iteration.html",
            "relUrl": "/scala/reinforcement-learning/algorithms/dynamic-programming/2021/04/23/scala-value-iteration.html",
            "date": " • Apr 23, 2021"
        }
        
    
  
    
        ,"post34": {
            "title": "Scala Classes 3",
            "content": "Overview . In this third part regarding Scala classes we review nested classes. . Scala Classes 3 . In Scala, you can nest just about anything inside anything [1]. For example we can define functions inside other functions, and classes inside other classes. We discuss the latter feature here. . import scala.collection.mutable.ArrayBuffer . import scala.collection.mutable.ArrayBuffer . class Grid{ class Element(val idx: Int){ val children = new ArrayBuffer[Element] } private val elements = new ArrayBuffer[Element] def addElement() : Unit = { val element = new Element(this.elements.size) elements += element } def addElement(element: Element){ elements += element } def nElements(): Int = this.elements.size } . defined class Grid . val grid = new Grid . grid: Grid = ammonite.$sess.cmd10$Helper$Grid@662422e8 . grid.addElement() . println(&quot; Number of elements &quot; + grid.nElements) . Number of elements 1 . In Scala, each instance has its own class Element , just like each instance has its own field members [1]. You can see this below . val grid_2 = new Grid . grid_2: Grid = ammonite.$sess.cmd10$Helper$Grid@6116d1b9 . grid.addElement(new grid_2.Element(0)) . cmd15.sc:1: type mismatch; found : cmd15.this.cmd14.grid_2.Element required: cmd15.this.cmd11.grid.Element val res15 = grid.addElement(new grid_2.Element(0)) ^Compilation Failed . Compilation Failed . Assuming that grid holds quad elemennts and grid_2 holds triangular elements this behaviour makes sense. However, we may want to remove this behaviour. We can do this in two ways [1] . Use a companion object | Use type projection | . object Grid{ class Element(val idx: Int){ val children = new ArrayBuffer[Element] } } . defined object Grid . class Grid{ private val elements = new ArrayBuffer[Grid.Element] def addElement() : Unit = { val element = new Grid.Element(this.elements.size) elements += element } def addElement(element: Grid.Element){ elements += element } def nElements(): Int = this.elements.size } . defined class Grid . With type projection, we write our class as shown below . class Grid{ class Element(val idx: Int){ val children = new ArrayBuffer[Element] } private val elements = new ArrayBuffer[Element] def addElement() : Unit = { val element = new Element(this.elements.size) elements += element } def addElement(element: Element){ elements += element } def nElements(): Int = this.elements.size } . defined class Grid . Type projection means, for our case, Element from any Grid . val grid = new Grid val grid2 = new Grid . grid: Grid = ammonite.$sess.cmd19$Helper$Grid@3bc5f2f5 grid2: Grid = ammonite.$sess.cmd19$Helper$Grid@5f5eafec . grid.addElement(new grid_2.Element(0)) . cmd21.sc:1: type mismatch; found : cmd21.this.cmd14.grid_2.Element required: cmd21.this.cmd20.grid.Element val res21 = grid.addElement(new grid_2.Element(0)) ^Compilation Failed . Compilation Failed . Finally, in a nested class, we can access the this reference of the enclosing class as EnclosingClass.this [1]. This is similar to Java. Furthermore, we can establish an alias for that reference as shown below . class Grid{ outer =&gt; class Element(val idx: Int){ val children = new ArrayBuffer[Element] } private val elements = new ArrayBuffer[Element] def addElement() : Unit = { val element = new Element(this.elements.size) elements += element } def addElement(element: Element){ elements += element } def nElements(): Int = this.elements.size } . defined class Grid . The class Grid{ outer =&gt; syntax makes the variable outer refer to Grid.this. Note that we can choose any name for this variable. The name self is common, but perhaps confusing when used with nested classes [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/programming/2021/04/21/scala-classes-3.html",
            "relUrl": "/scala/classes/programming/2021/04/21/scala-classes-3.html",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post35": {
            "title": "Scala Classes 2",
            "content": "Overview . This post looks into primary and auxiliary constructors for Scala classes. . Scala Classes 2 . A Scala class can have as many constructors as we want. A Scala class has a so called primary constructor. Additionally, a class can have any number of auxiliary constructors. . Primary constructor . A Scala class has a so called primary constructor. The primary constructor is not defined with a this method. This is interwoven with the class definition [1]. . class Person(val name: String, val age: Int){ } . defined class Person . Parameters of the primary constructor turn into fields that are initialized with the construction parameters. . val p = new Person(&quot;Alex&quot;, 10) . p: Person = ammonite.$sess.cmd10$Helper$Person@62b0e31f . println(p.name + &quot; has age &quot; + p.age) . Alex has age 10 . The primary constructor executes all statements in the class definition. This is shown below . class AnotherClass(val val1: Double, val val2: Int){ show() def show()={ println(val1 + &quot;, &quot; + val2) } } . defined class AnotherClass . val cls = new AnotherClass(20.0, 10) . 20.0, 10 . cls: AnotherClass = ammonite.$sess.cmd13$Helper$AnotherClass@1ea96977 . The show() function call is a part of the primary constructor. It will be called every time a new object is created [1]. Moreover, if there are no parameters after the class name, then the class has a primary constructor with no parameters. That constructor simply executes all statements in the body of the class [1]. . The primary constructor of the AnotherClass declares and initializes the following fields . val val1: Double val val2: Int . Since the primary constructor parameters are declared with the private keyword, the getter function is public. Moreover only a getter is generated as both variables are declared with val. . Construction parameters can also be regular method parameters, without val or var . How these parameters are processed depends on their usage inside the class[1]. . If a parameter without val or var is used inside at least one method, it becomes a field. . class AnotherClass_2(val1: Double, val2: Int){ def description = val1 + &quot; , &quot; + val2 } . defined class AnotherClass_2 . The primary constructor above, declares and initializes immutable fields val1 and val2 that are object-private i.e. instances of the same class do not have access to these fields of another instance from the same class. . Otherwise, the parameter is not saved as a field. Meaning it is just a regular parameter that can be accessed in the code of the primary constructor [1]. . Finally, sometimes we may want to declare a primary constructor as private. We can do so as shown below . class AnotherClass_3 private(val1: Double, val2: Int){ def description = val1 + &quot; , &quot; + val2 } . defined class AnotherClass_3 . A class user must then use an auxiliary constructor to construct a AnotherClass_3 object [1]. . Auxiliary constructors . Auxiliary constructs are called this [1]. Each such constructor should start with a call to a previously defined auxiliary constructor or the primary constructor. . class MyCls{ private var name = &quot;&quot; private var age = 0 def this(name: String){ // first call primary constructor this() this.name = name } def this(name: String, age: Int){ this(name) //set also the age this.age = age } } . defined class MyCls . The first auxiliary constructor, i.e. this(name: String), calls the empty primary construtor this(). For a class we do not define a primary constructor has a primary constructor with no arguments [1]. . val cls1 = new MyCls val cls2 = new MyCls(&quot;Alex&quot;) val cls3 = new MyCls(&quot;Alex&quot;, 10) . cls1: MyCls = ammonite.$sess.cmd17$Helper$MyCls@78f51c1b cls2: MyCls = ammonite.$sess.cmd17$Helper$MyCls@4cf2a04c cls3: MyCls = ammonite.$sess.cmd17$Helper$MyCls@5ece3c6c . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/programming/2021/04/20/scala-classes-2.html",
            "relUrl": "/scala/classes/programming/2021/04/20/scala-classes-2.html",
            "date": " • Apr 20, 2021"
        }
        
    
  
    
        ,"post36": {
            "title": "Scala Classes 1",
            "content": "Overview . The next stop in the Scala journey is classes. Classes are the cornerstone of object oriented programming. In the simplest form, a Scala class looks similar to classes in Java and C++. . Scala Classes 1 . In the simplest case, a class will model an entity of the modeled domain. It will expose an API that application code can use. Private fields correspond to the state of the object modelled after a class. The following snippet shows how to create a class that has a private field named value, a getter function and a modifier function. . class MyClass { private var value = 0.0 def increment() = value += 1.0 def getValue = value } . defined class MyClass . Specific instances can be used by using the new operator. For example, the following creates an instance of MyClass. . val inst1 = new MyClass . inst1: MyClass = ammonite.$sess.cmd5$Helper$MyClass@5743442d . println(inst1.getValue) . 0.0 . inst1.increment() . println(inst1.getValue) . 1.0 . Getters and setters are frequently used to change properties of a class. Alghtough, such methods allow for every client of the class to modify the state of the class, they are the preferred way of doing so. This is because they allow us to control how the change of the class state is done. Scala can generate getters and setters for us for every private field of our class [1]. . class MyClass2{ var value = 0.0 } . defined class MyClass2 . val inst2 = new MyClass2 . inst2: MyClass2 = ammonite.$sess.cmd10$Helper$MyClass2@52cac8e8 . print(inst2.value) . 0.0 . inst2.value = 10.0 . print(inst2.value) . 10.0 . . Remark . In Scala, the getters and setters are not named as getXxx and setXxx. Instead, they take the name of the private variable [1]. Note also that if we don&#39;t want the getters and setters to be generated then we should declare this field with private[this]. . . Although Scala defines getters and setters for us, we can redefine the getter and setter methods generated for us [1]. This is shown below . class MyClass3{ var myValue = 0.0 def value = myValue def value_=(newValue: Double) : Unit = { // we can control how the state is // changed here if(newValue &gt; 10.0) myValue = 5 else myValue = newValue } } . defined class MyClass3 . val inst3 = new MyClass3 . inst3: MyClass3 = ammonite.$sess.cmd0$Helper$MyClass3@1384d29d . println(inst3.value) . 0.0 . inst3.value = 3.0 . println(inst3.value) . 3.0 . inst3.value = 50 println(inst3.value) . 5.0 . Here are a few points regarding setters and getters to remember [1] . If the field is private then both functions are private | If the field is declared using val, then only a getter is generated | If no getter or setter is needed for the field, declare this as private[this] | We cannot have a write-only field i.e a field with a setter and no getter | . Finally note that, in contrast to Java, a class source file in Scala can contain multiple classes. Moreover, we do not need to declare a Scala class as public [1]. . private[this] . Every instance of a class has access to the private members of instances of the same class. For example, the following is legal . class ShowMeMsg{ private var msg = &quot;Nothing&quot; def setMsg(newMesg: String) : Unit = msg=newMesg def isMessage(other: ShowMeMsg) = { if(other.msg == this.msg){ println(&quot;It is message&quot;) } } } . defined class ShowMeMsg . var msg1 = new ShowMeMsg msg1.setMsg(&quot;ONE&quot;) . msg1: ShowMeMsg = ammonite.$sess.cmd37$Helper$ShowMeMsg@4e22d967 . var msg2 = new ShowMeMsg msg2.setMsg(&quot;ONE&quot;) . msg2: ShowMeMsg = ammonite.$sess.cmd37$Helper$ShowMeMsg@7a1f4e7 . msg1.isMessage(msg2) . It is message . Thus, msg1 has access to the field of msg2. This means that if the field is of var type mgs1 could potentially change it. Certainly, this is not a desired scenario most of the times. We can prevent this sort of situation by using the private[this] qualifier. This allows an even more severe access restriction [1]. This is shown below. . class ShowMeMsg_2{ private[this] var msg = &quot;Nothing&quot; def setMsg(newMesg: String) : Unit = msg=newMesg def isMessage(other: ShowMeMsg) = { if(other.msg == this.msg){ println(&quot;It is message&quot;) } } } . cmd41.sc:7: variable msg in class ShowMeMsg cannot be accessed as a member of cmd41.this.cmd37.ShowMeMsg from class ShowMeMsg_2 in class Helper if(other.msg == this.msg){ ^Compilation Failed . Compilation Failed . Summary . This was a brief introduction to Scala classes. To a large extent classes in Scala are similar to classes in Java or C++. . Specifically, we touched on how to create simple classes in Scala, getters and setters and the private[this] qualifier. The getters and setters will be private if the field they correspond to is declared as such. The private[this] qualifier is even more restricitve; not even instances of the same class can access the field. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/programming/2021/04/18/scala-classes-1.html",
            "relUrl": "/scala/classes/programming/2021/04/18/scala-classes-1.html",
            "date": " • Apr 18, 2021"
        }
        
    
  
    
        ,"post37": {
            "title": "Linear Methods",
            "content": "Linear methods . In this article, we will continue our function approximation journey by introducing linear models for representing the state value function $V_{ pi}(S_t)$. Recall that in the case of large state spaces it is advantageous to use some sort of parametic funtion approximation rather than a tabular representation of the state value function. Furthermore, we need a model in order to to perform, for examle, the SGD update step given below . $$ mathbf{w}_{t+1} = mathbf{w}_t + eta left[V_{ pi}(S_t)- hat{V}_{ pi}(S_t, mathbf{w}_t) right ] nabla hat{V}(S_t, mathbf{w}_t)$$ . One of the simplest representations we can have, and the one we take in this post, is a linear model with resepct to the weights $ mathbf{w}$. In this case, $ hat{V}(s, mathbf{w})$ becomes . $$ hat{V}(s, mathbf{w}) = sum_{i=1}^{d} w_i x_i(s) = mathbf{w}^T mathbf{x}(s)$$ . The expression above implies that there is a vector $ mathbf{x}(s)$ for every state having the same number of components as the weights vector. The vector $ mathbf{x}(s)$ represents the features of the state $s$. For example for an autonomous vehicle, $ mathbf{x}(s)$ may correspond to the vector with components such as vehicle velocity, vehicle acceleration, vehicle position, vehicle orientatio, gas level e.t.c. Technically, each component $x_i$ represents a function such that $x_i: mathbb{S} rightarrow mathbb{R}$ [1]. For linear methods, features are basis functions because they form a linear basis for the set of approximate functions [1]. . Linear models have a straightforward gradient calculation. Indeed in this case . $$ nabla hat{V}(S_t, mathbf{w}_t) = mathbf{x}(s)$$ . Therefore, the SGD update step becomes . $$ mathbf{w}_{t+1}= mathbf{w}_t + eta left[V_{ pi}(S_t)- hat{V}_{ pi}(S_t, mathbf{w}_t) right ] mathbf{x}(S_t)$$ . Linear models are, in general, very well understood throughout science. For our case, when using a linear model case there is only one optimum. Therefore, any method that is guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum [1]. . As a final note, observe that we need $V_{ pi}(S_t)$ in order to perform SGD weights update. This many not always be available. We may, however, have in hand an approximation of it, let&#39;s calle it $U_t$ i.e. $V_{ pi}(S_t) approx U_t$. In this scenario, we are forced to use the latter. However, if $U_t$ is an unbiasd estimate, then $ mathbf{w}_t$ is guaranteed to converge to a local optimum under the conditions specified above for decreasing $ eta$. . References . Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. |",
            "url": "https://pockerman.github.io/qubit_opus/linear-model/reinforcement-learning/2021/04/16/rl-linear-methods.html",
            "relUrl": "/linear-model/reinforcement-learning/2021/04/16/rl-linear-methods.html",
            "date": " • Apr 16, 2021"
        }
        
    
  
    
        ,"post38": {
            "title": "Scala Maps",
            "content": "Overview . Maps are collections of key-value pais. Just like arrays, in Scala we can distinguish between mutable and immutable maps. Furthermore, the default map type is a hash map. However, tree maps are also provided [1]. . As mentioned above, in Scala, a map is a collection of key-value pairs. A pair is a grouping of two values that do not have necessarily the same type [1]. We have two ways cosntructing a pair . Using the -&gt; operator. | Using (key, value) constructs | . Immutable map . We can construct an immutable Map as shown below . val map1 = Map(&quot;France&quot; -&gt; &quot;Paris&quot;, &quot;England&quot; -&gt; &quot;London&quot;, &quot;Greece&quot; -&gt; &quot;Athens&quot;) . map1: Map[String, String] = Map( &#34;France&#34; -&gt; &#34;Paris&#34;, &#34;England&#34; -&gt; &#34;London&#34;, &#34;Greece&#34; -&gt; &#34;Athens&#34; ) . The elements in Map cannot be changed . map1(&quot;Greece&quot;) = &quot;New York&quot; . cmd1.sc:1: value update is not a member of scala.collection.immutable.Map[String,String] did you mean updated? val res1 = map1(&#34;Greece&#34;) = &#34;New York&#34; ^Compilation Failed . Compilation Failed . Nevertheless, the following is a way to update an immmutable map . val map2 = map1 + (&quot;Greece&quot; -&gt; &quot;New York&quot;) . map2: Map[String, String] = Map( &#34;France&#34; -&gt; &#34;Paris&#34;, &#34;England&#34; -&gt; &#34;London&#34;, &#34;Greece&#34; -&gt; &#34;New York&#34; ) . Similarly, we can remove or add a new element . val map3 = map2 + (&quot;Italy&quot; -&gt; &quot;Rome&quot;) . map3: Map[String, String] = Map( &#34;France&#34; -&gt; &#34;Paris&#34;, &#34;England&#34; -&gt; &#34;London&#34;, &#34;Greece&#34; -&gt; &#34;New York&#34;, &#34;Italy&#34; -&gt; &#34;Rome&#34; ) . val map4 = map3 - &quot;Greece&quot; . map4: Map[String, String] = Map( &#34;France&#34; -&gt; &#34;Paris&#34;, &#34;England&#34; -&gt; &#34;London&#34;, &#34;Italy&#34; -&gt; &#34;Rome&#34; ) . Mutable map . In order to get a mutable map we need to explicitly say so . import scala.collection.mutable.Map . import scala.collection.mutable.Map . val grades = Map(&quot;Alex&quot; -&gt; 10, &quot;Alice&quot; -&gt; 15, &quot;George&quot; -&gt;5) . grades: Map[String, Int] = HashMap(&#34;Alex&#34; -&gt; 10, &#34;George&#34; -&gt; 5, &#34;Alice&#34; -&gt; 15) . grades(&quot;Alex&quot;) = 12 . grades . res7: Map[String, Int] = HashMap(&#34;Alex&#34; -&gt; 12, &#34;George&#34; -&gt; 5, &#34;Alice&#34; -&gt; 15) . Above we have initialized the map at construction time. However, we might not always be able to do so. In this case, we need to specify explicitly what type of map we want [1] . import scala.collection.mutable.HashMap . import scala.collection.mutable.HashMap . val gradesEmpty = new HashMap[String, Int] . gradesEmpty: HashMap[String, Int] = HashMap() . If the map is mutable, this means tha we can add, change or remove elements. We can add a new element in two ways . Use operator (key). If the key exists it will update the value corresponding to the key. Otherwise, it will create a new key-value pair | Use operator += followed by a tuple of pairs | . gradesEmpty += (&quot;Suzana&quot; -&gt; 15, &quot;John&quot; -&gt; 3) . res10: HashMap[String, Int] = HashMap(&#34;Suzana&#34; -&gt; 15, &#34;John&#34; -&gt; 3) . We can remove a key-value pait using the -= operator . gradesEmpty -= &quot;Suzana&quot; . res11: HashMap[String, Int] = HashMap(&#34;John&#34; -&gt; 3) . Querying a map . How can we find whether a key is contained in a map? . grades.contains(&quot;Alex&quot;) . res12: Boolean = true . grades.contains(&quot;&quot;) . res13: Boolean = false . One nice feature is that we can query a map with a key and specify a default value in case that the key does not exist . grades.getOrElse(&quot;Alex&quot;, &quot;Invalid Name&quot;) . res14: Any = 12 . grades.getOrElse(&quot;SomeOne&quot;, &quot;Invalid Name&quot;) . res15: Any = &#34;Invalid Name&#34; . As shown above, we can access the value of a particular key using the () operator. This, however, will an exception if the key does not exit. Finally, we can use grades.get( someKey ). This returns an Option object that is either Some( value for key ) or None [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/maps/programming/2021/04/15/scala-maps.html",
            "relUrl": "/scala/maps/programming/2021/04/15/scala-maps.html",
            "date": " • Apr 15, 2021"
        }
        
    
  
    
        ,"post39": {
            "title": "Stochastic Gradient Descent",
            "content": "Stochastic Gradient Descent . Stochastic gradient descent (SGD) methods are among the most widely used of all function approximation methods. Moreover, they are particularly well suited to online reinforcement learning. The article An overview of gradient descent optimization algorithms gives a nice review of gradient descent methods. . Gradient descent methods assume that the function approximation ( in our case this is the approximate value function $ hat{V}(s, mathbf{w})$) is a differentiable function of $ mathbf{w}$. Furthermore, we assume that this is true for $s in mathbb{S}$ . Gradient-descent methods are iterative algorithms. Thus, we denote with $ mathbf{w}_t$ the weight vector at the $t-th$ iteration. Furtheremore, at each iteration, we observe $S_t rightarrow V_{ pi}(S_t)$ i.e. the example consists of a state $S_t$ and its true value under the policy. Note that we can choose the state randomly. These states might be successive states from an interaction with the environment [1]. . Hence, we have in hand $S_t$ and the corresponding state value i.e. $V_{ pi} (S_t)$. However, the problem we now face is that the number of weights is far less than the number of states i.e. our function approximator has a rather limited resolution. In particular, there is generally no $ mathbf{w}$ that gets all the states, or even all the examples, exactly correct. A second problem is that the function approximator must generalize to all the other states that have not appeared yet [1]. . We assume that states appear in examples with the same distribution, $ mu$, over which we are trying to minimize the $MSVE$ given by: . $$MSVE( mathbf{w}) = sum_{s in S} mu(s) left[V_{ pi}(s) - hat{V}_{ pi}(s, mathbf{w}) right]^2$$ . Stochastic gradient-descent (SGD) methods do this by adjusting the weight vector after each example is visited by a small amount in the direction that would most reduce the error on that example. Namely . $$ mathbf{w}_{t+1} = mathbf{w}_t - frac{1}{2} eta nabla left[V_{ pi}(S_t)- hat{V}_{ pi}(S_t, mathbf{w}_t) right ]^2 = mathbf{w}_t + eta left[V_{ pi}(S_t)- hat{V}_{ pi}(S_t, mathbf{w}_t) right ] nabla hat{V}(S_t, mathbf{w}_t)$$ . where $ eta$ is a positive step-size parameter also known as learning rate and . $$ nabla hat{V}(S_t, mathbf{w}) = left( frac{ partial hat{V}(S, mathbf{w})}{ partial w_1}, ..., frac{ partial hat{V}(S, mathbf{w})}{ partial w_m} right)$$ . SGD methods are gradient descent methods because. The latter methods update $ mathbf{w}_t$ by a small amount towards the direction that most reduces the error. The error metric, $MSVE$, that we use here, uses as an error indicator the following quantity . $$e^2 = left[V_{ pi}(s) - hat{V}_{ pi}(s, mathbf{w}) right]^2$$ . This is reduced most rapidly in the direction of $- nabla hat{V}(S_t, mathbf{w})$. . There are various gradient descent methods, see for example [2]. Gradient descent methods are called stochastic when the update is done on only a single example, which might have been selected stochastically. Over many examples, making small steps, the overall effect is to minimize an average performance measure such as the MSVE. [1] . SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily. This is shown in the image below. . Figure 1. SGD fluctuation. Image from [2]. . This overshooting behavior, in general, complicates convergence to the exact minimum. However, by slowly decreasing the learning rate $ eta$ it has been shown that SGD shows the same convergence behaviour as batch gradient descent and almost certainly converges to a local or the global minimum both for convex and non-convex optimization problems. One other problem one may face with the SGD algorithm is that some bias may be introduced as the algorithm updates the weights on a per weights basis. This can be mitigated by suffling the data after each iteration. . Given that SGD does frequent updates that may exhibit high variance why don&#39;t we move in the minimizing direction in one step and therefore completely eliminate the error on the visited example? Altghough many times this can be done, it may not be the right thing to do so. The reason why this is the case, is that the weights are far less than the states. Hence, we should not seek to find a value function that has zero error for all states. Instead, the approximation should balance the errors in the different states [1]. If we completely corrected each example in one step, then we would not find such a balance [1]. Note also that the convergence results for SGD methods assume that the learning rate, $ eta$, decreases over time. Moreover, if it decreases in such a way as to satisfy the following stochastic approximation conditions . $$ sum_{n = 1}^{ infty} eta_n( alpha) = infty ~~ text{and} ~~ sum_{n = 1}^{ infty} eta_{n}^2( alpha) &lt; infty $$ . then the SGD method is guaranteed to converge to a local optimum [1]. . As a final note, observe that we need $V_{ pi}(S_t)$ in order to perform SGD weights update. This many not always be available. We may, however, have in hand an approximation of it, let&#39;s calle it $U_t$ i.e. $V_{ pi}(S_t) approx U_t$. In this scenario, we are forced to use the latter. However, if $U_t$ is an unbiasd estimate, then $ mathbf{w}_t$ is guaranteed to converge to a local optimum under the conditions specified above for decreasing $ eta$ [1]. . References . Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. | An overview of gradient descent optimization algorithms |",
            "url": "https://pockerman.github.io/qubit_opus/stochastic-gradient-descent/gradient-descent/reinforcement-learning/algorithms/2021/04/12/stochastic-gradient-descent.html",
            "relUrl": "/stochastic-gradient-descent/gradient-descent/reinforcement-learning/algorithms/2021/04/12/stochastic-gradient-descent.html",
            "date": " • Apr 12, 2021"
        }
        
    
  
    
        ,"post40": {
            "title": "Controllability",
            "content": "Overview . When we want to control a dynamical system, the natural question that arises is to what extent can we achive this? The term controllability is used to describe whether a systme is controllable altogether. We will see that whether a system is controllable or not is determined completely by the controllability matrix [1] . Controllability . Recall that we deal with linear systems of the form . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . We can understand whether the linear system above is controllable or not by examing the controllability matrix $ mathbf{ cal{C}}$. In particular, the column space of that matrix. This matrix is defined as follows . $$ mathbf{ cal{C}} = begin{bmatrix} mathbf{B} &amp;&amp; mathbf{AB} &amp;&amp; mathbf{A}^2 mathbf{B} &amp;&amp; dots &amp;&amp; mathbf{A}^{n-1} mathbf{B} end{bmatrix}$$ . Where $n$ is the number of state variables. If the controllability matrix has $n$ linearly independent columns, then the system under consideration is controllable [1]. Note that this does not mean that the columns of $ mathbf{ cal{C}}$ should be linearly independent. All that we require, is that we can find $n$ linearly independent columns (see example 2 below). Let&#39;s see two examples taken from [1]. . Example 1 . Let&#39;s consider the following system . $$ frac{d}{dt} begin{bmatrix}x_1 x_2 end{bmatrix} = begin{bmatrix}1 &amp;&amp; 0 0 &amp;&amp; 2 end{bmatrix} begin{bmatrix}x_1 x_2 end{bmatrix} + begin{bmatrix}0 1 end{bmatrix} u$$ . Immediatelly, we can see that the system is not controllable. This is because the two state variables are decoupled and the control input affects only $x_2$. The controllability matrix is . $$ mathbf{ cal{C}} = begin{bmatrix}0 &amp;&amp; 0 1 &amp;&amp; 2 end{bmatrix} $$ . and we can see that the columns of that matrix are not independent. . Example 2 . If we include a control signal for the both state variables, we can turn this system into a controllable one. The new system now is . $$ frac{d}{dt} begin{bmatrix}x_1 x_2 end{bmatrix} = begin{bmatrix}1 &amp;&amp; 0 0 &amp;&amp; 2 end{bmatrix} begin{bmatrix}x_1 x_2 end{bmatrix} + begin{bmatrix}1 &amp;&amp; 0 0 &amp;&amp; 1 end{bmatrix} begin{bmatrix} u_1 u_2 end{bmatrix} u$$ . By including a control signal for bith state variables, we can control them independently. The controllability matrix now becomes [1] . $$ mathbf{ cal{C}} = begin{bmatrix}1 &amp;&amp; 0 &amp;&amp; 1 &amp;&amp; 0 0 &amp;&amp; 1 &amp;&amp; 0 &amp;&amp; 2 end{bmatrix} $$ . and we can verify that the columns of this matrix do span $ mathbb{R}^2$. Note that the columns of the matrix above are not linearly independent. Indeed the third column is a copy of the first and the fourth is a product of the second. However, the first two columns do span $ mathbb{R}^2$. . Three equivalent conditions . The span of the columns of matrix $ mathbf{ cal{C}}$ form a Krylov subspace [1]. The space determines which state vectors can be controlled. Hence, controllability implies two things [1] . Eigenvalue placement | Any state vector $ boldsymbol{ xi} in mathbb{R}^n$ is reachable with some actuation signal | . The following three conditions are equivalent [1] . Controllability | Arbitrary eigenvalue placement | Reachability of $ mathbb{R}^n$ | . We already saw what controllability means in terms of the controllability matrix. Arbitrary eigenvalue placement means that we can design the eigenvalues of the system through the choice of the feedback signal $ mathbf{u}$. In particular, for $ mathbf{u} = - mathbf{K} mathbf{x}$, the system becomes . $$ frac{d mathbf{x}}{dt} = ( mathbf{A} - mathbf{B} mathbf{K}) mathbf{x}$$ . The matrix $ mathbf{K}$ is called the gain matrix. There are various methods to design this matrix. Finally, reachability in practical terms means that we can steer the system to any arbitrary state with some actuation signal, or, conversely, there exists an actuation signal so that the system can be pushed to an arbitrary state $ boldsymbol{ xi} in mathbb{R}^n$ [1]. . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/dynamical-systems/linear-systems/control/controllability/2021/04/11/controllability.html",
            "relUrl": "/dynamical-systems/linear-systems/control/controllability/2021/04/11/controllability.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post41": {
            "title": "Longitudinal Car Model",
            "content": "Longitudinal car model . In this section, we will go over the concept of the vehicle longitudinal dynamics. The two major elements of the longitudinal vehicle model discussed in this section are . Vehicle dynamics | Powertrain dynamics | . The vehicle dynamics are influenced by longitudinal tire forces, aerodynamic drag forces, rolling resistance forces and gravitational forces. The longitudinal powertrain system of the vehicle consists of the internal combustion engine, the torque converter, the transmission and the wheels. This video explains nicely the concepts. . The longitudinal vehicle dynamic model is simply based on the dynamics of the vehicle that generate forward motion. The following figure shows a typical vehicle longitudinal motion force diagram on an inclined road. . Figure 1. Schematics of vehicle logitudinal model on an inclined road. Image from [1]. . We have the followig forces acting on the vehicle . The front tire forces $F_{xf}$ | The rear tire forces $F_{xr}$ | The aerodynamic drag force $F_{aero}$ | The rolling resistance forces $R_{xf}$ and $R_{xr}$ | The force due to gravity $F_g$ | . According to Newton’s laws of motion, and in particular the second law, the longitudinal tire forces of the front and rear tyres, $F_{xf}$ and $F_{xr}$, should balance the resistance forces $F_{aero}$, the gravitational force $F_g$ , and the rolling resistance of the front and rear tires, $R_{xf}$ and $R_{xr}$. Any imbalance between these forces creates an acceleration of the vehicle in the longitudinal direction denoted by $ ddot{x}$. Thus, the basic logintudinal motion model is given by . $$m ddot{x} = F_{xf} + F_{xr} - F_{aero} - F_g - R_{xf} - R_{xr}$$ . where $m$ is the mass of the vehicle. The forces $F_{xf}$ and $F_{xr}$ come from the vehicle power train. We can express them collectively as $F_x$. Furthermore, we group together the rolling resistance forces under the symbol $R_x$. Thus, the reduced model is . $$m ddot{x} = F_x - F_{aero} - F_g - R_x $$ . We will need a way to express the involved quantities in order to be able to solve for $ ddot{x}$. Let&#39;s start with the gravitational force $F_g$. . Gravitational froce . We can express $F_g$ as [2] . $$F_g = mg sin ( alpha)$$ . where $ alpha$ is the local road slope. For small slope angles, we can write . $$sin ( alpha) approx alpha$$ . Aerodynamic drag . A vehicles longitudinal motion is resisted by aerodynamic drag rolling resistance and the force due to gravity. The aerodynamic drag force $F_{aero}$ is typically modeled as dependent on air density $ rho$, frontal area of the vehicle $A$, the vehicles coefficient of friction $C_D$, and the current speed of the vehicle. The functional relationship of all these quantities is given in the equation beow . $$F_{aero} = frac{1}{2}C_D rho A v^2$$ . Rolling resistance . Tires are elastic materials that are subject to deformation in the patch which is in contact with the road surface. Let&#39;s neglect the the deformation of the road. The tire is subject to a normal load. Due to this load, the tire material will be deflected normally at the contact patch and then regaining its shape whilst leaving the patch neighborhood. However, internal damping of the material does not allow the energy lost during deforming the tire to be completely recovered when the material returns to its original shape [1]. It appears therefore, that some loss of energy occurs. This loss is represented by a force on the tires called the rolling resistance that acts in the opposite direction of the motion of the vehicle. . Hence, the rolling resistance depends on the normal tire load, tire pressure and vehicle speed. A model is given below [1], . $$R_x = N(c_{r, 0} + c_{r,1}| dot{x}| + c_{r,2}| dot{x}|^2)$$ . see also [2] for further modelling. If we assume nominal operating conditions and drop the second-order terms for simplicity, we can arrive at a linear rolling resistance model, where $c_{r,1}$ is the linear rolling resistance coefficient. . $$R_x approx c_{r,1}| dot{x}|$$ . Tire forces . We now discuss the longitudinal tire forces expressed under the term $F_x$. Longitudinal tire forces depend on the following factors [2] . Slip ratio | Normal load on the tires | Friction coefficient on the tire road interface | . Let&#39;s see these components . Slip ratio . For an effective wheel radius $R_{effective}$ and a wheel velocity $ omega_w$ the velocity is described by . $$V_{wheel} = R_{effective} omega_{wheel}$$ . However, the actual longitudinal velocity at the axle of the wheel, $V_x$ may be different than that. This is called longitudinal slip [2]. In other words, the longitudinal slip is defined as [2] . $$ sigma = V_{wheel} - V_x$$ . Moreover, we define the longitudinal slip ratio during braking and acceleration as [2] . $$ sigma_{xf} = begin{cases} frac{R_{effecive} omega_{wf} - V_x}{V_{x}}, ~~ text{during breaking} frac{R_{effecive} omega_{wf} - V_x}{R_{effecive} omega_{wf}}, ~~ text{during acceleration} end{cases}$$ . We have a similar expression for the rear wheels. Given the slip coefficients, we can express the longitudinal tire forces as . $$F_{xf} = C_{ sigma f} sigma_{xf}, ~~ F_{xr} = C_{ sigma r} sigma_{xr}$$ . where $C_{ sigma f}$ and $C_{ sigma r}$ are called the longitudinal tire stiffness parameters of the front and rear tires respectively [2]. . Powertrain forces . The longitudinal tire forces, denoted collectivelly above with $F_x$, acting on the driving wheels are the main forces that drive the vehicle forward [2]. These forces depend on the difference between the rotational wheel velocity $R_{effective} omega_{w}$ and the vehicle longitudinal velocity $ dot{x}$. In particular, we saw that we can model the longitudinal tire forces as . $$F_{xf} = C_{ sigma f} sigma_{xf}, ~~ F_{xr} = C_{ sigma r} sigma_{xr}$$ . where $C_{ sigma f}$ and $C_{ sigma r}$ are called the longitudinal tire stiffness parameters of the front and rear tires respectively [2]. However, $ omega_w$ is highly influence by the powertrain dynamics of the vehicle. The powertrain has the following major components [2] . Engine | Transmission or gearbox | Torque converter or clutch | Differential | Wheels | . Figure 2. Powertrain schematics. Image from [1]. . Let&#39;s see each of the components separately . Torque converter . The torque cnverter connects the engine to the transmission. When the engine is turning slowly, e.g. when the car waits at a stoplight, the amount of torque passed through the torque converter is very small. Thus, maintaining the the car stopped requires only a light pressure on the brake pedal. Hence, we don&#39;t have to stall the engine in order to maintain the vehicle stopped. In contrast, when the vehicle accelerates the torque converter gives the car more torque [2]. . The torque converter has the following major components [2] . pump | turbine | transmission fluid | . The pump turns at the same speed as the engine whilst the turbine is connected to the transmission and causes the transmission to spin at the same speed as the turbine [2] This is what basically moves the vehicle. The coupling between the turbine and the pump is through the transmission fluid. Torque is transmitted from the pump to the turbine of the torque converter [2]. . Various models have been introduced to model the pump torque $T_{pump}$ and the turbine torque $T_{turbine}$ see [2 page 103]. . Transmission dynamics . Let&#39;s denote with $GR$ the gear ratio of the transmission. In general, $GR &lt; 1$ and increases as the gear shifts upwards. The input to the transmission module is the torbine torque $T_{turbine}$ [2]. The torque transmitted to the wheels is $T_{wheels}$. Then, at steady state, this torque is given by . $$ T_{wheels} = frac{1}{GR} T_{turbine}$$ . Furthermore, we have the following relaton between the transmission and the wheel speed [2] . $$ omega_{transmission} = frac{1}{GR} omega_{wheels}$$ . Note that these equations cannot be used during gear change. See [2 page 105] for a model based on first order equations. . Engine dynamics . A simplified engine dynamic model is . $$J_{engine} dot{ omega}_e = T_{engine} - T_{pump}$$ . In general, the engine torque $T_{engine}$ depends on the dynamics in the intake and exhaust manifold of the engine and on the accelerator input from the driver [2]. $T_{pump}$ is the torque from the pump is the load of the engine from the torque converter [2]. . Wheel dynamics . The driving wheels rotational dynamics, e.g. for the rear wheels in a rear wheel driven vehicle, are dictated by [2] . $$J_{wheel} dot{ omega}_{wheel, r} = T_{wheel} - R_{effective}F_{xr}$$ . For the non-driven wheels the torque term is zero. . Refernces . Lesson 4: Longitudinal Vehicle Modeling | Rajamani R. Longitudinal Vehicle Dynamics. In: Vehicle Dynamics and Control., Mechanical Engineering Series. Springer 2012. |",
            "url": "https://pockerman.github.io/qubit_opus/longitudinal-dynamics/autonomous-vehicles/mathematical-modelling/vehicle-dynamics/2021/04/10/longitudinal-vehicle-model.html",
            "relUrl": "/longitudinal-dynamics/autonomous-vehicles/mathematical-modelling/vehicle-dynamics/2021/04/10/longitudinal-vehicle-model.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post42": {
            "title": "Linear Time-Invariant Systems",
            "content": "Linear time-invariant systems . Linear systems form a cornerstone of mathematical modelling of dynamical systems. Indeed a system or some aspects of it can be modelled using a linear model. Furthermore, non-linear systems can be linearized around a certain mode of operation. In this section, we give a brief overview of linear time-invariant systems. The major advantage of linear systems is that in terms of analysis a far simpler. Moreover, understanding the dynamics and thus stability of the system is easier. . Let&#39;s consider the following system . $$ frac{d mathbf{x}}{dt} = mathbf{f}( mathbf{x}, mathbf{u}), ~~ mathbf{y} = mathbf{g}( mathbf{x}, mathbf{u})$$ . As before, $ mathbf{x}$ represents the state of the modelled system whilst $ mathbf{u}$ represents some control input to the system. Note that the right-hand side terms do not depend explicitly on the time variable $t$. . If $ mathbf{f}$ or $ mathbf{g}$ or both are non-linear, then the system is non-linear. In this case, we can linearize the system i.e. its dynamics, using a Taylor series expansion near a fixed point $( bar{ mathbf{x}}, bar{ mathbf{u}})$. Recall that at a fixed point is a point where . $$ mathbf{f}( bar{ mathbf{x}}, bar{ mathbf{u}}) = mathbf{0}$$ . The linear, or linearized, dynamics can be written in the following matrix form (assuming no errors) . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . Unforced dynamics . When $ mathbf{u} = mathbf{0}$ and when there are no measurement errors i.e. $ mathbf{y} = mathbf{x}$. The system reduces to . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x}$$ . The solution to this ODE is [1] . $$ mathbf{x}(t) = e^{ mathbf{A}t} mathbf{x}(0)$$ . Thus $ mathbf{x}(t)$ depends or is determined entirely by the matrix $ mathbf{A}$. The stability of the unforced system therefore, can be understood via the eigenvalues and eigenvectors of $ mathbf{A}$. In particular we have the following cases . All the eigenvalues $ lambda$ satisfy $Re( lambda) &lt; 0$. Then the system is stable and all solutions decay to $ mathbf{u} = mathbf{0}$ as $t rightarrow infty$ | There exists at least one eigenvalue $ lambda$ with $Re( lambda) &gt; 0$ then the system is unsatble and will diverge from the fixed point along the corresponding unstable eigenvector direction. | . Forced dynamics . Now let&#39;s assume that $ mathbf{u} neq mathbf{0}$ and that $ mathbf{x}(0) = mathbf{0}$. In this case the solution up to time $t$ is given by [1] . $$ mathbf{x}(t) = int_{0}^t e^{ mathbf{A}(t - tau)} mathbf{B} mathbf{u}( tau)d tau $$ . This integral is nothing more than a convolution. Thus, we can write . $$ mathbf{x}(t) = e^{ mathbf{A}t} mathbf{B} * mathbf{u}(t)$$ . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/dynamical-systems/linear-systems/ode/time-invariant/2021/04/10/linear-time-invariant-systems.html",
            "relUrl": "/dynamical-systems/linear-systems/ode/time-invariant/2021/04/10/linear-time-invariant-systems.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post43": {
            "title": "Η Τραγώδια Της Κύπρου",
            "content": "from IPython.display import YouTubeVideo YouTubeVideo(&#39;UAdKpsdCQtc&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;00iwLvhAomQ&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;dDvmbjiorEM&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;BIs2Y_ndq7w&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;9pdh_XnoFpM&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;f-pKhRLp4ko&#39;, width=800, height=300) .",
            "url": "https://pockerman.github.io/qubit_opus/%CE%B9%CF%83%CF%84%CE%BF%CF%81%CE%AF%CE%B1/%CE%BA%CF%8D%CF%80%CF%81%CE%BF%CF%82/%CE%B5%CE%B9%CF%83%CE%B2%CE%BF%CE%BB%CE%AE/2021/04/09/tragodia-kipros.html",
            "relUrl": "/%CE%B9%CF%83%CF%84%CE%BF%CF%81%CE%AF%CE%B1/%CE%BA%CF%8D%CF%80%CF%81%CE%BF%CF%82/%CE%B5%CE%B9%CF%83%CE%B2%CE%BF%CE%BB%CE%AE/2021/04/09/tragodia-kipros.html",
            "date": " • Apr 9, 2021"
        }
        
    
  
    
        ,"post44": {
            "title": "Lorenz System Simulation",
            "content": "Lorenz System Simulation . The Lorenz system is a system of ordinary differential equations first studied by Edward Lorenz. The system of ODEs is given below . $$ dot{x} = sigma(y -x), ~~ dot{y} = x ( rho - z) -y, ~~ dot{z} = xy - beta z$$ . The model has three parameters i.e. $ sigma, rho$ and $ beta$. . It is notable for having chaotic solutions for certain parameter values and initial conditions. In particular, the Lorenz attractor is a set of chaotic solutions of the Lorenz system. In popular media the &#39;butterfly effect&#39; stems from the real-world implications of the Lorenz attractor, i.e. that in any physical system, in the absence of perfect knowledge of the initial conditions (even the minuscule disturbance of the air due to a butterfly flapping its wings), our ability to predict its future course will always fail. This underscores that physical systems can be completely deterministic and yet still be inherently unpredictable even in the absence of quantum effects. The shape of the Lorenz attractor itself, when plotted graphically, may also be seen to resemble a butterfly. . import numpy as np from mpl_toolkits import mplot3d import matplotlib.pyplot as plt . class ODE45(object): def __init__(self, dt, n_steps, rhs, y0) -&gt; None: self._dt = dt self._n_steps = n_steps self._rhs = rhs self._time = 0.0 self._yold = y0 def step(self): k1 = self._k1(t=self._time) k2 = self._k2(t=self._time, k1=k1) k3 = self._k3(t=self._time, k2=k2) k4 = self._k4(t=self._time, k3=k3) self._yold += k1/6. + k2/3. + k3/3. + k4/6. def integrate(self) -&gt; None: self._time = 0.0 solutions = [[self._yold[0]], [self._yold[1]], [self._yold[2]]] times = [self._time] for itr in range(self._n_steps): self.step() self._time += self._dt times.append(self._time) solutions[0].append(self._yold[0]) solutions[1].append(self._yold[1]) solutions[2].append(self._yold[2]) return times, solutions def _k1(self, t: float) -&gt; np.array: return self._dt * self._rhs(t, self._yold) def _k2(self, t: float, k1: np.array) -&gt; np.array: return self._dt * self._rhs(t + 0.5 * self._dt, self._yold + 0.5 * k1) def _k3(self, t: float, k2: np.array) -&gt; np.array: return self._dt * self._rhs(t + 0.5 * self._dt, self._yold + 0.5 * k2) def _k4(self, t: float, k3: np.array) -&gt; np.array: return self._dt * self._rhs(t + self._dt, self._yold + k3) . class LorenzRhs(object): def __init__(self, beta: np.array): self._beta = beta def __call__(self, t: float, x:np.array) -&gt; np.array: result = np.array([beta[0]*(x[1] - x[0]), x[0]*(beta[1] - x[2]) - x[1], x[0]*x[1] - beta[2]*x[2]]) return result . beta = np.array([10., 28., 8./3.]) x0 = np.array([0., 1.0, 20.]) dt = 0.001 n_steps = 50000 . rhs = LorenzRhs(beta=beta) rk45 = ODE45(dt=dt, n_steps=n_steps, rhs=rhs, y0=x0) . times, solutions = rk45.integrate() . fig = plt.figure(figsize=(15,15)) ax = plt.axes(projection=&#39;3d&#39;) ax.plot3D(solutions[0], solutions[1], solutions[2], &#39;gray&#39;) . [&lt;mpl_toolkits.mplot3d.art3d.Line3D at 0x7fd911347a90&gt;] . The following video discusses how to simulate the Lorenz system with Matlab. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EnsB1wP3LFM&#39;, width=800, height=300) .",
            "url": "https://pockerman.github.io/qubit_opus/lorenz-system/python/simulation/numerics/2021/04/09/simulate-lorenz-system.html",
            "relUrl": "/lorenz-system/python/simulation/numerics/2021/04/09/simulate-lorenz-system.html",
            "date": " • Apr 9, 2021"
        }
        
    
  
    
        ,"post45": {
            "title": "Scala Programming. Functions",
            "content": "Overview . In this Scala programming notebook, we will review functions in Scala. . Functions in Scala . The Scala programming language is a JVM based language worthwhile exploring. In this post I give a brief review of functions in Scala as I continue my exploration of the language. . Functions, in general, are at the core of every programming language when it comes to code organization and implementin the DRY principle. Scala supports two types of functions namely functions and methods. The difference is that a method operates on an object a function does not. . We define a function as follows . def myFunc(x: Integer) = if(x &gt;=0) x else -x . When defining a function we must . specify the types of all parameters | if the function is not recursive i.e. does not call itself, we do not have to specify the return type | if the body of the function requires more than one expression,then we should use a block i.e. {}. The last expression of the block becomes the value that the function returns. | . As an aside, it is possible to omit the return type of the function. Indeed, the Scala compiler can determine the return type from the type of the expression to the right of the = symbol. However, this may not be always the case. The following example demonstrates that . def myAbs(x: Double) = if(x &gt;=0) x else -x . val x = -10 myAbs(x) . x: Int = -10 res1_1: Double = 10.0 . However, with a recursive function, we must specify the return type . def fuc(x: Int): Int = if(x &lt;= 0) 1 else x*fuc(x-1) . Default &amp; Named Arguments . Just like C++, Scala also supports default arguments i.e. the default arguments for functions that are used when we do not specify explicit values . def showMe(x: Int=5) = println(&quot;You want to show &quot; + x) . showMe() &gt; You want to show 5 showMe(6) &gt; You want to show 6 . One of the features that I really like in Python is the named argument(s). This feature is very handy for understanding arguments at call sites i.e. I don&#39;t need to do the trip to the (unavailable) documentation but more important is really helpful in mitigating errors. Scala also supports the idea of named arguments . def speak(arg1: String, arg2: String, arg3: String=&quot; the end&quot;) = println(arg1 + arg2 + arg3) . speak(arg2=&quot; is &quot;, arg1=&quot;This &quot;) &gt; This is the end . As you can see, the named arguments need not be in the same order as the parameters. Furthermore, we can mix unnamed and named arguments, provided the unnamed ones come first. This is similar to Python. . Variable Arguments . Often it is useful to have a function that can take a variable number of arguments think of printf. Usually we call these as varargs functions Scala supports this idea too . def sum(args: Int*): Int = { var result = 0 for(arg &lt;- args) result += arg result } . val s = sum(1, 4, 9, 16, 25) &gt; s:Int = 55 . The actual type received by the function is of type Seq. However, we can not do the following . val s = sum(1 to 5) &gt; cmd10.sc:1: type mismatch; found : scala.collection.immutable.Range.Inclusive required: Int val s = sum(1 to 5) ^Compilation Failed Compilation Failed . That&#39;s because if the sum function is called with one argument, that must be a single integer. Here is how we can fix this . val s = sum(1 to 5:_*) &gt; s:Int = 15 . References . Cay Horstmann Scala for the impatient, Addison-Wesley. |",
            "url": "https://pockerman.github.io/qubit_opus/scala/functions/programming/2021/04/08/scala-functions.html",
            "relUrl": "/scala/functions/programming/2021/04/08/scala-functions.html",
            "date": " • Apr 8, 2021"
        }
        
    
  
    
        ,"post46": {
            "title": "Logistic Map Simulation",
            "content": "Logistic Map Simulation With Python . The logistic map is a discrete time system of the form . $$x_{k+1} = beta x_k (1-x_k)$$ . The logistic map is a polynomial mapping (equivalently, recurrence relation) of degree 2, often cited as an archetypal example of how complex, chaotic behaviour can arise from very simple non-linear dynamical equations. The map was popularized in a 1976 paper by the biologist Robert May,[1] in part as a discrete-time demographic model analogous to the logistic equation written down by Pierre François Verhulst . import numpy as np import matplotlib.pyplot as plt . betas = np.linspace(0.0, 4.0, 400) . def get_steady_state(xinit, nitrs, beta): xold = xinit for itr in range(nitrs): xnew = (xold - xold**2)*beta xold = xnew return xold . def iterate(xinit, betas, use_steady_state, steady_state_itrs, itrs): xvals = [] beta_vals = [] xinit = xinit for beta in betas: #print(&quot;Working with beta={0}&quot;.format(beta)) if use_steady_state: xold = get_steady_state(xinit=xinit, nitrs=steady_state_itrs, beta=beta) else: xold = xinit xss = xold for i in range(itrs): xnew = (xold - xold**2)*beta xold = xnew beta_vals.append(beta) xvals.append(xnew) # if this is the case # the solution is boring :) if np.abs(xnew - xss) &lt; 0.001: break return beta_vals, xvals . beta_vals, xvals = iterate(xinit=0.5, betas=betas, use_steady_state=True, steady_state_itrs=2000, itrs=1000) . plt.plot(beta_vals, xvals) plt.xlabel(&quot;beta&quot;) plt.ylabel(&quot;x&quot;) plt.show() . beta_vals, xvals = iterate(xinit=0.5, betas=betas, use_steady_state=False, steady_state_itrs=2000, itrs=1000) . plt.plot(beta_vals, xvals) plt.xlabel(&quot;beta&quot;) plt.ylabel(&quot;x&quot;) plt.show() . from IPython.display import YouTubeVideo YouTubeVideo(&#39;_BvAkyuWhOI&#39;, width=800, height=300) .",
            "url": "https://pockerman.github.io/qubit_opus/logistic-map/python/simulation/numerics/2021/04/08/logistic-map-simulation.html",
            "relUrl": "/logistic-map/python/simulation/numerics/2021/04/08/logistic-map-simulation.html",
            "date": " • Apr 8, 2021"
        }
        
    
  
    
        ,"post47": {
            "title": "Deutsch's algorithm",
            "content": "Deutsch&#39;s algorithm . Quantum computers pose as the future of computing. Although, at the time of writing, the computing harwdare based on quantum mechanics principles can accommodate only a small number of qubits, algorithms have been developed that demonstrate the superiority of quantum computers for certain class problems. . One such algorithm, and perhaps the simplest one is Deutsch&#39;s algorithm. The algorithm solves the following problem [1] . Given a boolean function $f: {0,1 } rightarrow {0,1 }$ determine if $f$ is constant. . The algorithm, can solve the problem with fewer calls to the function $f$ than is possible on a classical machine [1]. A function is called constant if $f(0) = f(1)$. On the other hand, if $f$ is one-to-one, is called balanced [1]. . Using a classical computer we need to do two evaluations of the function; one for each of the two inputs [1, 2]. On the other hand, Deutsch&#39;s algorithm requires only a single call to a black box to solve the problem. The key to the algorithm is the ability to place the second qubit of the input to the black box in a superposition [2]. Let&#39;s see how to do this. . Deutsch&#39;s algorithm works by putting both qubits representing the two inputs into a superposition [1]. The way to do this is using the Hadamard gate. The following image shows this schematically. . Figure 1. Deutsch&#39;s algorithm circuit. Image from [1]. . Let&#39;s study how the state system $| psi rangle$ evolves. Initially the system is at . $$| psi rangle = |01 rangle$$ . Appication of the Hadamard gate moves the two qubits respectively to . $$|0 rangle = frac{|0 rangle + |1 rangle}{ sqrt{2}}$$ . $$ |1 rangle = frac{|0 rangle - |1 rangle}{ sqrt{2}}$$ . Thus, $| psi rangle$ will be at . $$| psi rangle = left[ frac{|0 rangle + |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . Let&#39;s rename the top qubit as $|x rangle$. We want to evaluate $f(x)$. Note that when the bottom qubit is put into a superposition and then multiply by $U_f$, the system will be at state [1] . $$| psi rangle = (-1)^{f(x)}|x rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . Given however that $|x rangle$ is also in superposition, we will have that the system will be at state [1] . $$| psi rangle = left[ frac{(-1)^{f(0)}|0 rangle + (-1)^{f(1)}|1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . The actual state, as shown in the equation above, depends on the values of $f$. We can summarize this as follows [1]. . $$| psi rangle = begin{cases} ( pm1) left[ frac{|0 rangle + |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] ( pm1) left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] end{cases}$$ The final step is to apply the Hadamard gate on the top qubit. Recall that the Hadamard matrix is its own inverse. Thus applying it to the top qubit we get [1] . $$| psi rangle = begin{cases} ( pm1) |0 rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right], ~~ text{if} ~~ f ~~ text{is constant} ( pm1) |1 rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right], ~~ text{if} ~~ f ~~ text{is balanced} end{cases}$$ Now, we simply measure the top qubit. If it is in state $|0 rangle$, then we know that f is a constant function [1]. This was all accomplished with only one function evaluation. . One of the nice points demonstared by the algorithm is that a change of basis can allow solving a problem that otherwise requires more questions to the oracle. In Deutsch algorithm, we start in the canonical basis $|01 rangle$. The first application of the Hadamard matrices is used to change the basis to go into a balanced superposition of basic states. While in this noncanonical basis, we evaluate $f$ with the bottom qubit. The last Hadamard matrix is used as a change of basis matrix to revert back to the canonical basis [1]. . import numpy as np import random . H = np.array([[1.0/np.sqrt(2.0), 1.0/np.sqrt(2.0)], [1.0/np.sqrt(2.0), - 1.0/np.sqrt(2.0)]]) . def oracle(x, y, constant): if constant: f0 = 0 #random.choice([0,1]) f1 = 0 #random.choice([0,1]) else: f0 = 0 #random.choice([0,1]) f1 = 1 #random.choice([0,1]) return np.array([(-1)**f0*x[0], (-1)**f1*x[1]]) . zero = np.array([1., 0.]) one = np.array([0.0, 1.0]) . zero_H = np.dot(H, zero) one_H = np.dot(H, one) . print(zero_H) print(one_H) . [0.70710678 0.70710678] [ 0.70710678 -0.70710678] . out_oracle = oracle(x=zero_H, y=one_H, constant=True) . x = np.dot(H, out_oracle) . print(x) . [1. 0.] . out_oracle = oracle(x=zero_H, y=one_H, constant=False) . x = np.dot(H, out_oracle) . print(x) . [0. 1.] . References . Noson S. Yanofsky and Mirco A. Mannucci, Quantum Computing for Computer Scientists, Cambridge University Press | Eleanor Rieffel, Wolfgang Polak, Quantum Computing: A Gentle Introduction, The MIT Press. | Deutsch&#39;s algorithm |",
            "url": "https://pockerman.github.io/qubit_opus/deutsch/quantum-computing/algorithms/numerics/2021/03/20/deutsch-algo.html",
            "relUrl": "/deutsch/quantum-computing/algorithms/numerics/2021/03/20/deutsch-algo.html",
            "date": " • Mar 20, 2021"
        }
        
    
  
    
        ,"post48": {
            "title": "Singular Value Decomposition",
            "content": "Singular Value Decomposition . One of the most important matrix factorization techniques is the singular value decomposition most often abbreviated as SVD. The reason why is so popular lies on the fact that it is the foundation for many other computational techniques. For example, just to name a few: . Computing pseudo-inverses | Obtaining low-rank matrix approximations | Dynamic mode decomposition | Proper orthogonal ecomposition | Principal components analysis | . For a complex matrix $A in mathbb{C}^{n times m}$, its SVD is . $$A = U Sigma V^{*}$$ . where $V^{*}$ is the complex conjugate transpose. Both $U$ and $V$ are unitary matrices that is the following holds . $$UU^{*} = U^{*}U = I$$ . In general, if a matrix $W$ is a real matrix i.e. its entries are real numbers, then $W^{*} = W^T$. Thus, if $A in mathbb{R}^{n times m}$ the matrices $U$ and $V$ are real orthogonal matrices i.e. . $$UU^{T} = U^{T}U = I$$ . The matrix $ Sigma$ is a diagonal matrix with real and nonnegative entries on the diagonal. The entries $ Sigma_{ii}$ are called the singular values of $A$. The number of the non-zero singular values corresponds to the rank of the matrix $A$. . Given the popularity of the SVD method, it is not surpsising that most linear algebra libraries provide a way to perform it. The following script shows how to compute the SVD in Python using numpy . import numpy as np X = np.random.rand(10 , 10) U, S, V = np.linalg.svd(X, full_matrices=True) # or doing economy SVD U, S, V = np.linalg.svd(X, full_matrices=False) . You can find the documentation at numpy.linalg.svd. Similarly, using the Blaze C++ library . template&lt; typename MT1, bool SO, typename VT, bool TF, typename MT2, typename MT3 &gt; void svd( const DenseMatrix&lt;MT1,SO&gt;&amp; A, DenseMatrix&lt;MT2,SO&gt;&amp; U, DenseVector&lt;VT,TF&gt;&amp; s, DenseMatrix&lt;MT3,SO&gt;&amp; V ); . Overall, the SVD algorithm is a very important matrix decomposition technique used throughout numerical modeling control theory and system identification. We will see applications of the method in future posts. .",
            "url": "https://pockerman.github.io/qubit_opus/linear-algebra/singular-value-decomposition/algorithms/numerics/2021/03/13/singular-value-decomposition.html",
            "relUrl": "/linear-algebra/singular-value-decomposition/algorithms/numerics/2021/03/13/singular-value-decomposition.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post49": {
            "title": "Scala Programming.  Conditionals and loops",
            "content": "Overview . For loops . a to b: iteration range is $[a,b]$ | a until b: iteration range is $[a, b-1]$ | . This is shown below. . for( i &lt;- 0 to 5){ println(i) } . 0 1 2 3 4 5 . for( i &lt;- 0 until 5){ println(i) } . 0 1 2 3 4 . References .",
            "url": "https://pockerman.github.io/qubit_opus/scala/conditionals/loops/programming/2021/01/10/scala-prog-conditionals-loops.html",
            "relUrl": "/scala/conditionals/loops/programming/2021/01/10/scala-prog-conditionals-loops.html",
            "date": " • Jan 10, 2021"
        }
        
    
  
    
        ,"post50": {
            "title": "Scala Arrays",
            "content": "Scala Arrays . There are two types of arrays in Scala just like in most programming languanges. Fixed length and variable length arrays. . Fixed-length arrays . If we know the size of the needed array and that size does not change, we can use the Array class. . val nums = new Array[Int](10) . nums: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) . nums(0) = 10 . nums . res2: Array[Int] = Array(10, 0, 0, 0, 0, 0, 0, 0, 0, 0) . A Scala Array is implemented as a Java array. For example the nums array above, inside the JVM is represented as int[] in the JVM. . Variable-length arrays . Variable-length arrays in Scala are utilized via the ArrayBuffer class . val b = ArrayBuffer[Int]() . cmd3.sc:1: not found: value ArrayBuffer val b = ArrayBuffer[Int]() ^Compilation Failed . Compilation Failed . In contrast to the Array class, we need to explicitly import ArrayBuffer . import scala.collection.mutable.ArrayBuffer . import scala.collection.mutable.ArrayBuffer . val b = ArrayBuffer[Int]() . b: ArrayBuffer[Int] = ArrayBuffer() . We can now add elements to the buffer . b += 1 . res5: ArrayBuffer[Int] = ArrayBuffer(1) . or add more than one elements in one go . b += (5, 6, 7, 8, 9) . res6: ArrayBuffer[Int] = ArrayBuffer(1, 5, 6, 7, 8, 9) . b ++= Array(0, 0, 0) . res7: ArrayBuffer[Int] = ArrayBuffer(1, 5, 6, 7, 8, 9, 0, 0, 0) . There are various operations supported by the ArrayBuffer class; check the Scala documentation. One thing to note however is the following. Adding or removing elements at the end of an ArrayBuffer is an amortized constant time operation [1]. We can insert and remove elements at an arbitrary location, but those operations are not as efficient since all elements after that location must be shifted [1]. . Traversing arrays . Scala is much more uniform compared to C++ when it comes to traversing arrays. . for(i &lt;- 0 until b.length) println(b(i)) . 1 5 6 7 8 9 0 0 0 . Note that we can also use a guard inside the for expression . for(i &lt;- 0 until b.length if b(i) &gt; 0) println(b(i)) . 1 5 6 7 8 9 . Algorithms . Scala arrays have built-in some commin algorithms e.g. sum and sort, min and max . println(&quot;Max element of b &quot; + b.max) println(&quot;Min element of b &quot; + b.min) println(&quot;Sum of element of b &quot; + b.sum) . Max element of b 9 Min element of b 0 Sum of element of b 36 . The sorted method sorts an Array or ArrayBuffer and returns the sorted array without modifying the original [1]. . val newB = b.sorted println(b) . ArrayBuffer(1, 5, 6, 7, 8, 9, 0, 0, 0) . newB: ArrayBuffer[Int] = ArrayBuffer(0, 0, 0, 1, 5, 6, 7, 8, 9) . Note that you can sort an Array, but not an array buffer, in place [1]. Also note that for the min, max , and quickSort algorithms, the element type must have a comparison operation. This is the case for types with the Ordered trait [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/arrays/programming/2021/01/08/scala-arrays.html",
            "relUrl": "/scala/arrays/programming/2021/01/08/scala-arrays.html",
            "date": " • Jan 8, 2021"
        }
        
    
  
    
        ,"post51": {
            "title": "Scala Programming. Values, variables & types",
            "content": "Overview . In this post, we will briefly discuss values, variables and types in Scala. . Values &amp; variables . A value indicated by the keyword val denotes an identifier that has a constant value; we cannot change its contents [2]. Obviously, a val must be initialized at the point of declaration. . val v1 = 0.5 . v1: Double = 0.5 . v1 = 1.0 . cmd1.sc:1: reassignment to val val res1 = v1 = 1.0 ^Compilation Failed . Compilation Failed . val v2; . (console):1: &#39;=&#39; expected but &#39;;&#39; found. val v2; ^ . (console):1: &#39;=&#39; expected but &#39;;&#39; found. val v2; ^ . In contrast, a variable, indicated with the keyword var, means that it can have its contents changed [2]. . var one = 1.0 . one: Double = 2.0 . // if we want we can say that one equals 2 one = 2.0 . In Scala we don&#39;t need to specify the type of either a val or var. This is inferred from the type of the expression that is used to initialize it. However, here is how we can specify the type if needed . val hello: String = &quot;Hello&quot; val weight: Double = 14.5 . hello: String = &#34;Hello&#34; weight: Double = 14.5 . Lazy values . Lazy evaluation is a programming technique where the evaluation of an expression is done when it is needed for the first time and not before. Lazy evaluation is useful when we want to delay costly initialization statements. Moreover, lazy evaluation can deal with circular dependencies and for developing lazy data structures. . Scala allows to use the keyword lazy when declaring val values. In this case the value initialization is deferred until it is accessed for the first time. . val v1 = 3.5 . v1: Double = 3.5 . lazy val v2 = 3.6 . v2: Double = 3.6 . println(&quot;What is the value of v2?&quot; + v2) . What is the value of v2?3.6 . . Remark . Lazy evaluation is not cost-free. In fact, every time a lazy value is accessed, a method is called that checks, in a threadsafe manner, whether the value has already been initialized [2]. . . Types . In Scala there is no distinction between primitives and class types [2]; all types in Scala are classes. This means we can do things likes calling methods on numbers . 1.toString . res7: String = &#34;1&#34; . or things like . 1.to(5) . res8: Range.Inclusive = Range(1, 2, 3, 4, 5) . In Scala, we do not need wrapper types [2]. The Scala compiler does the conversion for us between primitive types and wrappers. For example, if you make an array of Int , you get an int[] array in the virtual machine [2]. . The compiler also checks that we do not combine expressions of different type. For example: . 1 to 4.0 . cmd9.sc:1: type mismatch; found : Double(4.0) required: Int val res9 = 1 to 4.0 ^Compilation Failed . Compilation Failed . References . Martin Odersky, Lex Spoon, Bill Venners, Programming in Scala, 3rd Edition, artima. | Cay Horstmann, Scala for the Impatient 1st Edition, |",
            "url": "https://pockerman.github.io/qubit_opus/scala/values-variables/programming/2021/01/05/scala-values-variables-types.html",
            "relUrl": "/scala/values-variables/programming/2021/01/05/scala-values-variables-types.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post52": {
            "title": "Occupancy grids",
            "content": "Overview . In this post we look into occupancy grid maps. . Occupancy grids . Simply put an occupancy grid is a discretized version of the environment surrounding the ego vehicle [3]. The discretization can be either two or three dimensional. The following images show versions of 2D and 3D grids. . Figure 1. 2D occupancy grid map. . Figure 2. 3D occupancy grid map. . Each cell in an occupancy grid indicates if the space represented by the cell is empty or occupied by an obstacle. Given this, the more dense the grid is the finer the representation of the environment will be. However, the more dense the grid is the more computationally expensive is to create it. Each cell in the grid is binary i.e. it has a value, $m_i$, of either one or zero [3]. . Typically, in order to create an occupancy grid, we make the following assumptions [3] . Static environment i.e. no dynamic objects | Grid cells are independent | The vehicle state is known | . Given the gride cell value $mi_i$, we can construct a belief map meaning a map at time $t$ such that for each cell . $$bel_{t}(m_i) = P(m_i | (x,y))$$ . where $(x,y)$ denotes the vehicle state and the sensor measurements for a given cell. We can establish a threshold at which a given belief can be classified as occupied. Furthermore, we can combine measurements from different time steps to obtain a more accurate belief. In particular, using Baye&#39;s theorem we can come up with the following equation [3] . $$bel_{t}(m_i) = eta P(y_t | m_i)bel_{t-1}(m^i)$$ . where $ eta$ is a scaling or normalization constant to ensure that the $bel$ function represents a probability. . Disadvantages of occupancy grids . In occupancy grid mapping every grid cell is one of two states; occupied or empty [2]. But in some situations it makes sense for a cell to be partially filled. This may occur when only part of the grid cell is filled and the rest is empty, or when the objects that occupies the grid cell have special characteristics [2]. For example it may be that we want a grid cell occupied with vegetation to be somehow less occupied than a grid cell filled with solid rock. . Semi-transparent obstacles and Mitigation Classical occupancy grids have trouble dealing with semi-transparent obstacles such as glass and vegetation. These obstacles may return hits to the laser rangefinder about half of the time, but eventually the occupancy grid will converge to either occupied or not, both of which are incorrect. [2] . A possible solution to this problem would be to consider a more continous measure that measures the density or probability of a beam to have reflected and not passed. Perhaps the simplest approach towards this is to treat the random variable as a biased coin and for each state keep a count of the number of hits and pass throughs [2]. Thus, the only difference is that each state tracks two numbers. Then, the probability can be calculated empirically as the ratio of the hits to the sum of hits and passes. For the inverse sensor model, we either spread the high probability zone or use fractional number of hits and misses for each state [2]. . References . Bruno Siciliano, Lorenzo Sciavicco, Luigi Villani, Giuseppe Oriolo, Robotics Modelling, Planning and Control, Springer | Drew Bagnell Statistical Techniques in Robotics, Lecture notes. | Motion planning for self-driving cars |",
            "url": "https://pockerman.github.io/qubit_opus/robotics/planning/mapping/navigation/occupancy-grid/2020/09/15/occupancy-grids.html",
            "relUrl": "/robotics/planning/mapping/navigation/occupancy-grid/2020/09/15/occupancy-grids.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post53": {
            "title": "Gradient Descent",
            "content": "Gradient Descent . Perhaps the simplest algorithm for uncostrained optimization is gradient descent also known as steepest descent. Consider the following function [1] . $$f( theta_1, theta_2) = frac{1}{2}( theta_{1}^2 - theta_2)^2 + frac{1}{2}( theta_1 -1)^2$$ . We are interested in finding $ theta_1, theta_2$ that minimize $f$. Gradient descent is an iterative algorithm that uses the gradient of the function in order to update the parameters. The update rule is . $$ boldsymbol{ theta}_k = boldsymbol{ theta}_{k-1} - eta nabla f|_{ boldsymbol{ theta}_{k-1}} $$ . $ eta$ is the so called learning rate and tunes how fast we move to the direction of the gradient. A small $ eta$ slows down convergence whilst a large value may not allow convergence of the algorithm. This is shown in the two figures below: . Figure 1. Gradient descent with eta 0.1. . Figure 2. Gradient descent with eta 0.6. . The code below is a simple implementation of the gradient descent algorithm. . import numpy as np import matplotlib.pyplot as plt . def f(theta1, theta2): return 0.5*(theta1**2 - theta2)**2 + 0.5*(theta1 -1.0)**2 . def f_grad(theta1, theta2): return (2.0*theta1*(theta1**2 - theta2) + (theta1 - 1.0), -(theta1**2 - theta2)) . def gd(eta, itrs, tol): coeffs_series = [] coeffs = [0.0, 0.0] coeffs_series.append([coeffs[0], coeffs[1]]) val_old = f(theta1=coeffs[0], theta2=coeffs[1]) for itr in range(itrs): grad = f_grad(theta1=coeffs[0], theta2=coeffs[1]) coeffs[0] -= eta*grad[0] coeffs[1] -= eta*grad[1] coeffs_series.append([coeffs[0], coeffs[1]]) val = f(theta1=coeffs[0], theta2=coeffs[1]) abs_error = np.abs(val - val_old) print(&quot;&gt;Iteration {0} absolute error {1} exit tolerance {2}&quot;.format(itr, abs_error, tol)) if abs_error &lt; tol: print(&quot;&gt;GD converged with residual {0}&quot;.format(np.abs(val - val_old))) return coeffs_series val_old = val return coeffs_series . coeffs_series = gd(eta=0.1, itrs=100, tol=1.0e-4) . &gt;Iteration 0 absolute error 0.09494999999999998 exit tolerance 0.0001 &gt;Iteration 1 absolute error 0.07622463831103915 exit tolerance 0.0001 &gt;Iteration 2 absolute error 0.05968293530998797 exit tolerance 0.0001 &gt;Iteration 3 absolute error 0.04523783343545831 exit tolerance 0.0001 &gt;Iteration 4 absolute error 0.03333771938318547 exit tolerance 0.0001 &gt;Iteration 5 absolute error 0.024254166137182898 exit tolerance 0.0001 &gt;Iteration 6 absolute error 0.01782239274907732 exit tolerance 0.0001 &gt;Iteration 7 absolute error 0.013533685475623586 exit tolerance 0.0001 &gt;Iteration 8 absolute error 0.010768584908705553 exit tolerance 0.0001 &gt;Iteration 9 absolute error 0.008983651981479004 exit tolerance 0.0001 &gt;Iteration 10 absolute error 0.007786932467621618 exit tolerance 0.0001 &gt;Iteration 11 absolute error 0.006930789314960939 exit tolerance 0.0001 &gt;Iteration 12 absolute error 0.0062723026732663945 exit tolerance 0.0001 &gt;Iteration 13 absolute error 0.005733543774474825 exit tolerance 0.0001 &gt;Iteration 14 absolute error 0.0052730560188138376 exit tolerance 0.0001 &gt;Iteration 15 absolute error 0.004868553639578624 exit tolerance 0.0001 &gt;Iteration 16 absolute error 0.00450745300901266 exit tolerance 0.0001 &gt;Iteration 17 absolute error 0.004182026970546315 exit tolerance 0.0001 &gt;Iteration 18 absolute error 0.003887028764196082 exit tolerance 0.0001 &gt;Iteration 19 absolute error 0.0036185521793817496 exit tolerance 0.0001 &gt;Iteration 20 absolute error 0.0033734842877468432 exit tolerance 0.0001 &gt;Iteration 21 absolute error 0.0031492347340679877 exit tolerance 0.0001 &gt;Iteration 22 absolute error 0.0029435928019137803 exit tolerance 0.0001 &gt;Iteration 23 absolute error 0.0027546441698343416 exit tolerance 0.0001 &gt;Iteration 24 absolute error 0.0025807166947806187 exit tolerance 0.0001 &gt;Iteration 25 absolute error 0.0024203414221908165 exit tolerance 0.0001 &gt;Iteration 26 absolute error 0.0022722224779619174 exit tolerance 0.0001 &gt;Iteration 27 absolute error 0.0021352127684441946 exit tolerance 0.0001 &gt;Iteration 28 absolute error 0.002008293861733755 exit tolerance 0.0001 &gt;Iteration 29 absolute error 0.0018905590854755572 exit tolerance 0.0001 &gt;Iteration 30 absolute error 0.0017811992002927796 exit tolerance 0.0001 &gt;Iteration 31 absolute error 0.0016794901835057927 exit tolerance 0.0001 &gt;Iteration 32 absolute error 0.0015847827649886279 exit tolerance 0.0001 &gt;Iteration 33 absolute error 0.0014964934298845774 exit tolerance 0.0001 &gt;Iteration 34 absolute error 0.0014140966564608337 exit tolerance 0.0001 &gt;Iteration 35 absolute error 0.0013371181986932233 exit tolerance 0.0001 &gt;Iteration 36 absolute error 0.0012651292559366714 exit tolerance 0.0001 &gt;Iteration 37 absolute error 0.0011977413984442034 exit tolerance 0.0001 &gt;Iteration 38 absolute error 0.001134602138991761 exit tolerance 0.0001 &gt;Iteration 39 absolute error 0.0010753910584805904 exit tolerance 0.0001 &gt;Iteration 40 absolute error 0.001019816407902937 exit tolerance 0.0001 &gt;Iteration 41 absolute error 0.0009676121210644636 exit tolerance 0.0001 &gt;Iteration 42 absolute error 0.0009185351824353497 exit tolerance 0.0001 &gt;Iteration 43 absolute error 0.0008723633028205682 exit tolerance 0.0001 &gt;Iteration 44 absolute error 0.0008288928625002738 exit tolerance 0.0001 &gt;Iteration 45 absolute error 0.0007879370873338197 exit tolerance 0.0001 &gt;Iteration 46 absolute error 0.0007493244282380136 exit tolerance 0.0001 &gt;Iteration 47 absolute error 0.0007128971186049562 exit tolerance 0.0001 &gt;Iteration 48 absolute error 0.000678509887739322 exit tolerance 0.0001 &gt;Iteration 49 absolute error 0.0006460288113815469 exit tolerance 0.0001 &gt;Iteration 50 absolute error 0.0006153302829237615 exit tolerance 0.0001 &gt;Iteration 51 absolute error 0.000586300091094321 exit tolerance 0.0001 &gt;Iteration 52 absolute error 0.0005588325917408928 exit tolerance 0.0001 &gt;Iteration 53 absolute error 0.000532829962933393 exit tolerance 0.0001 &gt;Iteration 54 absolute error 0.0005082015339742743 exit tolerance 0.0001 &gt;Iteration 55 absolute error 0.00048486318008077005 exit tolerance 0.0001 &gt;Iteration 56 absolute error 0.00046273677552067724 exit tolerance 0.0001 &gt;Iteration 57 absolute error 0.0004417496988608476 exit tolerance 0.0001 &gt;Iteration 58 absolute error 0.00042183438475075323 exit tolerance 0.0001 &gt;Iteration 59 absolute error 0.00040292791732379415 exit tolerance 0.0001 &gt;Iteration 60 absolute error 0.00038497166087553616 exit tolerance 0.0001 &gt;Iteration 61 absolute error 0.00036791092397959677 exit tolerance 0.0001 &gt;Iteration 62 absolute error 0.00035169465363989703 exit tolerance 0.0001 &gt;Iteration 63 absolute error 0.00033627515646207293 exit tolerance 0.0001 &gt;Iteration 64 absolute error 0.00032160784416228154 exit tolerance 0.0001 &gt;Iteration 65 absolute error 0.0003076510010270577 exit tolerance 0.0001 &gt;Iteration 66 absolute error 0.00029436557119745174 exit tolerance 0.0001 &gt;Iteration 67 absolute error 0.000281714963878678 exit tolerance 0.0001 &gt;Iteration 68 absolute error 0.00026966487477881555 exit tolerance 0.0001 &gt;Iteration 69 absolute error 0.000258183122257602 exit tolerance 0.0001 &gt;Iteration 70 absolute error 0.0002472394968245067 exit tolerance 0.0001 &gt;Iteration 71 absolute error 0.0002368056227645514 exit tolerance 0.0001 &gt;Iteration 72 absolute error 0.0002268548307944717 exit tolerance 0.0001 &gt;Iteration 73 absolute error 0.00021736204076207993 exit tolerance 0.0001 &gt;Iteration 74 absolute error 0.00020830365349946717 exit tolerance 0.0001 &gt;Iteration 75 absolute error 0.00019965745102808012 exit tolerance 0.0001 &gt;Iteration 76 absolute error 0.0001914025043919798 exit tolerance 0.0001 &gt;Iteration 77 absolute error 0.00018351908846457078 exit tolerance 0.0001 &gt;Iteration 78 absolute error 0.0001759886031370006 exit tolerance 0.0001 &gt;Iteration 79 absolute error 0.00016879350035171707 exit tolerance 0.0001 &gt;Iteration 80 absolute error 0.0001619172164950512 exit tolerance 0.0001 &gt;Iteration 81 absolute error 0.00015534410970717907 exit tolerance 0.0001 &gt;Iteration 82 absolute error 0.000149059401708577 exit tolerance 0.0001 &gt;Iteration 83 absolute error 0.0001430491237779689 exit tolerance 0.0001 &gt;Iteration 84 absolute error 0.00013730006654984238 exit tolerance 0.0001 &gt;Iteration 85 absolute error 0.000131799733328664 exit tolerance 0.0001 &gt;Iteration 86 absolute error 0.00012653629664396288 exit tolerance 0.0001 &gt;Iteration 87 absolute error 0.00012149855779407855 exit tolerance 0.0001 &gt;Iteration 88 absolute error 0.00011667590914838256 exit tolerance 0.0001 &gt;Iteration 89 absolute error 0.00011205829899737585 exit tolerance 0.0001 &gt;Iteration 90 absolute error 0.00010763619875779401 exit tolerance 0.0001 &gt;Iteration 91 absolute error 0.00010340057235624662 exit tolerance 0.0001 &gt;Iteration 92 absolute error 9.934284762933097e-05 exit tolerance 0.0001 &gt;GD converged with residual 9.934284762933097e-05 . coeffs_x = [] coeffs_y = [] for item in coeffs_series: coeffs_x.append(item[0]) coeffs_y.append(item[1]) . theta1 = np.linspace(0.0, 2.0, 100) theta2 = np.linspace(-0.5, 3.0, 100) . X, Y = np.meshgrid(theta1, theta2) . Z = f(X, Y) . plt.contour(X, Y, Z, 60, colors=&#39;black&#39;); plt.plot(coeffs_x, coeffs_y, &#39;r-o&#39;) plt.show() . References . Kevin P. Murphy, Machine Learning A Probabilistic Perspective, The MIT Press |",
            "url": "https://pockerman.github.io/qubit_opus/gradient-descent/unconstrained-optimization/machine-learning/algorithms/numerics/2020/06/22/gradient-descent.html",
            "relUrl": "/gradient-descent/unconstrained-optimization/machine-learning/algorithms/numerics/2020/06/22/gradient-descent.html",
            "date": " • Jun 22, 2020"
        }
        
    
  
    
        ,"post54": {
            "title": "The Viterbi algorithm",
            "content": "The Viterbi algorithm . The backward and forward algorithms can be used to compute $P(O| lambda)$. In this notebook we are interested in computing the most likely path given a sequence $O$ and a hidden Markov model $ lambda$. The Viterbi algorithm gives us a way to do so. The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states, also called the Viterbi path, that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM) [2]. The algorithm uses a maximum operation instead of the sum. The operation of Viterbi&#39;s algorithm can be visualized by means of a trellis diagram [2]. It is essentially the shortest path through this trellis. . Given a state sequence $Q=q_1q_2, cdots,q_T$ and an observation sequence $O=O_1O_2, cdots,O_T$ we dfine the variable $ delta_t(i)$ as the probability of the highest probability path at time $t$ that accounts for the first $t$ observations and ends in $S_i$ [1]: . $$ delta_t(i) = max_{Q}p(q_1q_2, cdots,q_t=S_i,O_1O_2, cdots,O_t| lambda)$$ . Then we can recursively calculate $ delta_{t+1}(i)$ and the optimal path can be read by backtracking from $T$ , choosing the most probable at each instant. The algorithm is as follows [1]: . Initialize | . $$ delta_1(i) = pi_i b_i(O_1)$$ . This is initialization is the same as in the forward algorithm. To retrieve the state sequence we also need to keep track of the argument which maximized for each $t$ and $j$. We therefore use the array $ psi$, and in the initialization step the first $ psi$ variable of every state will be equal to 0 because no specific argument coming from the initial probability maximized the value of the first state. . $$ psi_1(i) = 0$$ . Recurse | . $$ delta_t(j) = max_{i=1}^{N} delta_{t-1}(i)a_{ij}b_j(O_t)$$ . $$ psi_t(j) = argmax_{i=1}^{N} delta_{t-1}(i)a_{ij}$$ . Termination | . $$p^{*} = max_i delta_T(i)$$ . $$q^{*}_T = arg max_i delta_T(i)$$ . Path backtracking | . $$q_t{*} = psi_{t+1}(q^{*}_{t+1}), t= T-1, T-2, cdots, 1$$ . $ psi_t (j)$ keeps track of the state that maximizes $ delta_t(j)$ at time $t-1$, that is, the best previous state. The Viterbi algorithm has the same complexity with the forward phase, where instead of the sum, we take the maximum at each step [1]. . Let&#39;s see an example applying the Viterbi algorithm. The example is taken from [2]. Some coding hints from Implement Viterbi Algorithm in Hidden Markov Model using Python and R have also been used. . import numpy as np . obs_to_idx = {&#39;normal&#39;:0, &#39;cold&#39;: 1, &#39;dizzy&#39;:2} # state to index map state_to_idx = {&#39;Healthy&#39;:0, &#39;Fever&#39;:1} . pi = np.array([0.6, 0.4]) # transition probabilities A = np.array([[0.7, 0.3], [0.4, 0.6]]) # emission probabilties B = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]) . o = [&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;] . delta = np.zeros(shape=(len(o), A.shape[0])) previous = np.zeros((len(o)-1, A.shape[0])) for st in state_to_idx: state_idx = state_to_idx[st] delta[0][state_idx] = pi[state_idx] * B[state_idx][obs_to_idx[o[0]]] print(delta) . [[0.3 0.04] [0. 0. ] [0. 0. ]] . for t in range(1, len(o)): obs_idx = obs_to_idx[o[t]] for i in state_to_idx: i_st_idx = state_to_idx[i] probs=[] for j in state_to_idx: j_st_idx = state_to_idx[j] probs.append(delta[t - 1][j_st_idx]*A[j_st_idx][i_st_idx]*B[i_st_idx][obs_idx]) # This is our most probable state given previous state at time t (1) previous[t-1, i_st_idx] = np.argmax(probs) delta[t, i_st_idx] = np.max(probs) # Path Array S = np.zeros(len(o)) # Find the most probable last hidden state last_state = np.argmax(delta[len(o) - 1, :]) S[0] = last_state backtrack_index = 1 for i in range(len(o) - 2, -1, -1): S[backtrack_index] = previous[i, int(last_state)] last_state = previous[i, int(last_state)] backtrack_index += 1 # Flip the path array since we were backtracking S = np.flip(S, axis=0) # Convert numeric values to actual hidden states path = [] for s in S: if s == 0: path.append(&quot;Healthy&quot;) else: path.append(&quot;Fever&quot;) . print(&quot;Path is &quot;, path) . Path is [&#39;Healthy&#39;, &#39;Healthy&#39;, &#39;Fever&#39;] . Another way to compute the $ delta$ matrix is the following more Pythonic way, taken from Implement Viterbi Algorithm in Hidden Markov Model using Python and R. Note the use of the log function. . delta = np.zeros(shape=(len(o), A.shape[0])) previous = np.zeros((len(o)-1, A.shape[0])) for st in state_to_idx: state_idx = state_to_idx[st] delta[0, :] = np.log(pi * B[:, obs_to_idx[o[0]]]) print(delta) . [[-1.2039728 -3.21887582] [ 0. 0. ] [ 0. 0. ]] . for t in range(1, len(o)): obs_idx = obs_to_idx[o[t]] for i in state_to_idx: i_st_idx = state_to_idx[i] # Same as Forward Probability probability = delta[t - 1] + np.log(A[:, i_st_idx]) + np.log(B[i_st_idx, obs_idx]) # This is our most probable state given previous state at time t (1) previous[t - 1, i_st_idx] = np.argmax(probability) # This is the probability of the most probable state (2) delta[t, i_st_idx] = np.max(probability) # Path Array S = np.zeros(len(o)) # Find the most probable last hidden state last_state = np.argmax(delta[len(o) - 1, :]) S[0] = last_state backtrack_index = 1 for i in range(len(o) - 2, -1, -1): S[backtrack_index] = previous[i, int(last_state)] last_state = previous[i, int(last_state)] backtrack_index += 1 # Flip the path array since we were backtracking S = np.flip(S, axis=0) # Convert numeric values to actual hidden states path = [] for s in S: if s == 0: path.append(&quot;Healthy&quot;) else: path.append(&quot;Fever&quot;) . print(&quot;Path is &quot;, path) . Path is [&#39;Healthy&#39;, &#39;Healthy&#39;, &#39;Fever&#39;] . The following video provides a motivation behind the use of the Viterbi algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;MPeedE6Odj0&#39;, width=800, height=300) . The following video provides nice description of the Viterbi algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;s9dU3sFeE40&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Viterbi algorithm, Wikipedia. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20viterbi-algorithm/dynamic-programming/algorithms/numerics/2020/05/24/viterbi-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20viterbi-algorithm/dynamic-programming/algorithms/numerics/2020/05/24/viterbi-algorithm.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post55": {
            "title": "Backward Algorithm",
            "content": "The backward algorithm is the complement of the forward algorithm. Let&#39;s introduce the backward variable $ beta_t(i)$. This is the probability of being in $S_i$ at time $t$ abd observing the partial sequence $O_{t+1}, cdots,O_T$ [1]. This can be written as . $$ beta_t(i) = P(O_{t+1}, cdots,O_T | q_t=S_i, lambda)$$ . The backward algorithm computes this recursively . Initialize | . $$ beta_T(i) = 1$$ . Recurse | . $$ beta_t(i) = P(O_{t+1}, cdots,O_T | q_t=S_i, lambda)$$ . which can be written as, see [1], . $$ beta_{t}(i) = sum_{j}^{N}b_j(O_{t+1})a_{i,j} beta_{t+1}(j)$$ . Let&#39;s implement this as we did for the forward algorithm. . Assume a system with two states $S= {S_0, S_1 }$. Futher, assume that the observation sequence consists of elements from the following set $V= {a, b,c }$. Also let&#39;s assume the following HMM: . $$ boldsymbol{ pi}= begin{bmatrix}0.6 &amp; 0.4 end{bmatrix}$$ . $$ mathbf{A}= begin{bmatrix}0.7 &amp; 0.3 0.4 &amp; 0.6 end{bmatrix}$$ . $$ mathbf{B}= begin{bmatrix} 0.5 &amp; 0.4 &amp; 0.1 0.1 &amp; 0.3 &amp; 0.6 end{bmatrix}$$ . Assume the following sequence $V= {a, b, c }$. We introduce the $ beta$ matrix: . $$ beta = begin{bmatrix}0 &amp; 0 0 &amp; 0 0 &amp; 0 end{bmatrix}$$ . First we initialize . $$ beta = begin{bmatrix}0 &amp; 0 0 &amp; 0 1 &amp; 1 end{bmatrix}$$ . Then use the recursion formula . $$ beta_{t}(i) = sum_{j}^{N}b_j(O_{t+1})a_{i,j} beta_{t+1}(j)$$ . import numpy as np . obs_to_idx = {&#39;a&#39;:0, &#39;b&#39;: 1, &#39;c&#39;:2} # state to index map state_to_idx = {&#39;S0&#39;:0, &#39;S1&#39;:1} . pi = np.array([0.6, 0.4]) # transition probabilities A = np.array([[0.7, 0.3], [0.4, 0.6]]) # emission probabilties B = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]) . o = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] . beta = np.zeros(shape=(len(o),A.shape[0])) . beta[len(o) - 1] = np.ones(A.shape[0]) . # start at the position one before the end # proceed until t is -1. Move back one step at the time for t in range(len(o)-2, -1, -1): for j in range(A.shape[0]): # for x = np.array([1.,2.]) and # y = np.array([1.,2.])then # x*y = np.array([1., 4.]) # that is element-wise product is performed beta[t, j] = (beta[t + 1] * B[:, obs_to_idx[o[t + 1]]]).dot(A[j, :]) . print(beta) . [[0.106 0.112] [0.25 0.4 ] [1. 1. ]] . Overall the forward and backward algorithms can be used to compute $P(O| lambda)$. Indeed using the forward algorithm we have: . $$P(O| lambda) = sum_{i}^{N} alpha_{T}(i)$$ . whilst using the backward algorithm . $$P(O| lambda) = sum_{i}^{N} pi_i b_i(O_1) beta_{1}(i)$$ . The following video nicely explains the motivation behind the backward algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EbxLWGw2zJ4&#39;, width=800, height=300) . The following video nicely explains both the forward and backward algorithms. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;gYma8Gw38Os&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Forward–backward algorithm. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20backward-algorithm/dynamic-programming/algorithms/numerics/2020/05/23/backward-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20backward-algorithm/dynamic-programming/algorithms/numerics/2020/05/23/backward-algorithm.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post56": {
            "title": "Forward Algorithm",
            "content": "Given a hidden Markov model $ lambda$, meaning . A set of $N$ states $ mathbb{S}= {S_1, dots S_N }$ | A set of $M$ distinct observation symbols $ mathbf{V} = {v_1 dots v_M }$ | A transition probability matrix $ mathbf{A}$ | An observations probabilities matrix $ mathbf{B}$ | An initial state of probabilities $ boldsymbol{ pi}$ | . we want to be able to compute the marginal probability $P(O| lambda)$. This is the so called evaluation problem [1]; given the observation sequence $O$ calculate the probability that the sequence can occur under $ lambda$ . In theory, this can be calculated by using . $$P(O| lambda) = sum_{Q}P(O, Q| lambda)$$ . where . $$P(O, Q| lambda) = P(q_1) Pi_{t=2}^{T}P(q_t|q_{t-1}) Pi_{t=1}^{T}P(O_t|q_{t})$$ . Despite this, you should note that there are $N^T$ possible $Q$s assuming that all probabilities are nonzero [1]. Thus, the marginalization step above is rather, or can be, computationally expensive. We need another way to calculate $P(O| lambda)$. . In order to compute the probability $P(O| lambda)$ we need to know the joint probability $P(O, Q| lambda)$. The forward algorithm allows us to compute the latter without using marginalization. It does so by using recursion. We will divide the observation sequence into two parts; the first part will be $[1,t]$, the second is $[t+1, T]$ [1]. We further define the forward variable $ alpha_t(i)$. This will denote the probability of observing the partial sequence $ {O_1, cdots,O_t }$ unitl time $t$ and being in $S_i$ at time $t$ given $ lambda$: . $$ alpha_t(i) = P(O_1, cdots,O_t, q_t = S_i | lambda)$$ . This can be calculated recursively: . Initialize | . $$ alpha_t(i) = pi_ib_i(O_1)$$ . Recurse | . $$ alpha_{t+1}(j) = P(O_1, cdots,O_{t+1}, q_{t+1} = S_j | lambda)$$ . which can be written as . $$ alpha_{t+1}(j) = [ sum_{i}^{N} alpha_t(i)a_{i,j}]b_j(O_{t+1})$$ . Now $ alpha_t(i)$ explains the first $t$ observations and ends in state $S_i$. We multiply with the transition probability $a_{ij}$ in order to move to state $S_j$. since the are $N$ possible previous states we have to sum over all of them. Finally, we weight the result with $b_j(O_{t+1})$ which is the probability of observing $O_{t+1}$ at state $S_j$ at time $t+1$. . Once we know the forward variables, it is easy to calculate $P(O| lambda)$: . $$P(O| lambda) = sum_{i}^{N} alpha_{T}(i)$$ . $ alpha_{T}(i)$ is the probability of generating the full observation sequence and ending up in state $S_i$. We need to sum up over all the possible final states. The $ alpha_{t}(i)$ can be represented as a matrix of size $T times N$. Where $T$ is the size of the observation sequence and $N$ the number of states. Let&#39;s see an example. . Let&#39;s see a simple example. Assume a system with three states $S= {S_1, S_2, S_3 }$. Futher, assume that the observation sequence consists of elements from the following set $V= {a, b,c }$. Also let&#39;s assume the following HMM: . $$ boldsymbol{ pi}= begin{bmatrix}0.7 &amp; 0.15 &amp; 0.15 end{bmatrix}$$ . $$ mathbf{A}= begin{bmatrix}0.5 &amp; 0.25 &amp; 0.25 0.1 &amp; 0.8 &amp; 0.1 0.3 &amp; 0.15 &amp; 0.6 end{bmatrix}$$ . $$ mathbf{B}= begin{bmatrix} 0.16 &amp; 0.26 &amp; 0.58 0.25 &amp; 0.28 &amp; 0.47 0.2 &amp; 0.1 &amp; 0.7 end{bmatrix}$$ . We want to calculate the probability $P(O| lambda)$ where $O= {a, b, a, c, b, a }$. We will use the forward algorithm for this. We will first do the computation using pencil and paper and then write a small Python script for us. We create the matrix $ alpha$: . $$ alpha = begin{bmatrix}0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 end{bmatrix}$$ . The first step is the initialization of the matrix. We take the first observation in the sequence $O$ which &#39;a&#39;. This has to be mapped to an index. Assume that we have available such a mapping: . $$ {a:0, b:1, c:2 }$$ . Further assume that we use zero-based counting. We have . $$ alpha_{0,0} = pi_0 mathbf{B}_{0, 0} = 0.7 0.16 = 0.112$$ . $$ alpha_{0,1} = pi_1 mathbf{B}_{1, 0} = 0.15 0.25 = 0.0375 $$ . $$ alpha_{0,2} = pi_2 mathbf{B}_{2, 0} = 0.15 0.2 = 0.03$$ . we now proceed to the calculation of the probabilities of the next symbols. The symbol &#39;b&#39;, which is the next symbol in $O$ has index 1. Its probabilities are . $$ alpha_{1,0} = mathbf{B}_{0,1}( alpha_{0,0} mathbf{A}_{0,0} + alpha_{0,1} mathbf{A}_{1,0} + alpha_{0,2} mathbf{A}_{2,0}) $$ . Similarly for $ alpha_{1,1}$ and $ alpha_{1,2}$ . $$ alpha_{1,1} = mathbf{B}_{1,1}( alpha_{0,0} mathbf{A}_{0,1} + alpha_{0,1} mathbf{A}_{1,1} + alpha_{0,2} mathbf{A}_{2,1}) $$ . $$ alpha_{1,2} = mathbf{B}_{2,1}( alpha_{0,0} mathbf{A}_{0,2} + alpha_{0,1} mathbf{A}_{1,2} + alpha_{0,2} mathbf{A}_{2,2}) $$ . After filling the matrix $ alpha$ we can calculate the probability $P(O| lambda)$, This is given by the following sum: . $$P(O| lambda) = alpha_{5,0} + alpha_{5,1} + alpha_{5,2}$$ . Below is a simple Python script that performs these tedious calculations for us. . import numpy as np . obs_to_idx = {&#39;a&#39;:0, &#39;b&#39;: 1, &#39;c&#39;:2} # state to index map state_to_idx = {&#39;S1&#39;:0, &#39;S2&#39;:1, &#39;S3&#39;: 2} . pi = np.array([0.7, 0.15, 0.15]) # transition probabilities A = np.array([[0.5, 0.25, 0.25], [0.1, 0.8, 0.1], [0.3, 0.15, 0.6]]) # emission probabilties B = np.array([[0.16, 0.26, 0.58], [0.25, 0.28, 0.47], [0.2, 0.1, 0.7]]) . o = [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] . a = np.zeros(shape=(len(o),A.shape[0])) . # initialize alpha for i in range(len(state_to_idx)): # only first row the rest is zero a[0][i] = pi[i]*B[i][obs_to_idx[o[0]]] . print(&quot;Initial probabilities&quot;) print(a) . Initial probabilities [[0.112 0.0375 0.03 ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ]] . for t in range(1, len(o)): for j in range(A.shape[0]): a[t][j] = 0 # fix j = state_idx and sum over the states for i in range(A.shape[0]): a[t][j] += a[t -1][i] * A[i][j] a[t][j] *= B[j][obs_to_idx[o[i]]] print(&quot;alpha matrix: &quot;) print(a) . alpha matrix: [[1.12000000e-01 3.75000000e-02 3.00000000e-02] [1.10000000e-02 1.56250000e-02 9.95000000e-03] [1.60760000e-03 4.18562500e-03 2.05650000e-03] [2.94290000e-04 1.01471875e-03 4.10872500e-04] [5.95005800e-05 2.36744594e-04 8.43135750e-05] [1.25950115e-05 5.42294641e-05 1.78275499e-05]] . prob = 0; for i in range(A.shape[0]): prob += a[len(o)-1][i] print(&quot;Probability for sequence {0} is {1} &quot;.format(o, prob)) . Probability for sequence [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] is 8.465202543750001e-05 . Log probabilities . The forward variable, as well as the backward variable used in the backward algorithm are calculated as products of probabilities. When we have long sequences this may result in underflow [1]. In order to avoid this, we normalize $ alpha_t(i)$ by multiplying it with . $$c_t = frac{1}{ sum_{j} alpha_t(j)}$$ . After this normalization, $P(O| lambda)$ is given by . $$P(O| lambda) = frac{1}{ Pi_t c_t}$$ . or . $$logP(O| lambda) = - sum_t log c_t$$ . The motivation behind the forward algorithm is nicely presented in the following video . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EbxLWGw2zJ4&#39;, width=800, height=300) . The following video nicely explains both the forward and backward algorithms. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;gYma8Gw38Os&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Forward–backward algorithm. | Lawrence R. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20forward-algorithm/dynamic-programming/algorithms/numerics/2020/05/22/forward-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20forward-algorithm/dynamic-programming/algorithms/numerics/2020/05/22/forward-algorithm.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post57": {
            "title": "Hidden Markov models",
            "content": "Introduction to hidden Markov models . We know that given a sample of independent observations we can get the likelihood of the sample by forming the product of the likelihoods of the individual instances. However, there are situations in which the assumption of observation independence simply breaks down. An example of such a situation is when we consider words from the English dictionary. In this case, within a word, successive letters are dependent; in English h is very likely to follow t but not x. In such a scenario, it is better to assume that the sequence is generated by a parametric random process. Our goal is to establish the parameters of this process. Hidden Markov models are a way to model such a process. Let&#39;s see how. . A hidden Markov model or HMM, is a statistical model in which the system being modeled is assumed to be a Markov process. Let&#39;s call that process with $X$. The system can be on a series of states from a give state set. However, we don&#39;t know at each time instant the specific state the system is in. In other words, the system state is unobservable. HMM assumes that there is another process, say $Y$, whose behavior depends on $X$. The goal is to learn about $X$ by observing $Y$. . Let&#39;s assume that at any time the system or random process we model can be in any of $N$ distinct states. Let&#39;s denote this set with $ mathbb{S}$ . $$ mathbb{S} = {S_1, S_2, cdots,S_N }$$ . Furthermore, let&#39;s denote the state that the system is at time $t$ by $q_t$. . The system can move from one state to another. The probability of being at state $S_j$ at time $t$ depends on the values of the previous states. We express this mathematically using the following conditional probability: . $$P(q_t=S_j | q_{t-1}=S_i, q_{t-2}=S_k, cdots)$$ . A hidden Markov model assume that system states form a Markov chain. To be more specific we will restrict ourselves to the first-order Markov model which is quite frequent in practice. In this case, the probability above simply becomes: . $$P(q_t=S_j | q_{t-1}=S_i, q_{t-2}=S_k, cdots)=P(q_t=S_j | q_{t-1}=S_i)$$ . In words, what the first order Markov property tells us is that the state of the system depends solely on the previous state; a rather memoryless situation. . Let&#39;s move further by introducing the so-called transition probabilities $ alpha_{i,j}$: . $$ alpha_{i,j}=P(q_t=S_j | q_{t-1}=S_i)$$ . Since the $ alpha_{i,j}$&#39;s are probabilities they should satisfy the followin constraints . $$ alpha_{i,j} geq 0, ~~~ sum_{j=1}^{N} alpha_{i,j} = 1$$ . These are nothing more than the usual axioms of the definition of probability. Note that for the latter condition we keep the $i$ index fixed. . We will assume that the transition probabilities are independent of time. What this means is that going from $S_i$ to $S_j$ has the same probability regardless of when it happens (i.e. in the observation sequence see below). . We usually arrange the transition probabilities into an $N times N$ matrix, denoted here with $ mathbf{A}$ that its rows sum to one. . We now have a way, or a model, that allows us to move from one state to another. However, we cannot do much with it as the states are unknown, hidden, unobserved or any other expression that suits your needs. The point is that we cannot access them. In order to have progress, hidden Markov models assume a second process that produces observation sequences. We can use this process to infer the state of the system. . Let&#39;s denote by $ lambda$ ( we will be more specific about what $ lambda$ denotes further below) the HMM instance we are using. Let $O_T$ be an observation sequence of length $T$. We assume that $O_T$ has elements from a given discrete set $ mathbb{V}$: . $$ mathbb{V}= {v_1, v_2, cdots, v_M }$$ . The set $ mathbb{V}$ has in total $M$ elements. Also let&#39;s introduce the mechanism that characterizes the generation of a sequence given a state $S_j$. This is done via the so-called emission probability matrix $b_j(m)$: . $$b_j(m) = P(O_t = v_m|q_t = S_j)$$ . this is the probability that we observe element $v_m$ at time $t$ when the system is at state $S_j$. For example, let&#39;s assume that that we have two states and three symbols and we are given the following emission probabilities matrix . $$ mathbf{B}= begin{bmatrix} 0.16 &amp; 0.26 &amp; 0.58 0.25 &amp; 0.28 &amp; 0.47 end{bmatrix}$$ . what this tells us is that at state $S_1$ symbol $v_1$ has probability 0.16 to be observed, symbol $v_2$ will be observed 26 % and symbol $v_3$ will be observed 58 %. . Although, we cannot observe the state sequence $Q$, this can be inferred from the observation sequence $O$. Note however that in general there are many different sequences $Q$ that can generate the same observation sequence. This is however done with different probabilities. This is similar when we have an iid sample from, say, a normal distribution; there are an infinite number of $ mu, sigma$ pairs possible which can generate the sameple. Thus, we are more interested in a maximum likelihood state sequence or a sequence that has the maximum probability of generating the sequence $O$. . What is $ lambda$? . Above we used the notation $ lambda$ in order to indicate a specific HMM instance. Let&#39;s see what this $ lambda$ parameter actually imply. This is also a summary of the basic element of an HMM. Specifically, . An HMM model assumes a set of states in the model $ mathbb{S} = {S_1, S_2, cdots,S_N }$ | . An HMM model assumes a number of distinct observation symbols $ mathbb{V}= {v_1, v_2, cdots, v_M }$ | . An HMM model assumes the existence of transition probabilities $ mathbf{A}$ where $ alpha_{i,j}=P(q_t=S_j | q_{t-1}=S_i)$ | . An HMM model assumes the existence of observation probabilities $ mathbf{B}$ where $b_j(m) = P(O_t = v_m|q_t = S_j)$ | . The last thing we need to talk about, is how to initialize the model. This is done by a vector of initial probabilties $ boldsymbol{ pi}$ where each $ pi_i = P(q_1 = S_i)$ that is each $ pi_i$ is the probability that the first state of the model is $S_i$. . The $ lambda$ parameter is the triplett consisting of the matrices $ mathbf{A}$, $ mathbf{B}$ and the vector $ boldsymbol{ pi}$ . $$ lambda = { mathbf{A}, mathbf{B}, boldsymbol{ pi} }$$ . For a state set with $N$ states, $ mathbf{A}$ is $N times N$. Likewise for a set $V$ with $M$ symbols, $ mathbf{B}$ is $N times M$. Finally the vector $ boldsymbol{ pi}$ has size $N$. . Typically, when dealing with an HMM we are intersted in the following three problems [1] . Given an HMM i.e. $ lambda$ evaluate the probability of a given observation sequence: | $$P(O| lambda)$$ . Given an HMM and an observation sequence $O$ we want to find the state sequence $Q$ with the highest probability of producing $O$ i.e we want to find $Q$ such that | $$P(Q|O, lambda) ~~ text{is maximum}$$ . Given a training set of observation sequences $ mathbf{X}$ we want to learn the HMM that maximizes the probability of generating $ mathbf{X}$ that is we want to find $ lambda$ so that | $$P( mathbf{X}| lambda)~~ text{is maximum}$$ . Checkout the video below for a motivation about Hidden Markov models . from IPython.display import YouTubeVideo YouTubeVideo(&#39;PAngl8DZ8yk&#39;, width=800, height=300) . The following video explains the Markov property . from IPython.display import YouTubeVideo YouTubeVideo(&#39;J_y5hx_ySCg&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning/algorithms/numerics/2020/05/21/hidden-markov-model.html",
            "relUrl": "/hidden-markov-model/machine-learning/algorithms/numerics/2020/05/21/hidden-markov-model.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post58": {
            "title": "Machine Learning Notes. The  Baum-Welch algorithm",
            "content": "Overview . The third problem associated with Hidden Markov models is that of learning the optimal parameters of the model given a set of sequences. In other words, we want to learn the optimal $ mathbf{B}$ and $ mathbf{A}$ that approximate the given set of sequences. An approach to do this is using the Expectation–Maximization algorithm or EM algorithm. In the context of HMM the algorithm bears the name Baum–Welch algorithm which is a special instance of the EM algorithm. The following video explains the method. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;JRsdt05pMoI&#39;, width=800, height=300) . Baum-Welch algorithm . In the so-called training problem we want to calculate $ lambda$ that maximizes the likelihood of the sample of training sequences $ mathbf{x} = {O_k }$ i.e. we want to maximize $P( mathbf{X}| lambda)$ [1]. Let&#39;s see how this can be done. Let the variable $ xi_t(i,j)$ denote the probability of being at $S_i$ at time $t$ and in $S_j$ at time $t+1$ given $O$ and $ lambda$ i.e. . $$ xi_t(i,j) = P(q_t = S_i, q_{t+1} = S_j | O, lambda)$$ . Note that here $O in mathbf{X}$ denotes just one observation sequence. This can be computed as [1] . $$ xi_t(i,j) = frac{ alpha_t(i)a_{ij}b_j(O_{t+1}) beta_{t+1}(j)}{ sum_{k=1}^{N} sum_{l=1}^N alpha_t(k)a_{kl}b_l(O_{t+1}) beta_{t+1}(l)}$$ . . Remark . Recall that the probability $P(X,Y|Z)$ can be calculated according to . $$P(X,Y|Z) = P(X|Y,Z)P(Y|Z)$$ . from this formula we can write: . $$P(X|Y,Z) = frac{P(X,Y|Z)}{P(Y|Z)}$$ . Thus, . $$P(q_t = S_i , q_{t+1} = S_j | O, lambda) = frac{P(q_t = S_i, q_{t+1} = S_j , O | lambda)}{P(O| lambda)}$$ . The numerator of the equation avove can be expressed using the forward and backward probabilities. . . $ alpha_t(i)$ and $ beta_{t+1}(l)$ are simply the forward and backward variables we saw in the forward and backawrds algorithms respectively. The probability of being in state $S_i$ at time $t$, i.e. $P(q_t=S_i|O, lambda)$, is given by marginalizing over the probabilities for all possible next states [1]: . $$ gamma_t(i) = sum_{j=1}^N xi_t(i,j)$$ . The Baum-Welch algorithm is an EM method. At each iteration we have two steps Expectation, or E-step, followed by a Maximization, or M-step. At E-step, one computes $ xi_t(i,j)$ and $ gamma_t(i)$ values given the current $ lambda$. At the M-step we recalculate $ lambda$ given the estimated $ xi_t(i,j)$ and $ gamma_t(i)$. The E and M steps are alternated until some specified threshold is reached at which point convergence of the algorithm is proclaimed. . Let&#39;s introduce the following two variables [1] . $$z_{i}^{t} = begin{cases} 1, text{if}~ q_t = S_i 0, text{otherwise} end{cases}$$ . and . $$Z_{i,j}^{t} = begin{cases} 1, text{if}~ q_t = S_i ~ text{and}~ q_{t+1}=S_j 0, text{otherwise} end{cases}$$ . We estimate these variables in the E-step as, see [1], . $$E left[z_{i}^{t} right] = gamma_t(i)$$ . $$E left[Z_{ij}^{t} right] = xi_t(i,j)$$ . At the M-step, given the estimation of $ xi$ and $ gamma$, we calculate the parameters of $ lambda$ as follows, see [1]: . $$ hat{a}_{ij} = frac{ sum_{t=1}^{T-1} xi_t(i,j)}{ sum_{t=1}^{T-1} gamma_t(i)}$$ . . Remark . Recall that the transition probability from state $S_i$ to state $S_j$, i.e. $a_{ij}$, is the number of transitions from $S_i$ to $S_j$ divided by the totlal number of transitions from $S_i$ over all sequences: . $$ a_{ij} = frac{ sum_k sum_{t=1}^{T-1} I(q_{t}^k = S_i, q_{t+1}^k = S_j)}{ sum_k sum_{t=1}^{T-1} I(q_{t}^k = S_i )}$$ . . Similarly, we calculate the probability of observing $O_m$ in $S_j$. This is the expected number of times $O_m$ is observed when the system is in $S_j$ over the total number of times the system is in $S_j$ [1]: . $$ hat{b}_j(m) = frac{ sum_{t=1}^T gamma_t(j)I(O_t=O_m)}{ sum_{t=1}^T gamma_t(j)}$$ . In the case where we have more than one observation sequences, say we have $K$ of these in total at our disposal, then the parameters are evaluated as averages over all observations in all sequences [1]: . $$ hat{a}_{ij} = frac{ sum_{k=1}^K sum_{t=1}^{T-1} xi_{t}^k(i,j)}{ sum_{k=1}^K sum_{t=1}^{T-1} gamma_{t}^k(i)}$$ . $$ hat{b}_j(m) = frac{ sum_{k=1}^K sum_{t=1}^T gamma_{t}^k(j)I(O_{t}^{k}=O_m)}{ sum_{k=1}^K sum_{t=1}^T gamma_{t}^{k}(j)}$$ . $$ hat{ pi}_i = frac{ sum_{k=1}^K gamma_{1}^k(i)}{K}$$ . Example . We will continue with the example we used in the Viterbi path notebook. . import numpy as np . obs_to_idx = {&#39;normal&#39;:0, &#39;cold&#39;: 1, &#39;dizzy&#39;:2} # state to index map state_to_idx = {&#39;Healthy&#39;:0, &#39;Fever&#39;:1} . pi = np.array([0.6, 0.4]) # transition probabilities A = np.array([[0.7, 0.3], [0.4, 0.6]]) # emission probabilties B = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]) . # the sequence X = [[&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;], [&#39;normal&#39;, &#39;cold&#39;, &#39;cold&#39;], [&#39;cold&#39;, &#39;cold&#39;, &#39;dizzy&#39;], [&#39;normal&#39;, &#39;normal&#39;, &#39;normal&#39;]] . def idx_to_obs(idx): global obs_to_idx for item in obs_to_idx: if obs_to_idx[item] == idx: return item return None . def forward(o, A, B, pi, state_to_idx, obs_to_idx): a = np.zeros(shape=(len(o), A.shape[0])) for i in range(len(state_to_idx)): a[0][i] = pi[i] * B[i][obs_to_idx[o[0]]] for t in range(1, len(o)): for j in range(A.shape[0]): a[t][j] = 0 # fix j = state_idx and sum over the states for i in range(A.shape[0]): a[t][j] += a[t - 1][i] * A[i][j] a[t][j] *= B[j][obs_to_idx[o[t]]] return a . def backward(o, A, B, obs_to_idx): beta = np.zeros(shape=(len(o), A.shape[0])) # initialize beta beta[len(o) - 1] = np.ones(A.shape[0]) for t in range(len(o) - 2, -1, -1): for j in range(A.shape[0]): beta[t, j] = (beta[t + 1] * B[:, obs_to_idx[o[t + 1]]]).dot(A[j, :]) return beta . def compute_xi_denom(alpha, A, B, beta, t, idx, idx2): denom = 0.0 for i in range(A.shape[0]): for j in range(A.shape[0]): denom += alpha[t, i]*A[i, j]*B[j, idx]*beta[idx2, j] return denom . def sequnce_xi_gamma_calculation(o, alpha, beta, A, B, obs_to_idx): N = A.shape[0] T = len(o) xi = [] #[np.zeros((N, N))]*(T-1) gamma = np.zeros((T-1, N)) # calculate xi and gamma for the sequence for t in range(T-1): local_xi = np.zeros((N, N)) for i in range(N): for j in range(N): numerator = alpha[t, i] * A[i, j] * B[j, obs_to_idx[o[t + 1]]] * beta[t + 1, j] denom = compute_xi_denom(alpha=alpha, A=A, B=B, beta=beta, t=t, idx=obs_to_idx[o[t + 1]], idx2=t+1) local_xi[i, j] = numerator / denom for k in range(N): gamma[t][i] += local_xi[i][k] xi.append(local_xi) return xi, gamma . def get_extra_gammas(alpha, beta, A, B, obs_to_idx, X): N = A.shape[0] extra_gammas = [] for k in range(len(X)): gamma_k = [0.0 for i in range(N)] seq = X[k] T = len(seq) t = T-2 # we need to compute the xi # for the last observation xi_k = np.zeros(shape=(N, N)) for i in range(N): for j in range(N): numerator = alpha[t, i] * A[i, j] * B[j, obs_to_idx[seq[t + 1]]] * beta[t + 1, j] denom = compute_xi_denom(alpha=alpha, A=A, B=B, beta=beta, t=t, idx=obs_to_idx[seq[t + 1]], idx2=t+1) xi_k[i, j] = numerator / denom for kg in range(N): gamma_k[i] += xi_k[i][kg] extra_gammas.append(gamma_k) return extra_gammas . def baum_welch(X, A, B, pi, n_iter, state_to_idx, obs_to_idx): print(&quot;Number of sequences: &quot;, len(X)) N = A.shape[0] M = B.shape[1] print(&quot;shape A: &quot;, A.shape) print(&quot;shape B: &quot;, B.shape) #import pdb #pdb.set_trace() for itr in range(n_iter): print(&quot;================&quot;) print(&quot;Iteration: &quot;, itr) # collect the xis and gammas for # evety sequence given in this iteration xis = [] gammas = [] for seq in range(len(X)): o = X[seq] T = len(o) alpha = forward(o, A, B, pi=pi, state_to_idx=state_to_idx, obs_to_idx=obs_to_idx) beta = backward(o, A, B, obs_to_idx=obs_to_idx) assert alpha.shape[0] == len(o), &quot;Invalid number of rows for alpha mat: {0} should be {1}&quot;.format(alpha.shape[0], len(o)) assert alpha.shape[1] == N, &quot;Invalid number of columns for alpha mat: {0} should be {1}&quot;.format( alpha.shape[1], N) assert beta.shape[0] == len(o), &quot;Invalid number of rows for beta mat: {0} should be {1}&quot;.format(beta.shape[0], len(o)) assert beta.shape[1] == N, &quot;Invalid number of columns for beta mat: {0} should be {1}&quot;.format( beta.shape[1], N) # calculate xi and gamma for this observation sequence xi, gamma = sequnce_xi_gamma_calculation(o=o, alpha=alpha, beta=beta, A=A, B=B, obs_to_idx=obs_to_idx) # xi is an array of length T-1 assert len(xi) == T - 1, &quot;Invalid size of xi array: {0} should be {1}&quot;.format(len(xi), T - 1) # for each t we computed xi which is an NxN matrix assert xi[0].shape == (N, N), &quot;Invalid xi matrix shape: {0} should be {1}&quot;.format(xi[0].shape, (N,N)) # gamma is a matrix T-1 x N assert gamma.shape == (T - 1, N), &quot;Invalid gamma matrix shape: {0} should be {1}&quot;.format(gamma.shape, (T - 1, N)) xis.append(xi) gammas.append(gamma) assert len(gammas) == len(X), &quot;Invalid number of gammas {0} should be {1}&quot;.format(len(gammas), len(X)) extra_gammas = get_extra_gammas(alpha=alpha, beta=beta, A=A, B=B, obs_to_idx=obs_to_idx, X=X) assert len(extra_gammas) == len(X), &quot;Invalid number of extra gammas {0} should be {1}&quot;.format(len(extra_gammas), len(X)) # update the transition probabilities # the calculations for A have up to T -1 for i in range(N): denom = 0.0 for k in range(len(X)): obs = X[k] T = len(obs) gamma_k = gammas[k] assert gamma_k.shape == (T - 1, N), &quot;Invalid gamma matrix &quot; &quot;shape: {0} should be {1}&quot;.format(gamma_k.shape, (T - 1, N)) for obs_item in range(T - 1): denom += gamma_k[obs_item][i] for j in range(A.shape[1]): nom = 0.0 for k in range(len(X)): obs = X[k] xi_k = xis[k] T = len(obs) assert len(xi_k) == T - 1, &quot;Invalid number of &quot; &quot;xi_k&#39;s: {0} should be {1}&quot;.format(len(xi_k), T - 1) for t in range(T - 1): nom += xi_k[t][i, j] A[i, j] = nom/denom # update the gammas so that we have the # final observations for k in range(len(gammas)): gamma_k = gammas[k] gamma_k = np.vstack([gamma_k, extra_gammas[k]]) gammas[k] = gamma_k # in order to calculate B we also need the final # bits for gammas for i in range(N): denom = 0.0 for k in range(len(X)): seq = X[k] gamma_k = gammas[k] assert len(seq) == gamma_k.shape[0], &quot;Invalid number of Gamma rows {0} should be {1}&quot;.format(gamma_k.shape[0], len(seq)) for t in range(len(seq)): denom += gamma_k[t, i] for j in range(M): nom = 0.0 for k in range(len(X)): seq = X[k] for t in range(len(seq)): obs = seq[t] if obs == idx_to_obs(j): nom += gammas[k][t, i] B[i, j] = nom/denom # update the pi vector for i in range(N): pi_val = 0.0 for k in range(len(X)): gamma_k = gammas[k] pi_val += gamma_k[0][i] pi[i] = pi_val/(len(X)) return A, B, pi . baum_welch(X=X, A=A, B=B, pi=pi, n_iter=10, state_to_idx=state_to_idx, obs_to_idx=obs_to_idx) . Number of sequences: 4 shape A: (2, 2) shape B: (2, 3) ================ Iteration: 0 ================ Iteration: 1 ================ Iteration: 2 ================ Iteration: 3 ================ Iteration: 4 ================ Iteration: 5 ================ Iteration: 6 ================ Iteration: 7 ================ Iteration: 8 ================ Iteration: 9 . (array([[0.60187444, 0.39812556], [0.71039728, 0.28960272]]), array([[0.50518563, 0.30097656, 0.19383781], [0.06660769, 0.87417724, 0.05921507]]), array([0.96748237, 0.03251763])) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Wikipedia-Baum–Welch algorithm. |",
            "url": "https://pockerman.github.io/qubit_opus/machine-learning/hidden-markov-model/algorithms/2020/05/13/ml-notes-hmm-baum-welch-algorithm.html",
            "relUrl": "/machine-learning/hidden-markov-model/algorithms/2020/05/13/ml-notes-hmm-baum-welch-algorithm.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post59": {
            "title": "Machine Learning Notes. Confusion Matrix",
            "content": "Overview . More than often, we want to evaluate the performance of a classifier. This can be summarised by using a table knowm as contingency table or confusion matrix . Confusion matrix . A confusion matrix is a table where each row refers to actual classes as recorder in the test set, and each column to classes as predicted by the classifier. When we have the confusion matrix computed, we can extract various perormance metrics. We will illustrate this with an example concerning two-class classification. Most, however not all, of the metrics can easilly be extended to more than two classes classification. The following table summarizes various metrics . Accuracy . Accuracy is perhaps the simplest of the metrics we can compute [2]. It is defined s the proportion of correctly classified instances: . $$ text{Accuracy} = frac{ text{Number of correctly classified examples}}{ text{Total number of examples}}$$ . Similarly, we can define the error rate as the proportion of incorrectly classified insrances. Clearly, this will be given by: . $$ text{Error rate} = 1 - text{accuracy}$$ . Conceptually, we can view accuracy as an estimate of the probability that an arbitrary instance $ mathbf{x} in mathbf{X}$ is classified correctly i.e. as the probability [2] . $$P( hat{c}( mathbf{x}) = c( mathbf{x})| hat{f})$$ . References . Confusion matrix | Peter Flach, Machine Learning The Art and Science of Algorithms that Make Sense of Data, Cambridge Press |",
            "url": "https://pockerman.github.io/qubit_opus/machine-learning/confusion-matrix/classifier-assessment/2020/03/25/ml-confusion-matrix.html",
            "relUrl": "/machine-learning/confusion-matrix/classifier-assessment/2020/03/25/ml-confusion-matrix.html",
            "date": " • Mar 25, 2020"
        }
        
    
  
    
        ,"post60": {
            "title": "Matrix approximation with SVD",
            "content": "Revisions . Fix paragraphs 2021-08-29 | . Overview . Very often in numerical modeling, the data we need to work with is rather large and therefore not really handy. Moreover, the behavior under investigation can be explained by small number of features. Thus, it is desireable to be able to approximate the matrices. One way to do so is using the singular value decomposition method. . Matrix approximation with SVD . In fact SVD provides an optimal low-rank approaximation to a matrix $A$ (this is the Eckart-Young theorem). We can obtain a hierarchy of low rank matrices by just keeping the leading $k$ singular values and the corresponding eigenvectors. . Image copression is a simple example illustrating matrix approximation using SVD. We can view a grayscale image as matrix $A in mathbb{R}^{n times m}$ where $n, m$ are the number of pixels in the vertical and horizonal directions. . The Python code below computes the full SVD of the matrix representing the loaded image. We then compute approximations of the image using a range of retained singular values. We can see that as the number of retained singular values increases the quality of the image increases. . import numpy as np from matplotlib import image import matplotlib.pyplot as plt from numpy.linalg import matrix_rank . def rgb2gray(rgb): return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]) . A = image.imread(&#39;my_icons/volvo_img.png&#39;) A = np.mean(A, -1)#rgb2gray(rgb=data) # summarize shape of the pixel array print(A.shape) # display the array of pixels as an image plt.imshow(A) plt.show() . (366, 424) . print(&quot;Matrix rank &quot;, matrix_rank(A)) . Matrix rank 307 . U, S, V = np.linalg.svd(A, full_matrices=False) S = np.diag(S) . print(&quot;Shape U &quot;, U.shape) print(&quot;Shape S &quot;, S.shape) print(&quot;Shape V &quot;, V.shape) . Shape U (366, 366) Shape S (366, 366) Shape V (366, 424) . for r in [5, 20, 100]: print(&quot;Working with r&quot;, r) # construct approximate image img_approx = U[:,:r] @ S[0:r,:r] @ V[:r,:] plt.imshow(img_approx) plt.show() . Working with r 5 . Working with r 20 . Working with r 100 . plt.semilogy(np.diag(S)) plt.show() . plt.plot(np.cumsum(np.diag(S))/np.sum(np.diag(S))) plt.show() . References .",
            "url": "https://pockerman.github.io/qubit_opus/linear-algebra/singular-value-decomposition/matrix-approximation/algorithms/numerics/2020/03/14/matrix-approximation.html",
            "relUrl": "/linear-algebra/singular-value-decomposition/matrix-approximation/algorithms/numerics/2020/03/14/matrix-approximation.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post61": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pockerman.github.io/qubit_opus/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi and welcome to my blog. This is Alex. I am a self-taught, at least to a large extent, software engineer. I am really intersted in developing scientific applications (that do not involve drop down menus), mathematical modeling, machine learning and AI. I am also heavily interested in robotics, here is my non-working and still under-development-whenever-I-have-the-time, attempt. Admitedly, I still have many thing to learn. In this blog, I write for myself, things that I work, or worked, on and find difficult to grasp, or things that I read and want to make my understanding more tangible by…explaining them to myself. In any case, if you find useful any information in here, feel free to use it. . You can reach me, optional :), via Linkedin or via GitHub. .",
          "url": "https://pockerman.github.io/qubit_opus/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Books",
          "content": "This is a (mixed) list of books that I think is worthwhile reading them. . Kevin P. Murphy Machine learning. A probabilistic perspective | Richard S. Sutton and Andrew G. Barto, Reinforcement learning: An introduction | Leonard Susskind and George Hrabovsky, Classical mechanics. The theoretical minimum | Leonard Susskind and Art Friedman, Quantum mechanics. The theoretical minimum | Robert C. Martin Clean Architecture. A Craftsman&#39;s Guide to Software Architecture and Design | Scott Meyers Effective C++: 50 specific ways to improve your programs and design | Scott Meyers Effective STL: 50 specific ways to improve your use of the STL | Scott Meyers Effective C++: 55 specific ways to improve your programs and design | Scott Meyers Effective modern C++ | Bjorn Andrist and Viktor Sehr C++ high performance | Herodotus The histories | Thucydides The histroy of the peloponnesian war | Henry Morgenthau Ambassador Morgenthau&#39;s story | Sun Tzu The art of war | Nicolo Machiavelli The prince | Antoine De Saint-Exupery Ο Μικρός πρίγγιπας | Albert Camus The fall | Albert Camus The outsider | George Orwell 1984 | Douglas R. Hostadter Godel, Escher, Bach: An enternal golden braid | Fyodor Dostoyevsky Crime and punishment | Fyodor Dostoyevsky The idiot | Ernest Hemingway For whom the bell tolls | Ernest Hemingway The old man and the sea | .",
          "url": "https://pockerman.github.io/qubit_opus/books/",
          "relUrl": "/books/",
          "date": ""
      }
      
  

  

  
      ,"page4": {
          "title": "Programming",
          "content": "",
          "url": "https://pockerman.github.io/qubit_opus/programming/",
          "relUrl": "/programming/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  

  

  

  

  
  

  
      ,"page16": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pockerman.github.io/qubit_opus/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}