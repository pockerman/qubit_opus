{
  
    
        "post0": {
            "title": "Deutsch's Algorithm",
            "content": "Quantum computers pose as the future of computing. Although, at the time of writing, the computing harwdare based on quantum mechanics principles can accommodate only a small number of qubits, algorithms have been developed that demonstrate the superiority of quantum computers for certain class problems. . One such algorithm, and perhaps the simplest one is Deutsch&#39;s algorithm. The algorithm solves the following problem [1] . Given a boolean function $f: {0,1 } rightarrow {0,1 }$ determine if $f$ is constant. . The algorithm, can solve the problem with fewer calls to the function $f$ than is possible on a classical machine [1]. A function is called constant if $f(0) = f(1)$. On the other hand, if $f$ is one-to-one, is called balanced [1]. . Using a classical computer we need to do two evaluations of the function; one for each of the two inputs [1, 2]. On the other hand, Deutsch&#39;s algorithm requires only a single call to a black box to solve the problem. The key to the algorithm is the ability to place the second qubit of the input to the black box in a superposition [2]. Let&#39;s see how to do this. . Deutsch&#39;s algorithm works by putting both qubits representing the two inputs into a superposition [1]. The way to do this is using the Hadamard gate. The following image shows this schematically. . Figure 1. Deutsch&#39;s algorithm circuit. Image from [1]. . Let&#39;s study how the state system $| psi rangle$ evolves. Initially the system is at . $$| psi rangle = |01 rangle$$ . Appication of the Hadamard gate moves the two qubits respectively to . $$|0 rangle = frac{|0 rangle + |1 rangle}{ sqrt{2}}$$ . $$ |1 rangle = frac{|0 rangle - |1 rangle}{ sqrt{2}}$$ . Thus, $| psi rangle$ will be at . $$| psi rangle = left[ frac{|0 rangle + |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . Let&#39;s rename the top qubit as $|x rangle$. We want to evaluate $f(x)$. Note that when the bottom qubit is put into a superposition and then multiply by $U_f$, the system will be at state [1] . $$| psi rangle = (-1)^{f(x)}|x rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . Given however that $|x rangle$ is also in superposition, we will have that the system will be at state [1] . $$| psi rangle = left[ frac{(-1)^{f(0)}|0 rangle + (-1)^{f(1)}|1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . The actual state, as shown in the equation above, depends on the values of $f$. We can summarize this as follows [1]. . $$| psi rangle = begin{cases} ( pm1) left[ frac{|0 rangle + |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] ( pm1) left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] end{cases}$$ The final step is to apply the Hadamard gate on the top qubit. Recall that the Hadamard matrix is its own inverse. Thus applying it to the top qubit we get [1] . $$| psi rangle = begin{cases} ( pm1) |0 rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right], ~~ text{if} ~~ f ~~ text{is constant} ( pm1) |1 rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right], ~~ text{if} ~~ f ~~ text{is balanced} end{cases}$$ Now, we simply measure the top qubit. If it is in state $|0 rangle$, then we know that f is a constant function [1]. This was all accomplished with only one function evaluation. . One of the nice points demonstared by the algorithm is that a change of basis can allow solving a problem that otherwise requires more questions to the oracle. In Deutsch algorithm, we start in the canonical basis $|01 rangle$. The first application of the Hadamard matrices is used to change the basis to go into a balanced superposition of basic states. While in this noncanonical basis, we evaluate $f$ with the bottom qubit. The last Hadamard matrix is used as a change of basis matrix to revert back to the canonical basis [1]. . import numpy as np import random . H = np.array([[1.0/np.sqrt(2.0), 1.0/np.sqrt(2.0)], [1.0/np.sqrt(2.0), - 1.0/np.sqrt(2.0)]]) . def oracle(x, y, constant): if constant: f0 = 0 #random.choice([0,1]) f1 = 0 #random.choice([0,1]) else: f0 = 0 #random.choice([0,1]) f1 = 1 #random.choice([0,1]) return np.array([(-1)**f0*x[0], (-1)**f1*x[1]]) . zero = np.array([1., 0.]) one = np.array([0.0, 1.0]) . zero_H = np.dot(H, zero) one_H = np.dot(H, one) . print(zero_H) print(one_H) . [0.70710678 0.70710678] [ 0.70710678 -0.70710678] . out_oracle = oracle(x=zero_H, y=one_H, constant=True) . x = np.dot(H, out_oracle) . print(x) . [1. 0.] . out_oracle = oracle(x=zero_H, y=one_H, constant=False) . x = np.dot(H, out_oracle) . print(x) . [0. 1.] . References . Noson S. Yanofsky and Mirco A. Mannucci, Quantum Computing for Computer Scientists, Cambridge University Press | Eleanor Rieffel, Wolfgang Polak, Quantum Computing: A Gentle Introduction, The MIT Press. | Deutsch&#39;s algorithm |",
            "url": "https://pockerman.github.io/qubit_opus/deutsch/quantum-computing/algorithms/numerics/2021/03/20/deutsch-algo.html",
            "relUrl": "/deutsch/quantum-computing/algorithms/numerics/2021/03/20/deutsch-algo.html",
            "date": " • Mar 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Singular Value Decomposition",
            "content": "One of the most important matrix factorization techniques is the singular value decomposition most often abbreviated as SVD. The reason why is so popular lies on the fact that it is the foundation for many other computational techniques. For example, just to name a few: . Computing pseudo-inverses | Obtaining low-rank matrix approximations | Dynamic mode decomposition | Proper orthogonal ecomposition | Principal components analysis | . For a complex matrix $A in mathbb{C}^{n times m}$, its SVD is . $$A = U Sigma V^{*}$$ . where $V^{*}$ is the complex conjugate transpose. Both $U$ and $V$ are unitary matrices that is the following holds . $$UU^{*} = U^{*}U = I$$ . In general, if a matrix $W$ is a real matrix i.e. its entries are real numbers, then $W^{*} = W^T$. Thus, if $A in mathbb{R}^{n times m}$ the matrices $U$ and $V$ are real orthogonal matrices i.e. . $$UU^{T} = U^{T}U = I$$ . The matrix $ Sigma$ is a diagonal matrix with real and nonnegative entries on the diagonal. The entries $ Sigma_{ii}$ are called the singular values of $A$. The number of the non-zero singular values corresponds to the rank of the matrix $A$. . Given the popularity of the SVD method, it is not surpsising that most linear algebra libraries provide a way to perform it. The following script shows how to compute the SVD in Python using numpy . import numpy as np X = np.random.rand(10 , 10) U, S, V = np.linalg.svd(X, full_matrices=True) # or doing economy SVD U, S, V = np.linalg.svd(X, full_matrices=False) . You can find the documentation at numpy.linalg.svd. Similarly, using the Blaze C++ library . template&lt; typename MT1, bool SO, typename VT, bool TF, typename MT2, typename MT3 &gt; void svd( const DenseMatrix&lt;MT1,SO&gt;&amp; A, DenseMatrix&lt;MT2,SO&gt;&amp; U, DenseVector&lt;VT,TF&gt;&amp; s, DenseMatrix&lt;MT3,SO&gt;&amp; V ); . Overall, the SVD algorithm is a very important matrix decomposition technique used throughout numerical modeling control theory and system identification. We will see applications of the method in future posts. .",
            "url": "https://pockerman.github.io/qubit_opus/linear-algebra/singular-value-decomposition/algorithms/numerics/2021/03/13/singular-value-decomposition.html",
            "relUrl": "/linear-algebra/singular-value-decomposition/algorithms/numerics/2021/03/13/singular-value-decomposition.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pockerman.github.io/qubit_opus/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Matrix Approximation",
            "content": "Very often in numerical modeling, the data we need to work with is rather large and therefore not really handy. Moreover, the behavior under investigation can be explained by small number of features. Thus, it is desireable to be able to approximate the matrices. One way to do so is using the singular value decomposition method. . In fact SVD provides an optimal low-rank approaximation to a matrix $A$ (this is the Eckart-Young theorem). We can obtain a hierarchy of low rank matrices by just keeping the leading $k$ singular values and the corresponding eigenvectors. . Image copression is a simple example illustrating matrix approximation using SVD. We can view a grayscale image as matrix $A in mathbb{R}^{n times m}$ where $n, m$ are the number of pixels in the vertical and horizonal directions. . The Python code below computes the full SVD of the matrix representing the loaded image. We then compute approximations of the image using a range of retained singular values. We can see that as the number of retained singular values increases the quality of the image increases. . import numpy as np from matplotlib import image import matplotlib.pyplot as plt from numpy.linalg import matrix_rank . def rgb2gray(rgb): return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]) . A = image.imread(&#39;my_icons/volvo_img.png&#39;) A = np.mean(A, -1)#rgb2gray(rgb=data) # summarize shape of the pixel array print(A.shape) # display the array of pixels as an image plt.imshow(A) plt.show() . (366, 424) . print(&quot;Matrix rank &quot;, matrix_rank(A)) . Matrix rank 307 . U, S, V = np.linalg.svd(A, full_matrices=False) S = np.diag(S) . print(&quot;Shape U &quot;, U.shape) print(&quot;Shape S &quot;, S.shape) print(&quot;Shape V &quot;, V.shape) . Shape U (366, 366) Shape S (366, 366) Shape V (366, 424) . for r in [5, 20, 100]: print(&quot;Working with r&quot;, r) # construct approximate image img_approx = U[:,:r] @ S[0:r,:r] @ V[:r,:] plt.imshow(img_approx) plt.show() . Working with r 5 . Working with r 20 . Working with r 100 . plt.semilogy(np.diag(S)) plt.show() . plt.plot(np.cumsum(np.diag(S))/np.sum(np.diag(S))) plt.show() .",
            "url": "https://pockerman.github.io/qubit_opus/linear-algebra/singular-value-decomposition/matrix-approximation/algorithms/numerics/2013/03/14/matrix-approximation.html",
            "relUrl": "/linear-algebra/singular-value-decomposition/matrix-approximation/algorithms/numerics/2013/03/14/matrix-approximation.html",
            "date": " • Mar 14, 2013"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://pockerman.github.io/qubit_opus/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Books",
          "content": "If you could only read a hundred books in your life which books will these be? Bottom line is make it worh. A collection of books that I believe is worth spending some time with. . Computing . History . Mathematics . Physics . Literature .",
          "url": "https://pockerman.github.io/qubit_opus/books/",
          "relUrl": "/books/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pockerman.github.io/qubit_opus/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}