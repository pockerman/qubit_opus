{
  
    
        "post0": {
            "title": "Deutsch's Algorithm",
            "content": "Quantum computers pose as the future of computing. Although, at the time of writing, the computing harwdare based on quantum mechanics principles can accommodate only a small number of qubits, algorithms have been developed that demonstrate the superiority of quantum computers for certain class problems. . One such algorithm, and perhaps the simplest one is Deutsch&#39;s algorithm. The algorithm solves the following problem [1] . Given a boolean function $f: {0,1 } rightarrow {0,1 }$ determine if $f$ is constant. . The algorithm, can solve the problem with fewer calls to the function $f$ than is possible on a classical machine [1]. A function is called constant if $f(0) = f(1)$. On the other hand, if $f$ is one-to-one, is called balanced [1]. . Using a classical computer we need to do two evaluations of the function; one for each of the two inputs [1, 2]. On the other hand, Deutsch&#39;s algorithm requires only a single call to a black box to solve the problem. The key to the algorithm is the ability to place the second qubit of the input to the black box in a superposition [2]. Let&#39;s see how to do this. . Deutsch&#39;s algorithm works by putting both qubits representing the two inputs into a superposition [1]. The way to do this is using the Hadamard gate. The following image shows this schematically. . Figure 1. Deutsch&#39;s algorithm circuit. Image from [1]. . Let&#39;s study how the state system $| psi rangle$ evolves. Initially the system is at . $$| psi rangle = |01 rangle$$ . Appication of the Hadamard gate moves the two qubits respectively to . $$|0 rangle = frac{|0 rangle + |1 rangle}{ sqrt{2}}$$ . $$ |1 rangle = frac{|0 rangle - |1 rangle}{ sqrt{2}}$$ . Thus, $| psi rangle$ will be at . $$| psi rangle = left[ frac{|0 rangle + |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . Let&#39;s rename the top qubit as $|x rangle$. We want to evaluate $f(x)$. Note that when the bottom qubit is put into a superposition and then multiply by $U_f$, the system will be at state [1] . $$| psi rangle = (-1)^{f(x)}|x rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . Given however that $|x rangle$ is also in superposition, we will have that the system will be at state [1] . $$| psi rangle = left[ frac{(-1)^{f(0)}|0 rangle + (-1)^{f(1)}|1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . The actual state, as shown in the equation above, depends on the values of $f$. We can summarize this as follows [1]. . $$| psi rangle = begin{cases} ( pm1) left[ frac{|0 rangle + |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] ( pm1) left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] end{cases}$$ The final step is to apply the Hadamard gate on the top qubit. Recall that the Hadamard matrix is its own inverse. Thus applying it to the top qubit we get [1] . $$| psi rangle = begin{cases} ( pm1) |0 rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right], ~~ text{if} ~~ f ~~ text{is constant} ( pm1) |1 rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right], ~~ text{if} ~~ f ~~ text{is balanced} end{cases}$$ Now, we simply measure the top qubit. If it is in state $|0 rangle$, then we know that f is a constant function [1]. This was all accomplished with only one function evaluation. . One of the nice points demonstared by the algorithm is that a change of basis can allow solving a problem that otherwise requires more questions to the oracle. In Deutsch algorithm, we start in the canonical basis $|01 rangle$. The first application of the Hadamard matrices is used to change the basis to go into a balanced superposition of basic states. While in this noncanonical basis, we evaluate $f$ with the bottom qubit. The last Hadamard matrix is used as a change of basis matrix to revert back to the canonical basis [1]. . import numpy as np import random . H = np.array([[1.0/np.sqrt(2.0), 1.0/np.sqrt(2.0)], [1.0/np.sqrt(2.0), - 1.0/np.sqrt(2.0)]]) . def oracle(x, y, constant): if constant: f0 = 0 #random.choice([0,1]) f1 = 0 #random.choice([0,1]) else: f0 = 0 #random.choice([0,1]) f1 = 1 #random.choice([0,1]) return np.array([(-1)**f0*x[0], (-1)**f1*x[1]]) . zero = np.array([1., 0.]) one = np.array([0.0, 1.0]) . zero_H = np.dot(H, zero) one_H = np.dot(H, one) . print(zero_H) print(one_H) . [0.70710678 0.70710678] [ 0.70710678 -0.70710678] . out_oracle = oracle(x=zero_H, y=one_H, constant=True) . x = np.dot(H, out_oracle) . print(x) . [1. 0.] . out_oracle = oracle(x=zero_H, y=one_H, constant=False) . x = np.dot(H, out_oracle) . print(x) . [0. 1.] . References . Noson S. Yanofsky and Mirco A. Mannucci, Quantum Computing for Computer Scientists, Cambridge University Press | Eleanor Rieffel, Wolfgang Polak, Quantum Computing: A Gentle Introduction, The MIT Press. | Deutsch&#39;s algorithm |",
            "url": "https://pockerman.github.io/qubit_opus/deutsch/quantum-computing/algorithms/numerics/2021/03/20/deutsch-algo.html",
            "relUrl": "/deutsch/quantum-computing/algorithms/numerics/2021/03/20/deutsch-algo.html",
            "date": " • Mar 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Singular Value Decomposition",
            "content": "One of the most important matrix factorization techniques is the singular value decomposition most often abbreviated as SVD. The reason why is so popular lies on the fact that it is the foundation for many other computational techniques. For example, just to name a few: . Computing pseudo-inverses | Obtaining low-rank matrix approximations | Dynamic mode decomposition | Proper orthogonal ecomposition | Principal components analysis | . For a complex matrix $A in mathbb{C}^{n times m}$, its SVD is . $$A = U Sigma V^{*}$$ . where $V^{*}$ is the complex conjugate transpose. Both $U$ and $V$ are unitary matrices that is the following holds . $$UU^{*} = U^{*}U = I$$ . In general, if a matrix $W$ is a real matrix i.e. its entries are real numbers, then $W^{*} = W^T$. Thus, if $A in mathbb{R}^{n times m}$ the matrices $U$ and $V$ are real orthogonal matrices i.e. . $$UU^{T} = U^{T}U = I$$ . The matrix $ Sigma$ is a diagonal matrix with real and nonnegative entries on the diagonal. The entries $ Sigma_{ii}$ are called the singular values of $A$. The number of the non-zero singular values corresponds to the rank of the matrix $A$. . Given the popularity of the SVD method, it is not surpsising that most linear algebra libraries provide a way to perform it. The following script shows how to compute the SVD in Python using numpy . import numpy as np X = np.random.rand(10 , 10) U, S, V = np.linalg.svd(X, full_matrices=True) # or doing economy SVD U, S, V = np.linalg.svd(X, full_matrices=False) . You can find the documentation at numpy.linalg.svd. Similarly, using the Blaze C++ library . template&lt; typename MT1, bool SO, typename VT, bool TF, typename MT2, typename MT3 &gt; void svd( const DenseMatrix&lt;MT1,SO&gt;&amp; A, DenseMatrix&lt;MT2,SO&gt;&amp; U, DenseVector&lt;VT,TF&gt;&amp; s, DenseMatrix&lt;MT3,SO&gt;&amp; V ); . Overall, the SVD algorithm is a very important matrix decomposition technique used throughout numerical modeling control theory and system identification. We will see applications of the method in future posts. .",
            "url": "https://pockerman.github.io/qubit_opus/linear-algebra/singular-value-decomposition/algorithms/numerics/2021/03/13/singular-value-decomposition.html",
            "relUrl": "/linear-algebra/singular-value-decomposition/algorithms/numerics/2021/03/13/singular-value-decomposition.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Gradient Descent",
            "content": "Perhaps the simplest algorithm for uncostrained optimization is gradient descent also known as steepest descent. Consider the following function [1] . $$f( theta_1, theta_2) = frac{1}{2}( theta_{1}^2 - theta_2)^2 + frac{1}{2}( theta_1 -1)^2$$ . We are interested in finding $ theta_1, theta_2$ that minimize $f$. Gradient descent is an iterative algorithm that uses the gradient of the function in order to update the parameters. The update rule is . $$ boldsymbol{ theta}_k = boldsymbol{ theta}_{k-1} - eta nabla f|_{ boldsymbol{ theta}_{k-1}} $$ . $ eta$ is the so called learning rate and tunes how fast we move to the direction of the gradient. A small $ eta$ slows down convergence whilst a large value may not allow convergence of the algorithm. This is shown in the two figures below: . Figure 1. Gradient descent with eta 0.1. . Figure 1. Gradient descent with eta 0.6. . References . Kevin P. Murphy, Machine Learning A Probabilistic Perspective, The MIT Press |",
            "url": "https://pockerman.github.io/qubit_opus/gradient-descent/unconstrained-optimization/machine-learning/algorithms/numerics/2020/06/22/gradient-descent.html",
            "relUrl": "/gradient-descent/unconstrained-optimization/machine-learning/algorithms/numerics/2020/06/22/gradient-descent.html",
            "date": " • Jun 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Backward Algorithm",
            "content": "The backward algorithm is the complement of the forward algorithm. Let&#39;s introduce the backward variable $ beta_t(i)$. This is the probability of being in $S_i$ at time $t$ abd observing the partial sequence $O_{t+1}, cdots,O_T$ [1]. This can be written as . $$ beta_t(i) = P(O_{t+1}, cdots,O_T | q_t=S_i, lambda)$$ . The backward algorithm computes this recursively . Initialize | . $$ beta_T(i) = 1$$ . Recurse | . $$ beta_t(i) = P(O_{t+1}, cdots,O_T | q_t=S_i, lambda)$$ . which can be written as, see [1], . $$ beta_{t}(i) = sum_{j}^{N}b_j(O_{t+1})a_{i,j} beta_{t+1}(j)$$ . Let&#39;s implement this as we did for the forward algorithm. . Assume a system with two states $S= {S_0, S_1 }$. Futher, assume that the observation sequence consists of elements from the following set $V= {a, b,c }$. Also let&#39;s assume the following HMM: . $$ boldsymbol{ pi}= begin{bmatrix}0.6 &amp; 0.4 end{bmatrix}$$ . $$ mathbf{A}= begin{bmatrix}0.7 &amp; 0.3 0.4 &amp; 0.6 end{bmatrix}$$ . $$ mathbf{B}= begin{bmatrix} 0.5 &amp; 0.4 &amp; 0.1 0.1 &amp; 0.3 &amp; 0.6 end{bmatrix}$$ . Assume the following sequence $V= {a, b, c }$. We introduce the $ beta$ matrix: . $$ beta = begin{bmatrix}0 &amp; 0 0 &amp; 0 0 &amp; 0 end{bmatrix}$$ . First we initialize . $$ beta = begin{bmatrix}0 &amp; 0 0 &amp; 0 1 &amp; 1 end{bmatrix}$$ . Then use the recursion formula . $$ beta_{t}(i) = sum_{j}^{N}b_j(O_{t+1})a_{i,j} beta_{t+1}(j)$$ . import numpy as np . obs_to_idx = {&#39;a&#39;:0, &#39;b&#39;: 1, &#39;c&#39;:2} # state to index map state_to_idx = {&#39;S0&#39;:0, &#39;S1&#39;:1} . pi = np.array([0.6, 0.4]) # transition probabilities A = np.array([[0.7, 0.3], [0.4, 0.6]]) # emission probabilties B = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]) . o = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] . beta = np.zeros(shape=(len(o),A.shape[0])) . beta[len(o) - 1] = np.ones(A.shape[0]) . # start at the position one before the end # proceed until t is -1. Move back one step at the time for t in range(len(o)-2, -1, -1): for j in range(A.shape[0]): # for x = np.array([1.,2.]) and # y = np.array([1.,2.])then # x*y = np.array([1., 4.]) # that is element-wise product is performed beta[t, j] = (beta[t + 1] * B[:, obs_to_idx[o[t + 1]]]).dot(A[j, :]) . print(beta) . [[0.106 0.112] [0.25 0.4 ] [1. 1. ]] . Overall the forward and backward algorithms can be used to compute $P(O| lambda)$. Indeed using the forward algorithm we have: . $$P(O| lambda) = sum_{i}^{N} alpha_{T}(i)$$ . whilst using the backward algorithm . $$P(O| lambda) = sum_{i}^{N} pi_i b_i(O_1) beta_{1}(i)$$ . The following video nicely explains the motivation behind the backward algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EbxLWGw2zJ4&#39;, width=800, height=300) . The following video nicely explains both the forward and backward algorithms. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;gYma8Gw38Os&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Forward–backward algorithm. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20backward-algorithm/dynamic-programming/algorithms/numerics/2020/05/23/backward-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20backward-algorithm/dynamic-programming/algorithms/numerics/2020/05/23/backward-algorithm.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Forward Algorithm",
            "content": "Given a hidden Markov model $ lambda$, meaning . A set of $N$ states $ mathbb{S}= {S_1, dots S_N }$ | A set of $M$ distinct observation symbols $ mathbf{V} = {v_1 dots v_M }$ | A transition probability matrix $ mathbf{A}$ | An observations probabilities matrix $ mathbf{B}$ | An initial state of probabilities $ boldsymbol{ pi}$ | . we want to be able to compute the marginal probability $P(O| lambda)$. This is the so called evaluation problem [1]; given the observation sequence $O$ calculate the probability that the sequence can occur under $ lambda$ . In theory, this can be calculated by using . $$P(O| lambda) = sum_{Q}P(O, Q| lambda)$$ . where . $$P(O, Q| lambda) = P(q_1) Pi_{t=2}^{T}P(q_t|q_{t-1}) Pi_{t=1}^{T}P(O_t|q_{t})$$ . Despite this, you should note that there are $N^T$ possible $Q$s assuming that all probabilities are nonzero [1]. Thus, the marginalization step above is rather, or can be, computationally expensive. We need another way to calculate $P(O| lambda)$. . In order to compute the probability $P(O| lambda)$ we need to know the joint probability $P(O, Q| lambda)$. The forward algorithm allows us to compute the latter without using marginalization. It does so by using recursion. We will divide the observation sequence into two parts; the first part will be $[1,t]$, the second is $[t+1, T]$ [1]. We further define the forward variable $ alpha_t(i)$. This will denote the probability of observing the partial sequence $ {O_1, cdots,O_t }$ unitl time $t$ and being in $S_i$ at time $t$ given $ lambda$: . $$ alpha_t(i) = P(O_1, cdots,O_t, q_t = S_i | lambda)$$ . This can be calculated recursively: . Initialize | . $$ alpha_t(i) = pi_ib_i(O_1)$$ . Recurse | . $$ alpha_{t+1}(j) = P(O_1, cdots,O_{t+1}, q_{t+1} = S_j | lambda)$$ . which can be written as . $$ alpha_{t+1}(j) = [ sum_{i}^{N} alpha_t(i)a_{i,j}]b_j(O_{t+1})$$ . Now $ alpha_t(i)$ explains the first $t$ observations and ends in state $S_i$. We multiply with the transition probability $a_{ij}$ in order to move to state $S_j$. since the are $N$ possible previous states we have to sum over all of them. Finally, we weight the result with $b_j(O_{t+1})$ which is the probability of observing $O_{t+1}$ at state $S_j$ at time $t+1$. . Once we know the forward variables, it is easy to calculate $P(O| lambda)$: . $$P(O| lambda) = sum_{i}^{N} alpha_{T}(i)$$ . $ alpha_{T}(i)$ is the probability of generating the full observation sequence and ending up in state $S_i$. We need to sum up over all the possible final states. The $ alpha_{t}(i)$ can be represented as a matrix of size $T times N$. Where $T$ is the size of the observation sequence and $N$ the number of states. Let&#39;s see an example. . Let&#39;s see a simple example. Assume a system with three states $S= {S_1, S_2, S_3 }$. Futher, assume that the observation sequence consists of elements from the following set $V= {a, b,c }$. Also let&#39;s assume the following HMM: . $$ boldsymbol{ pi}= begin{bmatrix}0.7 &amp; 0.15 &amp; 0.15 end{bmatrix}$$ . $$ mathbf{A}= begin{bmatrix}0.5 &amp; 0.25 &amp; 0.25 0.1 &amp; 0.8 &amp; 0.1 0.3 &amp; 0.15 &amp; 0.6 end{bmatrix}$$ . $$ mathbf{B}= begin{bmatrix} 0.16 &amp; 0.26 &amp; 0.58 0.25 &amp; 0.28 &amp; 0.47 0.2 &amp; 0.1 &amp; 0.7 end{bmatrix}$$ . We want to calculate the probability $P(O| lambda)$ where $O= {a, b, a, c, b, a }$. We will use the forward algorithm for this. We will first do the computation using pencil and paper and then write a small Python script for us. We create the matrix $ alpha$: . $$ alpha = begin{bmatrix}0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 end{bmatrix}$$ . The first step is the initialization of the matrix. We take the first observation in the sequence $O$ which &#39;a&#39;. This has to be mapped to an index. Assume that we have available such a mapping: . $$ {a:0, b:1, c:2 }$$ . Further assume that we use zero-based counting. We have . $$ alpha_{0,0} = pi_0 mathbf{B}_{0, 0} = 0.7 0.16 = 0.112$$ . $$ alpha_{0,1} = pi_1 mathbf{B}_{1, 0} = 0.15 0.25 = 0.0375 $$ . $$ alpha_{0,2} = pi_2 mathbf{B}_{2, 0} = 0.15 0.2 = 0.03$$ . we now proceed to the calculation of the probabilities of the next symbols. The symbol &#39;b&#39;, which is the next symbol in $O$ has index 1. Its probabilities are . $$ alpha_{1,0} = mathbf{B}_{0,1}( alpha_{0,0} mathbf{A}_{0,0} + alpha_{0,1} mathbf{A}_{1,0} + alpha_{0,2} mathbf{A}_{2,0}) $$ . Similarly for $ alpha_{1,1}$ and $ alpha_{1,2}$ . $$ alpha_{1,1} = mathbf{B}_{1,1}( alpha_{0,0} mathbf{A}_{0,1} + alpha_{0,1} mathbf{A}_{1,1} + alpha_{0,2} mathbf{A}_{2,1}) $$ . $$ alpha_{1,2} = mathbf{B}_{2,1}( alpha_{0,0} mathbf{A}_{0,2} + alpha_{0,1} mathbf{A}_{1,2} + alpha_{0,2} mathbf{A}_{2,2}) $$ . After filling the matrix $ alpha$ we can calculate the probability $P(O| lambda)$, This is given by the following sum: . $$P(O| lambda) = alpha_{5,0} + alpha_{5,1} + alpha_{5,2}$$ . Below is a simple Python script that performs these tedious calculations for us. . import numpy as np . obs_to_idx = {&#39;a&#39;:0, &#39;b&#39;: 1, &#39;c&#39;:2} # state to index map state_to_idx = {&#39;S1&#39;:0, &#39;S2&#39;:1, &#39;S3&#39;: 2} . pi = np.array([0.7, 0.15, 0.15]) # transition probabilities A = np.array([[0.5, 0.25, 0.25], [0.1, 0.8, 0.1], [0.3, 0.15, 0.6]]) # emission probabilties B = np.array([[0.16, 0.26, 0.58], [0.25, 0.28, 0.47], [0.2, 0.1, 0.7]]) . o = [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] . a = np.zeros(shape=(len(o),A.shape[0])) . # initialize alpha for i in range(len(state_to_idx)): # only first row the rest is zero a[0][i] = pi[i]*B[i][obs_to_idx[o[0]]] . print(&quot;Initial probabilities&quot;) print(a) . Initial probabilities [[0.112 0.0375 0.03 ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ]] . for t in range(1, len(o)): for j in range(A.shape[0]): a[t][j] = 0 # fix j = state_idx and sum over the states for i in range(A.shape[0]): a[t][j] += a[t -1][i] * A[i][j] a[t][j] *= B[j][obs_to_idx[o[i]]] print(&quot;alpha matrix: &quot;) print(a) . alpha matrix: [[1.12000000e-01 3.75000000e-02 3.00000000e-02] [1.10000000e-02 1.56250000e-02 9.95000000e-03] [1.60760000e-03 4.18562500e-03 2.05650000e-03] [2.94290000e-04 1.01471875e-03 4.10872500e-04] [5.95005800e-05 2.36744594e-04 8.43135750e-05] [1.25950115e-05 5.42294641e-05 1.78275499e-05]] . prob = 0; for i in range(A.shape[0]): prob += a[len(o)-1][i] print(&quot;Probability for sequence {0} is {1} &quot;.format(o, prob)) . Probability for sequence [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] is 8.465202543750001e-05 . Log probabilities . The forward variable, as well as the backward variable used in the backward algorithm are calculated as products of probabilities. When we have long sequences this may result in underflow [1]. In order to avoid this, we normalize $ alpha_t(i)$ by multiplying it with . $$c_t = frac{1}{ sum_{j} alpha_t(j)}$$ . After this normalization, $P(O| lambda)$ is given by . $$P(O| lambda) = frac{1}{ Pi_t c_t}$$ . or . $$logP(O| lambda) = - sum_t log c_t$$ . The motivation behind the forward algorithm is nicely presented in the following video . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EbxLWGw2zJ4&#39;, width=800, height=300) . The following video nicely explains both the forward and backward algorithms. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;gYma8Gw38Os&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Forward–backward algorithm. | Lawrence R. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20forward-algorithm/dynamic-programming/algorithms/numerics/2020/05/22/forward-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20forward-algorithm/dynamic-programming/algorithms/numerics/2020/05/22/forward-algorithm.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Matrix Approximation",
            "content": "Very often in numerical modeling, the data we need to work with is rather large and therefore not really handy. Moreover, the behavior under investigation can be explained by small number of features. Thus, it is desireable to be able to approximate the matrices. One way to do so is using the singular value decomposition method. . In fact SVD provides an optimal low-rank approaximation to a matrix $A$ (this is the Eckart-Young theorem). We can obtain a hierarchy of low rank matrices by just keeping the leading $k$ singular values and the corresponding eigenvectors. . Image copression is a simple example illustrating matrix approximation using SVD. We can view a grayscale image as matrix $A in mathbb{R}^{n times m}$ where $n, m$ are the number of pixels in the vertical and horizonal directions. . The Python code below computes the full SVD of the matrix representing the loaded image. We then compute approximations of the image using a range of retained singular values. We can see that as the number of retained singular values increases the quality of the image increases. . import numpy as np from matplotlib import image import matplotlib.pyplot as plt from numpy.linalg import matrix_rank . def rgb2gray(rgb): return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]) . A = image.imread(&#39;my_icons/volvo_img.png&#39;) A = np.mean(A, -1)#rgb2gray(rgb=data) # summarize shape of the pixel array print(A.shape) # display the array of pixels as an image plt.imshow(A) plt.show() . (366, 424) . print(&quot;Matrix rank &quot;, matrix_rank(A)) . Matrix rank 307 . U, S, V = np.linalg.svd(A, full_matrices=False) S = np.diag(S) . print(&quot;Shape U &quot;, U.shape) print(&quot;Shape S &quot;, S.shape) print(&quot;Shape V &quot;, V.shape) . Shape U (366, 366) Shape S (366, 366) Shape V (366, 424) . for r in [5, 20, 100]: print(&quot;Working with r&quot;, r) # construct approximate image img_approx = U[:,:r] @ S[0:r,:r] @ V[:r,:] plt.imshow(img_approx) plt.show() . Working with r 5 . Working with r 20 . Working with r 100 . plt.semilogy(np.diag(S)) plt.show() . plt.plot(np.cumsum(np.diag(S))/np.sum(np.diag(S))) plt.show() .",
            "url": "https://pockerman.github.io/qubit_opus/linear-algebra/singular-value-decomposition/matrix-approximation/algorithms/numerics/2020/03/14/matrix-approximation.html",
            "relUrl": "/linear-algebra/singular-value-decomposition/matrix-approximation/algorithms/numerics/2020/03/14/matrix-approximation.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pockerman.github.io/qubit_opus/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://pockerman.github.io/qubit_opus/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Books",
          "content": "Computing . Len Bass, Paul Clements, Rick Kazman Software Architecture In Practice | . History . Herodotus The Histories | . Mathematics . Kevin P. Murphy Machine Learning. A Probabilistic Perspective | . Physics . Leonard Susskind and Art Friedman, Quantum Mechanics. The Theoretical Minimum | . Literature . Antoine De Saint-Exupery Ο Μικρος Πριγγιπας | Albert Camus The Fall | Albert Camus The Outsider | George Orwell 1984 | Douglas R. Hostadter Godel, Escher, Bach: An Enternal Golden Braid | Fyodor Dostoyevsky Crime And Punishment | Fyodor Dostoyevsky The Idiot | Ernest Hemingway For Whom The Bell Tolls | Ernest Hemingway The Old Man And The Sea | .",
          "url": "https://pockerman.github.io/qubit_opus/books/",
          "relUrl": "/books/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pockerman.github.io/qubit_opus/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}