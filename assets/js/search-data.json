{
  
    
        "post0": {
            "title": "Scala Enumerations",
            "content": "Overview . Enumerations can be very useful when we want to create discrete set of items. Often these items help us to differentiate a run time instances having the same base class. Scala provides an Enumeration helper class that we can use to create enumerations[1]. . Enumerations . Scala does not have enumerated types [1]. In contrast, it provides the Enumeration helper class to help us create enumerations. This is shown in the code snippet below . object Element extends Enumeration{ val QUAD, TRI, HEX, TET = Value } . defined object Element . Above we defined an enumerated type with four fields. The above initialization is equivalent to [1] . ... val QUAD = Value val TRI = Value val HEX = Value val TET = Value ... . println(Element.QUAD) println(Element.TRI) . QUAD TRI . Each call to the Value method returns a new instance of an inner class, also called Value [1]. We can also initialize the enumeration fields with ids, names or both as shown below . object Element_2 extends Enumeration{ val QUAD = Value(0, &quot;QUAD&quot;) val TRI = Value(1, &quot;TRI3&quot;) } . defined object Element_2 . println(Element_2.QUAD) println(Element_2.TRI) . QUAD TRI3 . If not specified, the id is one more than the previously assigned one, starting with zero and the default name is the field name [1]. . Note that the type of the enumeration is Element.Value and not just Element. The latter is just the type of the object holding the values. We can use aliases to disambiguate this [1] . object Element_3 extends Enumeration{ type Element_3 = Value val QUAD = Value(0, &quot;QUAD&quot;) val TRI = Value(1, &quot;TRI3&quot;) } . defined object Element_3 . Now the type of the enumeration is Element_3.Element_3 [1]. . for( e &lt;- Element_3.values) println(e.id + &quot;:&quot; + e) . 0:QUAD 1:TRI3 . Finally, you can look up an enumeration value by its id or name [1]. Both of the following yield the object Element.HEX : . println(Element(2)) println(Element.withName(&quot;HEX&quot;)) . HEX HEX . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/enumerations/programming/2021/05/04/scala-enumerations.html",
            "relUrl": "/scala/enumerations/programming/2021/05/04/scala-enumerations.html",
            "date": " • May 4, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Testing for Controllability",
            "content": "Overview . We have introduced controllability and observability for linear time-invariant systems. We saw the conditions for such a system is controllalble and observable. Now we turn to the question how can we test is controllable and observable. . Testing for controllability . Recall that we deal with linear systems of the form . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . We can understand whether the linear system above is controllable or not by examing the controllability matrix $ mathbf{ cal{C}}$. In particular, the column space of that matrix. This matrix is defined as follows . $$ mathbf{ cal{C}} = begin{bmatrix} mathbf{B} &amp;&amp; mathbf{AB} &amp;&amp; mathbf{A}^2 mathbf{B} &amp;&amp; dots &amp;&amp; mathbf{A}^{n-1} mathbf{B} end{bmatrix}$$ . Where $n$ is the number of state variables. If the controllability matrix has $n$ linearly independent columns, then the system under consideration is controllable [1]. Note that this does not mean that the columns of $ mathbf{ cal{C}}$ should be linearly independent. All that we require, is that we can find $n$ linearly independent columns (see example 2 below). Let&#39;s see two examples taken from [1]. . The Popov-Belevich-Hautus or PBH is one of the most useful tests to determine whether or not a system is contollable [1]. The test says that the pair $( mathbf{A}, mathbf{B})$ is controllable if and only if the column rank of the matrix . $$ begin{bmatrix} mathbf{A} - lambda mathbf{I} &amp;&amp; mathbf{B} end{bmatrix}$$ . is equal to $n$ $ forall lambda in mathbb{C}$. Indeed the rank of $ mathbf{A} - lambda mathbf{I}$ is $n$ only when $ lambda$ is an eigenvalue of $ mathbf{A}$ [1]. Given that the $ mathbf{A} - lambda mathbf{I}$ is only rank deficient for the eigenvalues $ lambda$, then the kernel or null-space of $ mathbf{A} - lambda mathbf{I}$ is given by the span of the eigenvectors corresponding to $ lambda$ [1]. Therefore, for the matrix . $$ begin{bmatrix} mathbf{A} - lambda mathbf{I} &amp;&amp; mathbf{B} end{bmatrix}$$ . to have rank $n$, the columns in $ mathbf{B}$ must have some component in each of the eigenvector directions of $ mathbf{A}$ so that to complment the null space $ mathbf{A} - lambda mathbf{I}$ [1] . If $ mathbf{A}$ has $n$ distinct eigenvalues, then the system will be controllable with a single actuation input. In this call the matrix $ mathbf{A} - lambda mathbf{I}$ has at most one eigenvector direction in the null-space [1]. We can choose $ mathbf{B}$ as the sum of all $n$ linearly independent eigenvectors. This will guarantee to have some component in each direction. . Obviously, cases exist where we have degenerate eigenvalues with multiplicity greater than 2. The actuation input i.e. matrix $ mathbf{B}$ must then have multiple columns [1]. One more case where more it may be helpful to have multiple actuators is when we need better control of the system or when dealing with systes with large transient growth [1]. . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/robotics/autonomous-vehicles/controllability/dynamical-systems/controllability-test/2021/05/02/testing-for-controllability.html",
            "relUrl": "/robotics/autonomous-vehicles/controllability/dynamical-systems/controllability-test/2021/05/02/testing-for-controllability.html",
            "date": " • May 2, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "The Scala apply Method",
            "content": "Overview . The apply method is called in expression of the form Object(arg1,...,argN) [1]. . The apply method . We have seen that we can write both experssions . val arr1 = new Array[Int](10) . arr1: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) . val arr2 = Array(10) . arr2: Array[Int] = Array(10) . The first expression creates an Array of length 10. In this expression the constructor is called. In contrast, in the second expression, the apply method is called. And an Array instance is created having length one and value . println(arr2(0)) . 10 . Not having the new keyword is handy for nested expressions, like the one below . val arr3 = Array(Array(1, 7), Array(2, 9)) . arr3: Array[Array[Int]] = Array(Array(1, 7), Array(2, 9)) . In order to be able to use expressions like the above, the apply method must be defined [1]. We can do this as shown below [1] . class MyCls (val idx: Int, val value: Double){ } object MyCls{ def apply(idx: Int, value: Double) = new MyCls(idx, value) } . defined class MyCls defined object MyCls . val cls = MyCls(1, 20) . cls: MyCls = ammonite.$sess.cmd5$Helper$MyCls@5e0ad6c6 . println(cls.idx) println(cls.value) . 1 20.0 . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/apply-method/programming/2021/04/30/scala-apply-method.html",
            "relUrl": "/scala/classes/apply-method/programming/2021/04/30/scala-apply-method.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "More On Scala object",
            "content": "Overview . We have seen how we can create singletons and companion objects using object. In this notebook we introduce more things we can do with object. . More on Scala object . An object can extend one class. However, it can extend one or more traits [1]. This results in an object that has all of the features specified in the object definition [1]. One utilization of this pattern is to specify default objects as shown below. . abstract class Element(val idx: Int){ def nFaces: Int; def nVertices: Int; } . defined class Element . object DummyElement extends Element(-1){ override def nFaces: Int = -1 override def nVertices: Int = -1 } . defined object DummyElement . Whenever we want to use an Element that makes no sense but anyway it is needed we can use DummyElement. . Application objects . Just like Java and C++, a Scala application starts with a main method which has the following signature [1] . def main(args: Array[String]): Unit . We can wrap that in a companion object . class Hello{ def showMsg() = println(&quot;Hello...&quot;) } object Hello{ def main(args: Array[String]){ val msg = new Hello msg.showMsg() } } . defined class Hello defined object Hello . Note that we can also extend the App trait and place the program code into the constructor body [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/companion-object/programming/2021/04/29/scala-object.html",
            "relUrl": "/scala/classes/companion-object/programming/2021/04/29/scala-object.html",
            "date": " • Apr 29, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Scala Companion Objects",
            "content": "Overview . Scala does not support static functions. We saw that object classes can be used to implement patterns like the singleton pattern. Moreover, often it make more sense that a function is a class function. We can do this using companion objects. . Companion objects . A companion object has the same name as the class it refers to. It is implemented using the object keyword . object MyClass{ private var currentIndex = 0 def getNewIndex : Int = {currentIndex +=1; currentIndex} } . defined object MyClass . class MyClass{ val id = MyClass.getNewIndex def getIdx: Int = id } . defined class MyClass . val cls = new MyClass println(cls.getIdx) . 1 . cls: MyClass = ammonite.$sess.cmd5$Helper$MyClass@38c61f45 . Both the class and its companion object can access each other’s private features. Furthermore, they must be located in the same source file [1]. . Note that the companion object of a class is accessible, but it is not in scope [1]. This means that in the example above we need to use MyClass.getNewIndex and not just getNewIndex to invoke the method of the companion object [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/companion-object/programming/2021/04/27/scala-companion-objects.html",
            "relUrl": "/scala/classes/companion-object/programming/2021/04/27/scala-companion-objects.html",
            "date": " • Apr 27, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Scala Singletons",
            "content": "Overview . Often in software modeling we need to represent an entity that it does not make sense to have more than one instances throughout program execution. We call these objects singletons. Typically, singletons are modeled using static functions. Scala does not support static functions. Instead we use the object construct [1]. . Singletons . An object defines a single instance of a class with the features we want. For example . object Counter{ private var theCounter = 0 def getNewCounter : Int = {theCounter +=1; theCounter} } . defined object Counter . When the application requires a new counter, simply calls Counter.getNewCounter . println(&quot;New counter &quot; + Counter.getNewCounter) . New counter 1 . The constructor of an object is executed when the object is first used [1]. If an object is never used, its constructor is, obviously, not executed [1]. . An object can have all the features of a class, including extending other classes or traits [1]. However, an object cannot have a constructor with parameters. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/singletons/programming/2021/04/26/scala-singletons.html",
            "relUrl": "/scala/classes/singletons/programming/2021/04/26/scala-singletons.html",
            "date": " • Apr 26, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Observability",
            "content": "Overview . Using full state feedback i.e. $ mathbf{u} = - mathbf{K} mathbf{x}$, we can modify the behavior of a controllable system. However, it is not always possible to have full-state measurements of the state vector $ mathbf{x}$. In this case, we have to estimat it. This is only possible when the system observable [1]. In this post we will have a brief view of observability. . Observability . Recall that we deal with linear systems of the form . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . In this case, observability is similar to controlability [1]. Briefly, a system is observable if it is possible to estimate any state $ boldsymbol{ xi} in mathbb{R}^n$ from a history of measurements $ mathbf{y}(t)$ [1]. The observability matrix $ mathbf{ cal{O}}$ allows us to determin entirely whether a system is observable or not [1]. It is defined as . $$ mathcal{ cal{O}} = begin{bmatrix} mathbf{C} mathbf{CA} mathbf{C} mathbf{A}^2 vdots mathbf{C} mathbf{A}^{n-1} end{bmatrix}$$ . Where $n$ is the number of state variables. Specifically, if the rows of the matrix span $ mathbb{R}^n$ then it is possible to estimate any full-dimensional state vector $ mathbf{x} in mathbb{R}^n$ from the time-history of $ mathbf{y}(t)$ [1]. . If a system is observable, then it is possible to design the eignevalues of the estimated dynamics to have properties such as noise attenuation and fast estimation [1]. Finally, note that observability matrix is the transpose of the controllability matrix $ mathbf{ cal{C}}$. . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/dynamical-systems/linear-systems/control/observability/2021/04/25/observability.html",
            "relUrl": "/dynamical-systems/linear-systems/control/observability/2021/04/25/observability.html",
            "date": " • Apr 25, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Value Iteration With Scala",
            "content": "Overview . When policy evaluation is stopped after just one update of each state, the algorithm is called value iteration. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps [1]. This post looks at the value iteration algorithm. . Value iteration . One drawback to policy iteration is that each of its iterations involves a policy evaluation. This however may itself be an iterative computation; thus requiring multiple sweeps through the state set [1]. Furthermore, if the evaluation is done iteratively, then convergence to $V_{ pi}$ occurs only in the limit [1]. . Given the above limitations of policy iterations, the question posed is whether we could we stop earlier? [1]. Luckily, the policy evaluation step of policy iteration can truncated without loosing the convergence gurantees of the method. Moreover, this can be done in several ways [1]. . In particular, when policy evaluation is stopped after just one update of each state, the algorithm is called value iteration. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps [1] . $$V_{k+1}(s) = max_{ alpha} sum_{s^*, r}p(s^*, r | s, alpha) left[r + gamma V_{k}(s^*) right], ~~ forall s in mathbb{S}$$ . Figure 1: Value iteration algorithm. Image from [1]. . The value iteration is obtained simply by turning the Bellman optimality equation into an update rule [1]. It requires the maximum to be taken over all the actions. Furthermore, the algorithm terminates by checking the amount of change of the value function. . Code . import scala.collection.mutable.ArrayBuffer import scala.util.control.Breaks._ import scala.math.max . import scala.collection.mutable.ArrayBuffer import scala.util.control.Breaks._ import scala.math.max . object Grid{ class State(val idx: Int){ val neigbors = new ArrayBuffer[Int]() for(i &lt;- 0 until 4){ neigbors += -1 } def addNeighbors(neighbors: Array[Int]): Unit = { require(neighbors.length == 4) for(n &lt;- 0 until neighbors.length){ addNeighbor(n, neighbors(n)) } } def addNeighbor(idx: Int, nIdx: Int): Unit = { require(idx &lt; 4) neigbors(idx) = nIdx } def getNeighbor(idx: Int): Int = { require(idx &lt; 4) return neigbors(idx) } } } . defined object Grid . class Grid{ val states = new ArrayBuffer[Grid.State]() def nStates : Int = states.length def nActions: Int = 4 def envDynamics(state: Grid.State, action: Int): (Double, Int, Double, Boolean) = { (0.25, states(state.idx).getNeighbor(action), 1.0, false) } def getState(idx: Int): Grid.State = { require(idx &lt; nStates) states(idx) } def create(): Unit = { // add a new state for(s &lt;- 0 until 9){ states += new Grid.State(s) if(s == 0){ states(s).addNeighbors(Array(0, 1, 3, 0)) } else if(s == 1){ states(s).addNeighbors(Array(1, 2, 4, 0)) } else if(s == 2){ states(s).addNeighbors(Array(2, 2, 5, 1)) } else if(s == 3){ states(s).addNeighbors(Array(0, 4, 6, 3)) } else if(s == 4){ states(s).addNeighbors(Array(1, 5, 7, 3)) } else if(s == 5){ states(s).addNeighbors(Array(2, 5, 8, 4)) } else if(s == 6){ states(s).addNeighbors(Array(3, 7, 6, 6)) } else if(s == 7){ states(s).addNeighbors(Array(4, 8, 7, 6)) } else if(s == 8){ states(s).addNeighbors(Array(5, 8, 8, 7)) } } } } . defined class Grid . class ValueIteration(val numIterations: Int, val tolerance: Double, val gamma: Double){ val valueF = new ArrayBuffer[Double]() var residual = 1.0 def train(grid: Grid): Unit = { valueF.clear() for(i &lt;- 0 until grid.nStates){ valueF += 0.0 } breakable { for(itr &lt;- Range(0, numIterations)){ println(&quot;&gt; Learning iteration &quot; + itr) println(&quot;&gt; Learning residual &quot; + residual) step(grid) if(residual &lt; tolerance) break; } } } def step(grid: Grid): Unit = { var delta: Double = 0.0 for(sIdx &lt;- 0 until grid.nStates){ // Do a one-step lookahead to find the best action val lookAheadVals = this.one_step_lookahead(grid, grid.getState(sIdx)) val maxActionValue = lookAheadVals.max delta = max(delta, (maxActionValue - valueF(sIdx).abs)) // # Update the value function. Ref: Sutton book eq. 4.10. valueF(sIdx) = maxActionValue } this.residual = delta } // Helper function to calculate the value for // all action in a given state. // Returns a vector of length grid.nActions containing // the expected value of each action. def one_step_lookahead(grid: Grid, state: Grid.State): ArrayBuffer[Double] = { val values = new ArrayBuffer[Double](grid.nActions) for(i &lt;- 0 until grid.nActions){ values += 0.0 } for(i &lt;- 0 until grid.nActions){ val (prob, next_state, reward, done) = grid.envDynamics(state, i) val oldVal = values(i) values(i) = oldVal + prob * (reward + this.gamma * valueF(next_state)) } values } } . defined class ValueIteration . val grid = new Grid grid.create() . grid: Grid = ammonite.$sess.cmd2$Helper$Grid@794b701a . val valueFunction = new ValueIteration(100, 1.0e-4, 1.0) . valueFunction: ValueIteration = ammonite.$sess.cmd29$Helper$ValueIteration@44c65bd3 . valueFunction.train(grid) . &gt; Learning iteration 0 &gt; Learning residual 1.0 &gt; Learning iteration 1 &gt; Learning residual 0.3330078125 &gt; Learning iteration 2 &gt; Learning residual 0.078125 &gt; Learning iteration 3 &gt; Learning residual 0.0048828125 &gt; Learning iteration 4 &gt; Learning residual 3.0517578125E-4 . References . Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. |",
            "url": "https://pockerman.github.io/qubit_opus/scala/reinforcement-learning/algorithms/dynamic-programming/2021/04/23/scala-value-iteration.html",
            "relUrl": "/scala/reinforcement-learning/algorithms/dynamic-programming/2021/04/23/scala-value-iteration.html",
            "date": " • Apr 23, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Scala Classes 3",
            "content": "Overview . In this third part regarding Scala classes we review nested classes. . Scala Classes 3 . In Scala, you can nest just about anything inside anything [1]. For example we can define functions inside other functions, and classes inside other classes. We discuss the latter feature here. . import scala.collection.mutable.ArrayBuffer . import scala.collection.mutable.ArrayBuffer . class Grid{ class Element(val idx: Int){ val children = new ArrayBuffer[Element] } private val elements = new ArrayBuffer[Element] def addElement() : Unit = { val element = new Element(this.elements.size) elements += element } def addElement(element: Element){ elements += element } def nElements(): Int = this.elements.size } . defined class Grid . val grid = new Grid . grid: Grid = ammonite.$sess.cmd10$Helper$Grid@662422e8 . grid.addElement() . println(&quot; Number of elements &quot; + grid.nElements) . Number of elements 1 . In Scala, each instance has its own class Element , just like each instance has its own field members [1]. You can see this below . val grid_2 = new Grid . grid_2: Grid = ammonite.$sess.cmd10$Helper$Grid@6116d1b9 . grid.addElement(new grid_2.Element(0)) . cmd15.sc:1: type mismatch; found : cmd15.this.cmd14.grid_2.Element required: cmd15.this.cmd11.grid.Element val res15 = grid.addElement(new grid_2.Element(0)) ^Compilation Failed . Compilation Failed . Assuming that grid holds quad elemennts and grid_2 holds triangular elements this behaviour makes sense. However, we may want to remove this behaviour. We can do this in two ways [1] . Use a companion object | Use type projection | . object Grid{ class Element(val idx: Int){ val children = new ArrayBuffer[Element] } } . defined object Grid . class Grid{ private val elements = new ArrayBuffer[Grid.Element] def addElement() : Unit = { val element = new Grid.Element(this.elements.size) elements += element } def addElement(element: Grid.Element){ elements += element } def nElements(): Int = this.elements.size } . defined class Grid . With type projection, we write our class as shown below . class Grid{ class Element(val idx: Int){ val children = new ArrayBuffer[Element] } private val elements = new ArrayBuffer[Element] def addElement() : Unit = { val element = new Element(this.elements.size) elements += element } def addElement(element: Element){ elements += element } def nElements(): Int = this.elements.size } . defined class Grid . Type projection means, for our case, Element from any Grid . val grid = new Grid val grid2 = new Grid . grid: Grid = ammonite.$sess.cmd19$Helper$Grid@3bc5f2f5 grid2: Grid = ammonite.$sess.cmd19$Helper$Grid@5f5eafec . grid.addElement(new grid_2.Element(0)) . cmd21.sc:1: type mismatch; found : cmd21.this.cmd14.grid_2.Element required: cmd21.this.cmd20.grid.Element val res21 = grid.addElement(new grid_2.Element(0)) ^Compilation Failed . Compilation Failed . Finally, in a nested class, we can access the this reference of the enclosing class as EnclosingClass.this [1]. This is similar to Java. Furthermore, we can establish an alias for that reference as shown below . class Grid{ outer =&gt; class Element(val idx: Int){ val children = new ArrayBuffer[Element] } private val elements = new ArrayBuffer[Element] def addElement() : Unit = { val element = new Element(this.elements.size) elements += element } def addElement(element: Element){ elements += element } def nElements(): Int = this.elements.size } . defined class Grid . The class Grid{ outer =&gt; syntax makes the variable outer refer to Grid.this. Note that we can choose any name for this variable. The name self is common, but perhaps confusing when used with nested classes [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/programming/2021/04/21/scala-classes-3.html",
            "relUrl": "/scala/classes/programming/2021/04/21/scala-classes-3.html",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Scala Classes 2",
            "content": "Overview . This post looks into primary and auxiliary constructors for Scala classes. . Scala Classes 2 . A Scala class can have as many constructors as we want. A Scala class has a so called primary constructor. Additionally, a class can have any number of auxiliary constructors. . Primary constructor . A Scala class has a so called primary constructor. The primary constructor is not defined with a this method. This is interwoven with the class definition [1]. . class Person(val name: String, val age: Int){ } . defined class Person . Parameters of the primary constructor turn into fields that are initialized with the construction parameters. . val p = new Person(&quot;Alex&quot;, 10) . p: Person = ammonite.$sess.cmd10$Helper$Person@62b0e31f . println(p.name + &quot; has age &quot; + p.age) . Alex has age 10 . The primary constructor executes all statements in the class definition. This is shown below . class AnotherClass(val val1: Double, val val2: Int){ show() def show()={ println(val1 + &quot;, &quot; + val2) } } . defined class AnotherClass . val cls = new AnotherClass(20.0, 10) . 20.0, 10 . cls: AnotherClass = ammonite.$sess.cmd13$Helper$AnotherClass@1ea96977 . The show() function call is a part of the primary constructor. It will be called every time a new object is created [1]. Moreover, if there are no parameters after the class name, then the class has a primary constructor with no parameters. That constructor simply executes all statements in the body of the class [1]. . The primary constructor of the AnotherClass declares and initializes the following fields . val val1: Double val val2: Int . Since the primary constructor parameters are declared with the private keyword, the getter function is public. Moreover only a getter is generated as both variables are declared with val. . Construction parameters can also be regular method parameters, without val or var . How these parameters are processed depends on their usage inside the class[1]. . If a parameter without val or var is used inside at least one method, it becomes a field. . class AnotherClass_2(val1: Double, val2: Int){ def description = val1 + &quot; , &quot; + val2 } . defined class AnotherClass_2 . The primary constructor above, declares and initializes immutable fields val1 and val2 that are object-private i.e. instances of the same class do not have access to these fields of another instance from the same class. . Otherwise, the parameter is not saved as a field. Meaning it is just a regular parameter that can be accessed in the code of the primary constructor [1]. . Finally, sometimes we may want to declare a primary constructor as private. We can do so as shown below . class AnotherClass_3 private(val1: Double, val2: Int){ def description = val1 + &quot; , &quot; + val2 } . defined class AnotherClass_3 . A class user must then use an auxiliary constructor to construct a AnotherClass_3 object [1]. . Auxiliary constructors . Auxiliary constructs are called this [1]. Each such constructor should start with a call to a previously defined auxiliary constructor or the primary constructor. . class MyCls{ private var name = &quot;&quot; private var age = 0 def this(name: String){ // first call primary constructor this() this.name = name } def this(name: String, age: Int){ this(name) //set also the age this.age = age } } . defined class MyCls . The first auxiliary constructor, i.e. this(name: String), calls the empty primary construtor this(). For a class we do not define a primary constructor has a primary constructor with no arguments [1]. . val cls1 = new MyCls val cls2 = new MyCls(&quot;Alex&quot;) val cls3 = new MyCls(&quot;Alex&quot;, 10) . cls1: MyCls = ammonite.$sess.cmd17$Helper$MyCls@78f51c1b cls2: MyCls = ammonite.$sess.cmd17$Helper$MyCls@4cf2a04c cls3: MyCls = ammonite.$sess.cmd17$Helper$MyCls@5ece3c6c . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/programming/2021/04/20/scala-classes-2.html",
            "relUrl": "/scala/classes/programming/2021/04/20/scala-classes-2.html",
            "date": " • Apr 20, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Scala Classes 1",
            "content": "Overview . The next stop in the Scala journey is classes. Classes are the cornerstone of object oriented programming. In the simplest form, a Scala looks similar to classes in Java and C++. . Scala Classes 1 . Some points regarding Scala classes are . A Scala source file can contain multiple classes | A Scala class is not declared public | . Let&#39;s start with defining a simple class . class MyClass { private var value = 0.0 def increment() = value += 1.0 def getValue = value } . defined class MyClass . We can use the class as follows . val inst1 = new MyClass . inst1: MyClass = ammonite.$sess.cmd5$Helper$MyClass@5743442d . println(inst1.getValue) . 0.0 . inst1.increment() . println(inst1.getValue) . 1.0 . Getters and setters are frequently used to change properties of a class. Alghtough, such methods allow for every client of the class to modify the state of the class, they are the preferred way of doing so. This is because they allow us to control how the change of the class state is done. Scala can generate getters and setters for us for every private field of our class . class MyClass2{ var value = 0.0 } . defined class MyClass2 . val inst2 = new MyClass2 . inst2: MyClass2 = ammonite.$sess.cmd10$Helper$MyClass2@52cac8e8 . print(inst2.value) . 0.0 . inst2.value = 10.0 . print(inst2.value) . 10.0 . In Scala, the getters and setters are not named getXxx and setXxx, but, as you can see above, they are names after the name of the private variable, in our case is value. . However, we can redefine the getter and setter methods generated for us as shown below . class MyClass3{ var myValue = 0.0 def value = myValue def value_=(newValue: Double) : Unit = { // we can control how the state is // changed here myValue = newValue } } . defined class MyClass3 . val inst3 = new MyClass3 . inst3: MyClass3 = ammonite.$sess.cmd27$Helper$MyClass3@51b4dc09 . println(inst3.value) . 0.0 . inst3.value = 3.0 . println(inst3.value) . 3.0 . Regarding setters and getters a few points to remember follow [1] . If the field is private then both functions are private | If the field is declared using val, then only a getter is generated | If no getter or setter is needed for the field, declare this as private[this] | We cannot have a write-only field i.e a field with a setter and no getter | . private[this] . Every instance of a class has access to the private members of instances of the same class. For example, the following is legal . class ShowMeMsg{ private var msg = &quot;Nothing&quot; def setMsg(newMesg: String) : Unit = msg=newMesg def isMessage(other: ShowMeMsg) = { if(other.msg == this.msg){ println(&quot;It is message&quot;) } } } . defined class ShowMeMsg . var msg1 = new ShowMeMsg msg1.setMsg(&quot;ONE&quot;) . msg1: ShowMeMsg = ammonite.$sess.cmd37$Helper$ShowMeMsg@4e22d967 . var msg2 = new ShowMeMsg msg2.setMsg(&quot;ONE&quot;) . msg2: ShowMeMsg = ammonite.$sess.cmd37$Helper$ShowMeMsg@7a1f4e7 . msg1.isMessage(msg2) . It is message . Scala allows an even more severe access restriction, with the private[this] qualifier [1]: . class ShowMeMsg_2{ private[this] var msg = &quot;Nothing&quot; def setMsg(newMesg: String) : Unit = msg=newMesg def isMessage(other: ShowMeMsg) = { if(other.msg == this.msg){ println(&quot;It is message&quot;) } } } . cmd41.sc:7: variable msg in class ShowMeMsg cannot be accessed as a member of cmd41.this.cmd37.ShowMeMsg from class ShowMeMsg_2 in class Helper if(other.msg == this.msg){ ^Compilation Failed . Compilation Failed . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/programming/2021/04/18/scala-classes-1.html",
            "relUrl": "/scala/classes/programming/2021/04/18/scala-classes-1.html",
            "date": " • Apr 18, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Linear Methods",
            "content": "Linear methods . In this article, we will continue our function approximation journey by introducing linear models for representing the state value function $V_{ pi}(S_t)$. Recall that in the case of large state spaces it is advantageous to use some sort of parametic funtion approximation rather than a tabular representation of the state value function. Furthermore, we need a model in order to to perform, for examle, the SGD update step given below . $$ mathbf{w}_{t+1} = mathbf{w}_t + eta left[V_{ pi}(S_t)- hat{V}_{ pi}(S_t, mathbf{w}_t) right ] nabla hat{V}(S_t, mathbf{w}_t)$$ . One of the simplest representations we can have, and the one we take in this post, is a linear model with resepct to the weights $ mathbf{w}$. In this case, $ hat{V}(s, mathbf{w})$ becomes . $$ hat{V}(s, mathbf{w}) = sum_{i=1}^{d} w_i x_i(s) = mathbf{w}^T mathbf{x}(s)$$ . The expression above implies that there is a vector $ mathbf{x}(s)$ for every state having the same number of components as the weights vector. The vector $ mathbf{x}(s)$ represents the features of the state $s$. For example for an autonomous vehicle, $ mathbf{x}(s)$ may correspond to the vector with components such as vehicle velocity, vehicle acceleration, vehicle position, vehicle orientatio, gas level e.t.c. Technically, each component $x_i$ represents a function such that $x_i: mathbb{S} rightarrow mathbb{R}$ [1]. For linear methods, features are basis functions because they form a linear basis for the set of approximate functions [1]. . Linear models have a straightforward gradient calculation. Indeed in this case . $$ nabla hat{V}(S_t, mathbf{w}_t) = mathbf{x}(s)$$ . Therefore, the SGD update step becomes . $$ mathbf{w}_{t+1}= mathbf{w}_t + eta left[V_{ pi}(S_t)- hat{V}_{ pi}(S_t, mathbf{w}_t) right ] mathbf{x}(S_t)$$ . Linear models are, in general, very well understood throughout science. For our case, when using a linear model case there is only one optimum. Therefore, any method that is guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum [1]. . As a final note, observe that we need $V_{ pi}(S_t)$ in order to perform SGD weights update. This many not always be available. We may, however, have in hand an approximation of it, let&#39;s calle it $U_t$ i.e. $V_{ pi}(S_t) approx U_t$. In this scenario, we are forced to use the latter. However, if $U_t$ is an unbiasd estimate, then $ mathbf{w}_t$ is guaranteed to converge to a local optimum under the conditions specified above for decreasing $ eta$. . References . Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. |",
            "url": "https://pockerman.github.io/qubit_opus/linear-model/reinforcement-learning/2021/04/16/rl-linear-methods.html",
            "relUrl": "/linear-model/reinforcement-learning/2021/04/16/rl-linear-methods.html",
            "date": " • Apr 16, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Scala Maps",
            "content": "Overview . Maps are collections of key-value pais. Just like arrays, in Scala we can distinguish between mutable and immutable maps. Furthermore, the default map type is a hash map. However, tree maps are also provided [1]. . As mentioned above, in Scala, a map is a collection of key-value pairs. A pair is a grouping of two values that do not have necessarily the same type [1]. We have two ways cosntructing a pair . Using the -&gt; operator. | Using (key, value) constructs | . Immutable map . We can construct an immutable Map as shown below . val map1 = Map(&quot;France&quot; -&gt; &quot;Paris&quot;, &quot;England&quot; -&gt; &quot;London&quot;, &quot;Greece&quot; -&gt; &quot;Athens&quot;) . map1: Map[String, String] = Map( &#34;France&#34; -&gt; &#34;Paris&#34;, &#34;England&#34; -&gt; &#34;London&#34;, &#34;Greece&#34; -&gt; &#34;Athens&#34; ) . The elements in Map cannot be changed . map1(&quot;Greece&quot;) = &quot;New York&quot; . cmd1.sc:1: value update is not a member of scala.collection.immutable.Map[String,String] did you mean updated? val res1 = map1(&#34;Greece&#34;) = &#34;New York&#34; ^Compilation Failed . Compilation Failed . Nevertheless, the following is a way to update an immmutable map . val map2 = map1 + (&quot;Greece&quot; -&gt; &quot;New York&quot;) . map2: Map[String, String] = Map( &#34;France&#34; -&gt; &#34;Paris&#34;, &#34;England&#34; -&gt; &#34;London&#34;, &#34;Greece&#34; -&gt; &#34;New York&#34; ) . Similarly, we can remove or add a new element . val map3 = map2 + (&quot;Italy&quot; -&gt; &quot;Rome&quot;) . map3: Map[String, String] = Map( &#34;France&#34; -&gt; &#34;Paris&#34;, &#34;England&#34; -&gt; &#34;London&#34;, &#34;Greece&#34; -&gt; &#34;New York&#34;, &#34;Italy&#34; -&gt; &#34;Rome&#34; ) . val map4 = map3 - &quot;Greece&quot; . map4: Map[String, String] = Map( &#34;France&#34; -&gt; &#34;Paris&#34;, &#34;England&#34; -&gt; &#34;London&#34;, &#34;Italy&#34; -&gt; &#34;Rome&#34; ) . Mutable map . In order to get a mutable map we need to explicitly say so . import scala.collection.mutable.Map . import scala.collection.mutable.Map . val grades = Map(&quot;Alex&quot; -&gt; 10, &quot;Alice&quot; -&gt; 15, &quot;George&quot; -&gt;5) . grades: Map[String, Int] = HashMap(&#34;Alex&#34; -&gt; 10, &#34;George&#34; -&gt; 5, &#34;Alice&#34; -&gt; 15) . grades(&quot;Alex&quot;) = 12 . grades . res7: Map[String, Int] = HashMap(&#34;Alex&#34; -&gt; 12, &#34;George&#34; -&gt; 5, &#34;Alice&#34; -&gt; 15) . Above we have initialized the map at construction time. However, we might not always be able to do so. In this case, we need to specify explicitly what type of map we want [1] . import scala.collection.mutable.HashMap . import scala.collection.mutable.HashMap . val gradesEmpty = new HashMap[String, Int] . gradesEmpty: HashMap[String, Int] = HashMap() . If the map is mutable, this means tha we can add, change or remove elements. We can add a new element in two ways . Use operator (key). If the key exists it will update the value corresponding to the key. Otherwise, it will create a new key-value pair | Use operator += followed by a tuple of pairs | . gradesEmpty += (&quot;Suzana&quot; -&gt; 15, &quot;John&quot; -&gt; 3) . res10: HashMap[String, Int] = HashMap(&#34;Suzana&#34; -&gt; 15, &#34;John&#34; -&gt; 3) . We can remove a key-value pait using the -= operator . gradesEmpty -= &quot;Suzana&quot; . res11: HashMap[String, Int] = HashMap(&#34;John&#34; -&gt; 3) . Querying a map . How can we find whether a key is contained in a map? . grades.contains(&quot;Alex&quot;) . res12: Boolean = true . grades.contains(&quot;&quot;) . res13: Boolean = false . One nice feature is that we can query a map with a key and specify a default value in case that the key does not exist . grades.getOrElse(&quot;Alex&quot;, &quot;Invalid Name&quot;) . res14: Any = 12 . grades.getOrElse(&quot;SomeOne&quot;, &quot;Invalid Name&quot;) . res15: Any = &#34;Invalid Name&#34; . As shown above, we can access the value of a particular key using the () operator. This, however, will an exception if the key does not exit. Finally, we can use grades.get( someKey ). This returns an Option object that is either Some( value for key ) or None [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/maps/programming/2021/04/15/scala-maps.html",
            "relUrl": "/scala/maps/programming/2021/04/15/scala-maps.html",
            "date": " • Apr 15, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Stochastic Gradient Descent",
            "content": "Stochastic Gradient Descent . Stochastic gradient descent (SGD) methods are among the most widely used of all function approximation methods. Moreover, they are particularly well suited to online reinforcement learning. The article An overview of gradient descent optimization algorithms gives a nice review of gradient descent methods. . Gradient descent methods assume that the function approximation ( in our case this is the approximate value function $ hat{V}(s, mathbf{w})$) is a differentiable function of $ mathbf{w}$. Furthermore, we assume that this is true for $s in mathbb{S}$ . Gradient-descent methods are iterative algorithms. Thus, we denote with $ mathbf{w}_t$ the weight vector at the $t-th$ iteration. Furtheremore, at each iteration, we observe $S_t rightarrow V_{ pi}(S_t)$ i.e. the example consists of a state $S_t$ and its true value under the policy. Note that we can choose the state randomly. These states might be successive states from an interaction with the environment [1]. . Hence, we have in hand $S_t$ and the corresponding state value i.e. $V_{ pi} (S_t)$. However, the problem we now face is that the number of weights is far less than the number of states i.e. our function approximator has a rather limited resolution. In particular, there is generally no $ mathbf{w}$ that gets all the states, or even all the examples, exactly correct. A second problem is that the function approximator must generalize to all the other states that have not appeared yet [1]. . We assume that states appear in examples with the same distribution, $ mu$, over which we are trying to minimize the $MSVE$ given by: . $$MSVE( mathbf{w}) = sum_{s in S} mu(s) left[V_{ pi}(s) - hat{V}_{ pi}(s, mathbf{w}) right]^2$$ . Stochastic gradient-descent (SGD) methods do this by adjusting the weight vector after each example is visited by a small amount in the direction that would most reduce the error on that example. Namely . $$ mathbf{w}_{t+1} = mathbf{w}_t - frac{1}{2} eta nabla left[V_{ pi}(S_t)- hat{V}_{ pi}(S_t, mathbf{w}_t) right ]^2 = mathbf{w}_t + eta left[V_{ pi}(S_t)- hat{V}_{ pi}(S_t, mathbf{w}_t) right ] nabla hat{V}(S_t, mathbf{w}_t)$$ . where $ eta$ is a positive step-size parameter also known as learning rate and . $$ nabla hat{V}(S_t, mathbf{w}) = left( frac{ partial hat{V}(S, mathbf{w})}{ partial w_1}, ..., frac{ partial hat{V}(S, mathbf{w})}{ partial w_m} right)$$ . SGD methods are gradient descent methods because. The latter methods update $ mathbf{w}_t$ by a small amount towards the direction that most reduces the error. The error metric, $MSVE$, that we use here, uses as an error indicator the following quantity . $$e^2 = left[V_{ pi}(s) - hat{V}_{ pi}(s, mathbf{w}) right]^2$$ . This is reduced most rapidly in the direction of $- nabla hat{V}(S_t, mathbf{w})$. . There are various gradient descent methods, see for example [2]. Gradient descent methods are called stochastic when the update is done on only a single example, which might have been selected stochastically. Over many examples, making small steps, the overall effect is to minimize an average performance measure such as the MSVE. [1] . SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily. This is shown in the image below. . Figure 1. SGD fluctuation. Image from [2]. . This overshooting behavior, in general, complicates convergence to the exact minimum. However, by slowly decreasing the learning rate $ eta$ it has been shown that SGD shows the same convergence behaviour as batch gradient descent and almost certainly converges to a local or the global minimum both for convex and non-convex optimization problems. One other problem one may face with the SGD algorithm is that some bias may be introduced as the algorithm updates the weights on a per weights basis. This can be mitigated by suffling the data after each iteration. . Given that SGD does frequent updates that may exhibit high variance why don&#39;t we move in the minimizing direction in one step and therefore completely eliminate the error on the visited example? Altghough many times this can be done, it may not be the right thing to do so. The reason why this is the case, is that the weights are far less than the states. Hence, we should not seek to find a value function that has zero error for all states. Instead, the approximation should balance the errors in the different states [1]. If we completely corrected each example in one step, then we would not find such a balance [1]. Note also that the convergence results for SGD methods assume that the learning rate, $ eta$, decreases over time. Moreover, if it decreases in such a way as to satisfy the following stochastic approximation conditions . $$ sum_{n = 1}^{ infty} eta_n( alpha) = infty ~~ text{and} ~~ sum_{n = 1}^{ infty} eta_{n}^2( alpha) &lt; infty $$ . then the SGD method is guaranteed to converge to a local optimum [1]. . As a final note, observe that we need $V_{ pi}(S_t)$ in order to perform SGD weights update. This many not always be available. We may, however, have in hand an approximation of it, let&#39;s calle it $U_t$ i.e. $V_{ pi}(S_t) approx U_t$. In this scenario, we are forced to use the latter. However, if $U_t$ is an unbiasd estimate, then $ mathbf{w}_t$ is guaranteed to converge to a local optimum under the conditions specified above for decreasing $ eta$ [1]. . References . Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. | An overview of gradient descent optimization algorithms |",
            "url": "https://pockerman.github.io/qubit_opus/stochastic-gradient-descent/gradient-descent/reinforcement-learning/algorithms/2021/04/12/stochastic-gradient-descent.html",
            "relUrl": "/stochastic-gradient-descent/gradient-descent/reinforcement-learning/algorithms/2021/04/12/stochastic-gradient-descent.html",
            "date": " • Apr 12, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Controllability",
            "content": "Overview . When we want to control a dynamical system, the natural question that arises is to what extent can we achive this? The term controllability is used to describe whether a systme is controllable altogether. We will see that whether a system is controllable or not is determined completely by the controllability matrix [1] . Controllability . Recall that we deal with linear systems of the form . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . We can understand whether the linear system above is controllable or not by examing the controllability matrix $ mathbf{ cal{C}}$. In particular, the column space of that matrix. This matrix is defined as follows . $$ mathbf{ cal{C}} = begin{bmatrix} mathbf{B} &amp;&amp; mathbf{AB} &amp;&amp; mathbf{A}^2 mathbf{B} &amp;&amp; dots &amp;&amp; mathbf{A}^{n-1} mathbf{B} end{bmatrix}$$ . Where $n$ is the number of state variables. If the controllability matrix has $n$ linearly independent columns, then the system under consideration is controllable [1]. Note that this does not mean that the columns of $ mathbf{ cal{C}}$ should be linearly independent. All that we require, is that we can find $n$ linearly independent columns (see example 2 below). Let&#39;s see two examples taken from [1]. . Example 1 . Let&#39;s consider the following system . $$ frac{d}{dt} begin{bmatrix}x_1 x_2 end{bmatrix} = begin{bmatrix}1 &amp;&amp; 0 0 &amp;&amp; 2 end{bmatrix} begin{bmatrix}x_1 x_2 end{bmatrix} + begin{bmatrix}0 1 end{bmatrix} u$$ . Immediatelly, we can see that the system is not controllable. This is because the two state variables are decoupled and the control input affects only $x_2$. The controllability matrix is . $$ mathbf{ cal{C}} = begin{bmatrix}0 &amp;&amp; 0 1 &amp;&amp; 2 end{bmatrix} $$ . and we can see that the columns of that matrix are not independent. . Example 2 . If we include a control signal for the both state variables, we can turn this system into a controllable one. The new system now is . $$ frac{d}{dt} begin{bmatrix}x_1 x_2 end{bmatrix} = begin{bmatrix}1 &amp;&amp; 0 0 &amp;&amp; 2 end{bmatrix} begin{bmatrix}x_1 x_2 end{bmatrix} + begin{bmatrix}1 &amp;&amp; 0 0 &amp;&amp; 1 end{bmatrix} begin{bmatrix} u_1 u_2 end{bmatrix} u$$ . By including a control signal for bith state variables, we can control them independently. The controllability matrix now becomes [1] . $$ mathbf{ cal{C}} = begin{bmatrix}1 &amp;&amp; 0 &amp;&amp; 1 &amp;&amp; 0 0 &amp;&amp; 1 &amp;&amp; 0 &amp;&amp; 2 end{bmatrix} $$ . and we can verify that the columns of this matrix do span $ mathbb{R}^2$. Note that the columns of the matrix above are not linearly independent. Indeed the third column is a copy of the first and the fourth is a product of the second. However, the first two columns do span $ mathbb{R}^2$. . Three equivalent conditions . The span of the columns of matrix $ mathbf{ cal{C}}$ form a Krylov subspace [1]. The space determines which state vectors can be controlled. Hence, controllability implies two things [1] . Eigenvalue placement | Any state vector $ boldsymbol{ xi} in mathbb{R}^n$ is reachable with some actuation signal | . The following three conditions are equivalent [1] . Controllability | Arbitrary eigenvalue placement | Reachability of $ mathbb{R}^n$ | . We already saw what controllability means in terms of the controllability matrix. Arbitrary eigenvalue placement means that we can design the eigenvalues of the system through the choice of the feedback signal $ mathbf{u}$. In particular, for $ mathbf{u} = - mathbf{K} mathbf{x}$, the system becomes . $$ frac{d mathbf{x}}{dt} = ( mathbf{A} - mathbf{B} mathbf{K}) mathbf{x}$$ . The matrix $ mathbf{K}$ is called the gain matrix. There are various methods to design this matrix. Finally, reachability in practical terms means that we can steer the system to any arbitrary state with some actuation signal, or, conversely, there exists an actuation signal so that the system can be pushed to an arbitrary state $ boldsymbol{ xi} in mathbb{R}^n$ [1]. . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/dynamical-systems/linear-systems/control/controllability/2021/04/11/controllability.html",
            "relUrl": "/dynamical-systems/linear-systems/control/controllability/2021/04/11/controllability.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Longitudinal Car Model",
            "content": "Longitudinal car model . In this section, we will go over the concept of the vehicle longitudinal dynamics. The two major elements of the longitudinal vehicle model discussed in this section are . Vehicle dynamics | Powertrain dynamics | . The vehicle dynamics are influenced by longitudinal tire forces, aerodynamic drag forces, rolling resistance forces and gravitational forces. The longitudinal powertrain system of the vehicle consists of the internal combustion engine, the torque converter, the transmission and the wheels. This video explains nicely the concepts. . The longitudinal vehicle dynamic model is simply based on the dynamics of the vehicle that generate forward motion. The following figure shows a typical vehicle longitudinal motion force diagram on an inclined road. . Figure 1. Schematics of vehicle logitudinal model on an inclined road. Image from [1]. . We have the followig forces acting on the vehicle . The front tire forces $F_{xf}$ | The rear tire forces $F_{xr}$ | The aerodynamic drag force $F_{aero}$ | The rolling resistance forces $R_{xf}$ and $R_{xr}$ | The force due to gravity $F_g$ | . According to Newton’s laws of motion, and in particular the second law, the longitudinal tire forces of the front and rear tyres, $F_{xf}$ and $F_{xr}$, should balance the resistance forces $F_{aero}$, the gravitational force $F_g$ , and the rolling resistance of the front and rear tires, $R_{xf}$ and $R_{xr}$. Any imbalance between these forces creates an acceleration of the vehicle in the longitudinal direction denoted by $ ddot{x}$. Thus, the basic logintudinal motion model is given by . $$m ddot{x} = F_{xf} + F_{xr} - F_{aero} - F_g - R_{xf} - R_{xr}$$ . where $m$ is the mass of the vehicle. The forces $F_{xf}$ and $F_{xr}$ come from the vehicle power train. We can express them collectively as $F_x$. Furthermore, we group together the rolling resistance forces under the symbol $R_x$. Thus, the reduced model is . $$m ddot{x} = F_x - F_{aero} - F_g - R_x $$ . We will need a way to express the involved quantities in order to be able to solve for $ ddot{x}$. Let&#39;s start with the gravitational force $F_g$. . Gravitational froce . We can express $F_g$ as [2] . $$F_g = mg sin ( alpha)$$ . where $ alpha$ is the local road slope. For small slope angles, we can write . $$sin ( alpha) approx alpha$$ . Aerodynamic drag . A vehicles longitudinal motion is resisted by aerodynamic drag rolling resistance and the force due to gravity. The aerodynamic drag force $F_{aero}$ is typically modeled as dependent on air density $ rho$, frontal area of the vehicle $A$, the vehicles coefficient of friction $C_D$, and the current speed of the vehicle. The functional relationship of all these quantities is given in the equation beow . $$F_{aero} = frac{1}{2}C_D rho A v^2$$ . Rolling resistance . Tires are elastic materials that are subject to deformation in the patch which is in contact with the road surface. Let&#39;s neglect the the deformation of the road. The tire is subject to a normal load. Due to this load, the tire material will be deflected normally at the contact patch and then regaining its shape whilst leaving the patch neighborhood. However, internal damping of the material does not allow the energy lost during deforming the tire to be completely recovered when the material returns to its original shape [1]. It appears therefore, that some loss of energy occurs. This loss is represented by a force on the tires called the rolling resistance that acts in the opposite direction of the motion of the vehicle. . Hence, the rolling resistance depends on the normal tire load, tire pressure and vehicle speed. A model is given below [1], . $$R_x = N(c_{r, 0} + c_{r,1}| dot{x}| + c_{r,2}| dot{x}|^2)$$ . see also [2] for further modelling. If we assume nominal operating conditions and drop the second-order terms for simplicity, we can arrive at a linear rolling resistance model, where $c_{r,1}$ is the linear rolling resistance coefficient. . $$R_x approx c_{r,1}| dot{x}|$$ . Tire forces . We now discuss the longitudinal tire forces expressed under the term $F_x$. Longitudinal tire forces depend on the following factors [2] . Slip ratio | Normal load on the tires | Friction coefficient on the tire road interface | . Let&#39;s see these components . Slip ratio . For an effective wheel radius $R_{effective}$ and a wheel velocity $ omega_w$ the velocity is described by . $$V_{wheel} = R_{effective} omega_{wheel}$$ . However, the actual longitudinal velocity at the axle of the wheel, $V_x$ may be different than that. This is called longitudinal slip [2]. In other words, the longitudinal slip is defined as [2] . $$ sigma = V_{wheel} - V_x$$ . Moreover, we define the longitudinal slip ratio during braking and acceleration as [2] . $$ sigma_{xf} = begin{cases} frac{R_{effecive} omega_{wf} - V_x}{V_{x}}, ~~ text{during breaking} frac{R_{effecive} omega_{wf} - V_x}{R_{effecive} omega_{wf}}, ~~ text{during acceleration} end{cases}$$ . We have a similar expression for the rear wheels. Given the slip coefficients, we can express the longitudinal tire forces as . $$F_{xf} = C_{ sigma f} sigma_{xf}, ~~ F_{xr} = C_{ sigma r} sigma_{xr}$$ . where $C_{ sigma f}$ and $C_{ sigma r}$ are called the longitudinal tire stiffness parameters of the front and rear tires respectively [2]. . Powertrain forces . The longitudinal tire forces, denoted collectivelly above with $F_x$, acting on the driving wheels are the main forces that drive the vehicle forward [2]. These forces depend on the difference between the rotational wheel velocity $R_{effective} omega_{w}$ and the vehicle longitudinal velocity $ dot{x}$. In particular, we saw that we can model the longitudinal tire forces as . $$F_{xf} = C_{ sigma f} sigma_{xf}, ~~ F_{xr} = C_{ sigma r} sigma_{xr}$$ . where $C_{ sigma f}$ and $C_{ sigma r}$ are called the longitudinal tire stiffness parameters of the front and rear tires respectively [2]. However, $ omega_w$ is highly influence by the powertrain dynamics of the vehicle. The powertrain has the following major components [2] . Engine | Transmission or gearbox | Torque converter or clutch | Differential | Wheels | . Figure 2. Powertrain schematics. Image from [1]. . Let&#39;s see each of the components separately . Torque converter . The torque cnverter connects the engine to the transmission. When the engine is turning slowly, e.g. when the car waits at a stoplight, the amount of torque passed through the torque converter is very small. Thus, maintaining the the car stopped requires only a light pressure on the brake pedal. Hence, we don&#39;t have to stall the engine in order to maintain the vehicle stopped. In contrast, when the vehicle accelerates the torque converter gives the car more torque [2]. . The torque converter has the following major components [2] . pump | turbine | transmission fluid | . The pump turns at the same speed as the engine whilst the turbine is connected to the transmission and causes the transmission to spin at the same speed as the turbine [2] This is what basically moves the vehicle. The coupling between the turbine and the pump is through the transmission fluid. Torque is transmitted from the pump to the turbine of the torque converter [2]. . Various models have been introduced to model the pump torque $T_{pump}$ and the turbine torque $T_{turbine}$ see [2 page 103]. . Transmission dynamics . Let&#39;s denote with $GR$ the gear ratio of the transmission. In general, $GR &lt; 1$ and increases as the gear shifts upwards. The input to the transmission module is the torbine torque $T_{turbine}$ [2]. The torque transmitted to the wheels is $T_{wheels}$. Then, at steady state, this torque is given by . $$ T_{wheels} = frac{1}{GR} T_{turbine}$$ . Furthermore, we have the following relaton between the transmission and the wheel speed [2] . $$ omega_{transmission} = frac{1}{GR} omega_{wheels}$$ . Note that these equations cannot be used during gear change. See [2 page 105] for a model based on first order equations. . Engine dynamics . A simplified engine dynamic model is . $$J_{engine} dot{ omega}_e = T_{engine} - T_{pump}$$ . In general, the engine torque $T_{engine}$ depends on the dynamics in the intake and exhaust manifold of the engine and on the accelerator input from the driver [2]. $T_{pump}$ is the torque from the pump is the load of the engine from the torque converter [2]. . Wheel dynamics . The driving wheels rotational dynamics, e.g. for the rear wheels in a rear wheel driven vehicle, are dictated by [2] . $$J_{wheel} dot{ omega}_{wheel, r} = T_{wheel} - R_{effective}F_{xr}$$ . For the non-driven wheels the torque term is zero. . Refernces . Lesson 4: Longitudinal Vehicle Modeling | Rajamani R. Longitudinal Vehicle Dynamics. In: Vehicle Dynamics and Control., Mechanical Engineering Series. Springer 2012. |",
            "url": "https://pockerman.github.io/qubit_opus/longitudinal-dynamics/autonomous-vehicles/mathematical-modelling/vehicle-dynamics/2021/04/10/longitudinal-vehicle-model.html",
            "relUrl": "/longitudinal-dynamics/autonomous-vehicles/mathematical-modelling/vehicle-dynamics/2021/04/10/longitudinal-vehicle-model.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Linear Time-Invariant Systems",
            "content": "Linear time-invariant systems . Linear systems form a cornerstone of mathematical modelling of dynamical systems. Indeed a system or some aspects of it can be modelled using a linear model. Furthermore, non-linear systems can be linearized around a certain mode of operation. In this section, we give a brief overview of linear time-invariant systems. The major advantage of linear systems is that in terms of analysis a far simpler. Moreover, understanding the dynamics and thus stability of the system is easier. . Let&#39;s consider the following system . $$ frac{d mathbf{x}}{dt} = mathbf{f}( mathbf{x}, mathbf{u}), ~~ mathbf{y} = mathbf{g}( mathbf{x}, mathbf{u})$$ . As before, $ mathbf{x}$ represents the state of the modelled system whilst $ mathbf{u}$ represents some control input to the system. Note that the right-hand side terms do not depend explicitly on the time variable $t$. . If $ mathbf{f}$ or $ mathbf{g}$ or both are non-linear, then the system is non-linear. In this case, we can linearize the system i.e. its dynamics, using a Taylor series expansion near a fixed point $( bar{ mathbf{x}}, bar{ mathbf{u}})$. Recall that at a fixed point is a point where . $$ mathbf{f}( bar{ mathbf{x}}, bar{ mathbf{u}}) = mathbf{0}$$ . The linear, or linearized, dynamics can be written in the following matrix form (assuming no errors) . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . Unforced dynamics . When $ mathbf{u} = mathbf{0}$ and when there are no measurement errors i.e. $ mathbf{y} = mathbf{x}$. The system reduces to . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x}$$ . The solution to this ODE is [1] . $$ mathbf{x}(t) = e^{ mathbf{A}t} mathbf{x}(0)$$ . Thus $ mathbf{x}(t)$ depends or is determined entirely by the matrix $ mathbf{A}$. The stability of the unforced system therefore, can be understood via the eigenvalues and eigenvectors of $ mathbf{A}$. In particular we have the following cases . All the eigenvalues $ lambda$ satisfy $Re( lambda) &lt; 0$. Then the system is stable and all solutions decay to $ mathbf{u} = mathbf{0}$ as $t rightarrow infty$ | There exists at least one eigenvalue $ lambda$ with $Re( lambda) &gt; 0$ then the system is unsatble and will diverge from the fixed point along the corresponding unstable eigenvector direction. | . Forced dynamics . Now let&#39;s assume that $ mathbf{u} neq mathbf{0}$ and that $ mathbf{x}(0) = mathbf{0}$. In this case the solution up to time $t$ is given by [1] . $$ mathbf{x}(t) = int_{0}^t e^{ mathbf{A}(t - tau)} mathbf{B} mathbf{u}( tau)d tau $$ . This integral is nothing more than a convolution. Thus, we can write . $$ mathbf{x}(t) = e^{ mathbf{A}t} mathbf{B} * mathbf{u}(t)$$ . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/dynamical-systems/linear-systems/ode/time-invariant/2021/04/10/linear-time-invariant-systems.html",
            "relUrl": "/dynamical-systems/linear-systems/ode/time-invariant/2021/04/10/linear-time-invariant-systems.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Η Τραγώδια Της Κύπρου",
            "content": "from IPython.display import YouTubeVideo YouTubeVideo(&#39;UAdKpsdCQtc&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;00iwLvhAomQ&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;dDvmbjiorEM&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;BIs2Y_ndq7w&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;9pdh_XnoFpM&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;f-pKhRLp4ko&#39;, width=800, height=300) .",
            "url": "https://pockerman.github.io/qubit_opus/%CE%B9%CF%83%CF%84%CE%BF%CF%81%CE%AF%CE%B1/%CE%BA%CF%8D%CF%80%CF%81%CE%BF%CF%82/%CE%B5%CE%B9%CF%83%CE%B2%CE%BF%CE%BB%CE%AE/2021/04/09/tragodia-kipros.html",
            "relUrl": "/%CE%B9%CF%83%CF%84%CE%BF%CF%81%CE%AF%CE%B1/%CE%BA%CF%8D%CF%80%CF%81%CE%BF%CF%82/%CE%B5%CE%B9%CF%83%CE%B2%CE%BF%CE%BB%CE%AE/2021/04/09/tragodia-kipros.html",
            "date": " • Apr 9, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Lorenz System Simulation",
            "content": "Lorenz System Simulation . The Lorenz system is a system of ordinary differential equations first studied by Edward Lorenz. The system of ODEs is given below . $$ dot{x} = sigma(y -x), ~~ dot{y} = x ( rho - z) -y, ~~ dot{z} = xy - beta z$$ . The model has three parameters i.e. $ sigma, rho$ and $ beta$. . It is notable for having chaotic solutions for certain parameter values and initial conditions. In particular, the Lorenz attractor is a set of chaotic solutions of the Lorenz system. In popular media the &#39;butterfly effect&#39; stems from the real-world implications of the Lorenz attractor, i.e. that in any physical system, in the absence of perfect knowledge of the initial conditions (even the minuscule disturbance of the air due to a butterfly flapping its wings), our ability to predict its future course will always fail. This underscores that physical systems can be completely deterministic and yet still be inherently unpredictable even in the absence of quantum effects. The shape of the Lorenz attractor itself, when plotted graphically, may also be seen to resemble a butterfly. . import numpy as np from mpl_toolkits import mplot3d import matplotlib.pyplot as plt . class ODE45(object): def __init__(self, dt, n_steps, rhs, y0) -&gt; None: self._dt = dt self._n_steps = n_steps self._rhs = rhs self._time = 0.0 self._yold = y0 def step(self): k1 = self._k1(t=self._time) k2 = self._k2(t=self._time, k1=k1) k3 = self._k3(t=self._time, k2=k2) k4 = self._k4(t=self._time, k3=k3) self._yold += k1/6. + k2/3. + k3/3. + k4/6. def integrate(self) -&gt; None: self._time = 0.0 solutions = [[self._yold[0]], [self._yold[1]], [self._yold[2]]] times = [self._time] for itr in range(self._n_steps): self.step() self._time += self._dt times.append(self._time) solutions[0].append(self._yold[0]) solutions[1].append(self._yold[1]) solutions[2].append(self._yold[2]) return times, solutions def _k1(self, t: float) -&gt; np.array: return self._dt * self._rhs(t, self._yold) def _k2(self, t: float, k1: np.array) -&gt; np.array: return self._dt * self._rhs(t + 0.5 * self._dt, self._yold + 0.5 * k1) def _k3(self, t: float, k2: np.array) -&gt; np.array: return self._dt * self._rhs(t + 0.5 * self._dt, self._yold + 0.5 * k2) def _k4(self, t: float, k3: np.array) -&gt; np.array: return self._dt * self._rhs(t + self._dt, self._yold + k3) . class LorenzRhs(object): def __init__(self, beta: np.array): self._beta = beta def __call__(self, t: float, x:np.array) -&gt; np.array: result = np.array([beta[0]*(x[1] - x[0]), x[0]*(beta[1] - x[2]) - x[1], x[0]*x[1] - beta[2]*x[2]]) return result . beta = np.array([10., 28., 8./3.]) x0 = np.array([0., 1.0, 20.]) dt = 0.001 n_steps = 50000 . rhs = LorenzRhs(beta=beta) rk45 = ODE45(dt=dt, n_steps=n_steps, rhs=rhs, y0=x0) . times, solutions = rk45.integrate() . fig = plt.figure(figsize=(15,15)) ax = plt.axes(projection=&#39;3d&#39;) ax.plot3D(solutions[0], solutions[1], solutions[2], &#39;gray&#39;) . [&lt;mpl_toolkits.mplot3d.art3d.Line3D at 0x7fd911347a90&gt;] . The following video discusses how to simulate the Lorenz system with Matlab. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EnsB1wP3LFM&#39;, width=800, height=300) .",
            "url": "https://pockerman.github.io/qubit_opus/lorenz-system/python/simulation/numerics/2021/04/09/simulate-lorenz-system.html",
            "relUrl": "/lorenz-system/python/simulation/numerics/2021/04/09/simulate-lorenz-system.html",
            "date": " • Apr 9, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Scala Functions",
            "content": "The Scala programming language is a JVM based language worthwhile exploring. In this post I give a brief review of functions in Scala as I continue my exploration of the language. . Functions, in general, are at the core of every programming language when it comes to code organization and implementin the DRY principle. Scala supports two types of functions namely functions and methods. The difference is that a method operates on an object a function does not. . We define a function as follows . def myFunc(x: Integer) = if(x &gt;=0) x else -x . When defining a function we must . specify the types of all parameters | if the function is not recursive i.e. does not call itself, we do not have to specify the return type | if the body of the function requires more than one expression,then we should use a block i.e. {}. The last expression of the block becomes the value that the function returns. | . As an aside, it is possible to omit the return type of the function. Indeed, the Scala compiler can determine the return type from the type of the expression to the right of the = symbol. However, this may not be always the case. The following example demonstrates that . def myAbs(x: Double) = if(x &gt;=0) x else -x . val x = -10 myAbs(x) . x: Int = -10 res1_1: Double = 10.0 . However, with a recursive function, we must specify the return type . def fuc(x: Int): Int = if(x &lt;= 0) 1 else x*fuc(x-1) . Default &amp; Named Arguments . Just like C++, Scala also supports default arguments i.e. the default arguments for functions that are used when we do not specify explicit values . def showMe(x: Int=5) = println(&quot;You want to show &quot; + x) . showMe() &gt; You want to show 5 showMe(6) &gt; You want to show 6 . One of the features that I really like in Python is the named argument(s). This feature is very handy for understanding arguments at call sites i.e. I don&#39;t need to do the trip to the (unavailable) documentation but more important is really helpful in mitigating errors. Scala also supports the idea of named arguments . def speak(arg1: String, arg2: String, arg3: String=&quot; the end&quot;) = println(arg1 + arg2 + arg3) . speak(arg2=&quot; is &quot;, arg1=&quot;This &quot;) &gt; This is the end . As you can see, the named arguments need not be in the same order as the parameters. Furthermore, we can mix unnamed and named arguments, provided the unnamed ones come first. This is similar to Python. . Variable Arguments . Often it is useful to have a function that can take a variable number of arguments think of printf. Usually we call these as varargs functions Scala supports this idea too . def sum(args: Int*): Int = { var result = 0 for(arg &lt;- args) result += arg result } . val s = sum(1, 4, 9, 16, 25) &gt; s:Int = 55 . The actual type received by the function is of type Seq. However, we can not do the following . val s = sum(1 to 5) &gt; cmd10.sc:1: type mismatch; found : scala.collection.immutable.Range.Inclusive required: Int val s = sum(1 to 5) ^Compilation Failed Compilation Failed . That&#39;s because if the sum function is called with one argument, that must be a single integer. Here is how we can fix this . val s = sum(1 to 5:_*) &gt; s:Int = 15 . References . Cay Horstmann Scala for the impatient, Addison-Wesley. |",
            "url": "https://pockerman.github.io/qubit_opus/scala/functions/programming/2021/04/08/scala-functions.html",
            "relUrl": "/scala/functions/programming/2021/04/08/scala-functions.html",
            "date": " • Apr 8, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Logistic Map Simulation",
            "content": "Logistic Map Simulation With Python . The logistic map is a discrete time system of the form . $$x_{k+1} = beta x_k (1-x_k)$$ . The logistic map is a polynomial mapping (equivalently, recurrence relation) of degree 2, often cited as an archetypal example of how complex, chaotic behaviour can arise from very simple non-linear dynamical equations. The map was popularized in a 1976 paper by the biologist Robert May,[1] in part as a discrete-time demographic model analogous to the logistic equation written down by Pierre François Verhulst . import numpy as np import matplotlib.pyplot as plt . betas = np.linspace(0.0, 4.0, 400) . def get_steady_state(xinit, nitrs, beta): xold = xinit for itr in range(nitrs): xnew = (xold - xold**2)*beta xold = xnew return xold . def iterate(xinit, betas, use_steady_state, steady_state_itrs, itrs): xvals = [] beta_vals = [] xinit = xinit for beta in betas: #print(&quot;Working with beta={0}&quot;.format(beta)) if use_steady_state: xold = get_steady_state(xinit=xinit, nitrs=steady_state_itrs, beta=beta) else: xold = xinit xss = xold for i in range(itrs): xnew = (xold - xold**2)*beta xold = xnew beta_vals.append(beta) xvals.append(xnew) # if this is the case # the solution is boring :) if np.abs(xnew - xss) &lt; 0.001: break return beta_vals, xvals . beta_vals, xvals = iterate(xinit=0.5, betas=betas, use_steady_state=True, steady_state_itrs=2000, itrs=1000) . plt.plot(beta_vals, xvals) plt.xlabel(&quot;beta&quot;) plt.ylabel(&quot;x&quot;) plt.show() . beta_vals, xvals = iterate(xinit=0.5, betas=betas, use_steady_state=False, steady_state_itrs=2000, itrs=1000) . plt.plot(beta_vals, xvals) plt.xlabel(&quot;beta&quot;) plt.ylabel(&quot;x&quot;) plt.show() . from IPython.display import YouTubeVideo YouTubeVideo(&#39;_BvAkyuWhOI&#39;, width=800, height=300) .",
            "url": "https://pockerman.github.io/qubit_opus/logistic-map/python/simulation/numerics/2021/04/08/logistic-map-simulation.html",
            "relUrl": "/logistic-map/python/simulation/numerics/2021/04/08/logistic-map-simulation.html",
            "date": " • Apr 8, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Deutsch's Algorithm",
            "content": "Quantum computers pose as the future of computing. Although, at the time of writing, the computing harwdare based on quantum mechanics principles can accommodate only a small number of qubits, algorithms have been developed that demonstrate the superiority of quantum computers for certain class problems. . One such algorithm, and perhaps the simplest one is Deutsch&#39;s algorithm. The algorithm solves the following problem [1] . Given a boolean function $f: {0,1 } rightarrow {0,1 }$ determine if $f$ is constant. . The algorithm, can solve the problem with fewer calls to the function $f$ than is possible on a classical machine [1]. A function is called constant if $f(0) = f(1)$. On the other hand, if $f$ is one-to-one, is called balanced [1]. . Using a classical computer we need to do two evaluations of the function; one for each of the two inputs [1, 2]. On the other hand, Deutsch&#39;s algorithm requires only a single call to a black box to solve the problem. The key to the algorithm is the ability to place the second qubit of the input to the black box in a superposition [2]. Let&#39;s see how to do this. . Deutsch&#39;s algorithm works by putting both qubits representing the two inputs into a superposition [1]. The way to do this is using the Hadamard gate. The following image shows this schematically. . Figure 1. Deutsch&#39;s algorithm circuit. Image from [1]. . Let&#39;s study how the state system $| psi rangle$ evolves. Initially the system is at . $$| psi rangle = |01 rangle$$ . Appication of the Hadamard gate moves the two qubits respectively to . $$|0 rangle = frac{|0 rangle + |1 rangle}{ sqrt{2}}$$ . $$ |1 rangle = frac{|0 rangle - |1 rangle}{ sqrt{2}}$$ . Thus, $| psi rangle$ will be at . $$| psi rangle = left[ frac{|0 rangle + |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . Let&#39;s rename the top qubit as $|x rangle$. We want to evaluate $f(x)$. Note that when the bottom qubit is put into a superposition and then multiply by $U_f$, the system will be at state [1] . $$| psi rangle = (-1)^{f(x)}|x rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . Given however that $|x rangle$ is also in superposition, we will have that the system will be at state [1] . $$| psi rangle = left[ frac{(-1)^{f(0)}|0 rangle + (-1)^{f(1)}|1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . The actual state, as shown in the equation above, depends on the values of $f$. We can summarize this as follows [1]. . $$| psi rangle = begin{cases} ( pm1) left[ frac{|0 rangle + |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] ( pm1) left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] end{cases}$$ The final step is to apply the Hadamard gate on the top qubit. Recall that the Hadamard matrix is its own inverse. Thus applying it to the top qubit we get [1] . $$| psi rangle = begin{cases} ( pm1) |0 rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right], ~~ text{if} ~~ f ~~ text{is constant} ( pm1) |1 rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right], ~~ text{if} ~~ f ~~ text{is balanced} end{cases}$$ Now, we simply measure the top qubit. If it is in state $|0 rangle$, then we know that f is a constant function [1]. This was all accomplished with only one function evaluation. . One of the nice points demonstared by the algorithm is that a change of basis can allow solving a problem that otherwise requires more questions to the oracle. In Deutsch algorithm, we start in the canonical basis $|01 rangle$. The first application of the Hadamard matrices is used to change the basis to go into a balanced superposition of basic states. While in this noncanonical basis, we evaluate $f$ with the bottom qubit. The last Hadamard matrix is used as a change of basis matrix to revert back to the canonical basis [1]. . import numpy as np import random . H = np.array([[1.0/np.sqrt(2.0), 1.0/np.sqrt(2.0)], [1.0/np.sqrt(2.0), - 1.0/np.sqrt(2.0)]]) . def oracle(x, y, constant): if constant: f0 = 0 #random.choice([0,1]) f1 = 0 #random.choice([0,1]) else: f0 = 0 #random.choice([0,1]) f1 = 1 #random.choice([0,1]) return np.array([(-1)**f0*x[0], (-1)**f1*x[1]]) . zero = np.array([1., 0.]) one = np.array([0.0, 1.0]) . zero_H = np.dot(H, zero) one_H = np.dot(H, one) . print(zero_H) print(one_H) . [0.70710678 0.70710678] [ 0.70710678 -0.70710678] . out_oracle = oracle(x=zero_H, y=one_H, constant=True) . x = np.dot(H, out_oracle) . print(x) . [1. 0.] . out_oracle = oracle(x=zero_H, y=one_H, constant=False) . x = np.dot(H, out_oracle) . print(x) . [0. 1.] . References . Noson S. Yanofsky and Mirco A. Mannucci, Quantum Computing for Computer Scientists, Cambridge University Press | Eleanor Rieffel, Wolfgang Polak, Quantum Computing: A Gentle Introduction, The MIT Press. | Deutsch&#39;s algorithm |",
            "url": "https://pockerman.github.io/qubit_opus/deutsch/quantum-computing/algorithms/numerics/2021/03/20/deutsch-algo.html",
            "relUrl": "/deutsch/quantum-computing/algorithms/numerics/2021/03/20/deutsch-algo.html",
            "date": " • Mar 20, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "Singular Value Decomposition",
            "content": "One of the most important matrix factorization techniques is the singular value decomposition most often abbreviated as SVD. The reason why is so popular lies on the fact that it is the foundation for many other computational techniques. For example, just to name a few: . Computing pseudo-inverses | Obtaining low-rank matrix approximations | Dynamic mode decomposition | Proper orthogonal ecomposition | Principal components analysis | . For a complex matrix $A in mathbb{C}^{n times m}$, its SVD is . $$A = U Sigma V^{*}$$ . where $V^{*}$ is the complex conjugate transpose. Both $U$ and $V$ are unitary matrices that is the following holds . $$UU^{*} = U^{*}U = I$$ . In general, if a matrix $W$ is a real matrix i.e. its entries are real numbers, then $W^{*} = W^T$. Thus, if $A in mathbb{R}^{n times m}$ the matrices $U$ and $V$ are real orthogonal matrices i.e. . $$UU^{T} = U^{T}U = I$$ . The matrix $ Sigma$ is a diagonal matrix with real and nonnegative entries on the diagonal. The entries $ Sigma_{ii}$ are called the singular values of $A$. The number of the non-zero singular values corresponds to the rank of the matrix $A$. . Given the popularity of the SVD method, it is not surpsising that most linear algebra libraries provide a way to perform it. The following script shows how to compute the SVD in Python using numpy . import numpy as np X = np.random.rand(10 , 10) U, S, V = np.linalg.svd(X, full_matrices=True) # or doing economy SVD U, S, V = np.linalg.svd(X, full_matrices=False) . You can find the documentation at numpy.linalg.svd. Similarly, using the Blaze C++ library . template&lt; typename MT1, bool SO, typename VT, bool TF, typename MT2, typename MT3 &gt; void svd( const DenseMatrix&lt;MT1,SO&gt;&amp; A, DenseMatrix&lt;MT2,SO&gt;&amp; U, DenseVector&lt;VT,TF&gt;&amp; s, DenseMatrix&lt;MT3,SO&gt;&amp; V ); . Overall, the SVD algorithm is a very important matrix decomposition technique used throughout numerical modeling control theory and system identification. We will see applications of the method in future posts. .",
            "url": "https://pockerman.github.io/qubit_opus/linear-algebra/singular-value-decomposition/algorithms/numerics/2021/03/13/singular-value-decomposition.html",
            "relUrl": "/linear-algebra/singular-value-decomposition/algorithms/numerics/2021/03/13/singular-value-decomposition.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Scala Arrays",
            "content": "Scala Arrays . There are two types of arrays in Scala just like in most programming languanges. Fixed length and variable length arrays. . Fixed-length arrays . If we know the size of the needed array and that size does not change, we can use the Array class. . val nums = new Array[Int](10) . nums: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) . nums(0) = 10 . nums . res2: Array[Int] = Array(10, 0, 0, 0, 0, 0, 0, 0, 0, 0) . A Scala Array is implemented as a Java array. For example the nums array above, inside the JVM is represented as int[] in the JVM. . Variable-length arrays . Variable-length arrays in Scala are utilized via the ArrayBuffer class . val b = ArrayBuffer[Int]() . cmd3.sc:1: not found: value ArrayBuffer val b = ArrayBuffer[Int]() ^Compilation Failed . Compilation Failed . In contrast to the Array class, we need to explicitly import ArrayBuffer . import scala.collection.mutable.ArrayBuffer . import scala.collection.mutable.ArrayBuffer . val b = ArrayBuffer[Int]() . b: ArrayBuffer[Int] = ArrayBuffer() . We can now add elements to the buffer . b += 1 . res5: ArrayBuffer[Int] = ArrayBuffer(1) . or add more than one elements in one go . b += (5, 6, 7, 8, 9) . res6: ArrayBuffer[Int] = ArrayBuffer(1, 5, 6, 7, 8, 9) . b ++= Array(0, 0, 0) . res7: ArrayBuffer[Int] = ArrayBuffer(1, 5, 6, 7, 8, 9, 0, 0, 0) . There are various operations supported by the ArrayBuffer class; check the Scala documentation. One thing to note however is the following. Adding or removing elements at the end of an ArrayBuffer is an amortized constant time operation [1]. We can insert and remove elements at an arbitrary location, but those operations are not as efficient since all elements after that location must be shifted [1]. . Traversing arrays . Scala is much more uniform compared to C++ when it comes to traversing arrays. . for(i &lt;- 0 until b.length) println(b(i)) . 1 5 6 7 8 9 0 0 0 . Note that we can also use a guard inside the for expression . for(i &lt;- 0 until b.length if b(i) &gt; 0) println(b(i)) . 1 5 6 7 8 9 . Algorithms . Scala arrays have built-in some commin algorithms e.g. sum and sort, min and max . println(&quot;Max element of b &quot; + b.max) println(&quot;Min element of b &quot; + b.min) println(&quot;Sum of element of b &quot; + b.sum) . Max element of b 9 Min element of b 0 Sum of element of b 36 . The sorted method sorts an Array or ArrayBuffer and returns the sorted array without modifying the original [1]. . val newB = b.sorted println(b) . ArrayBuffer(1, 5, 6, 7, 8, 9, 0, 0, 0) . newB: ArrayBuffer[Int] = ArrayBuffer(0, 0, 0, 1, 5, 6, 7, 8, 9) . Note that you can sort an Array, but not an array buffer, in place [1]. Also note that for the min, max , and quickSort algorithms, the element type must have a comparison operation. This is the case for types with the Ordered trait [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/arrays/programming/2021/01/08/scala-arrays.html",
            "relUrl": "/scala/arrays/programming/2021/01/08/scala-arrays.html",
            "date": " • Jan 8, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Scala Values, Variables & Types",
            "content": "Values &amp; variables . A val variable denotes a variable with a constant value; we cannot change its contents [2]. In this sense calling it a variable may not be that correct. Obviously, a val must be initialized at the point of declaration. . val v1 = 0.5 . v1: Double = 0.5 . v1 = 1.0 . cmd1.sc:1: reassignment to val val res1 = v1 = 1.0 ^Compilation Failed . Compilation Failed . val v2; . (console):1: &#39;=&#39; expected but &#39;;&#39; found. val v2; ^ . (console):1: &#39;=&#39; expected but &#39;;&#39; found. val v2; ^ . In contrast, a var variable means that it can have its contents changed [2]. . var one = 1.0 . one: Double = 2.0 . // if we want we can say that one equals 2 one = 2.0 . In Scala we don&#39;t need to specify te type of a value or a variable. This is inferred from the type of the expression that is used to initialize it. However, here is how we can specify the type if needed . val hello: String = &quot;Hello&quot; val weight: Double = 14.5 . hello: String = &#34;Hello&#34; weight: Double = 14.5 . Lazy values . Lazy evaluation is a programming technique where the evaluation of an expression is done when it is needed for the first time and not before. Lazy evaluation is useful when we want to delay costly initialization statements. Moreover, lazy evaluation can deal with circular dependencies and for developing lazy data structures. . Scala allows to use the keyword lazy when declaring val values. In this case the value initialization is deferred until it is accessed for the first time. . val v1 = 3.5 . v1: Double = 3.5 . lazy val v2 = 3.6 . v2: Double = 3.6 . println(&quot;What is the value of v2?&quot; + v2) . What is the value of v2?3.6 . . Remark . Lazy evaluation is not cost-free. In fact, every time a lazy value is accessed, a method is called that checks, in a threadsafe manner, whether the value has already been initialized [2]. . . Types . In Scala there is no distinction between primitives and class types [2]; all types in Scala are classes. This means we can do things likes calling methods on numbers . 1.toString . res7: String = &#34;1&#34; . or things like . 1.to(5) . res8: Range.Inclusive = Range(1, 2, 3, 4, 5) . In Scala, we do not need wrapper types [2]. The Scala compiler does the conversion for us between primitive types and wrappers. For example, if you make an array of Int , you get an int[] array in the virtual machine [2]. . The compiler also checks that we do not combine expressions of different type. For example: . 1 to 4.0 . cmd9.sc:1: type mismatch; found : Double(4.0) required: Int val res9 = 1 to 4.0 ^Compilation Failed . Compilation Failed . References . Martin Odersky, Lex Spoon, Bill Venners, Programming in Scala, 3rd Edition, artima. | Cay Horstmann, Scala for the Impatient 1st Edition, |",
            "url": "https://pockerman.github.io/qubit_opus/scala/values-variables/programming/2021/01/05/scala-values-variables-types.html",
            "relUrl": "/scala/values-variables/programming/2021/01/05/scala-values-variables-types.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Occupancy Grids",
            "content": "Overview . In this post we look into occupancy grid maps. . Occupancy grids . Simply put an occupancy grid is a discretized version of the environment surrounding the ego vehicle [3]. The discretization can be either two or three dimensional. The following images show versions of 2D and 3D grids. . Figure 1. 2D occupancy grid map. . Figure 2. 3D occupancy grid map. . Each cell in an occupancy grid indicates if the space represented by the cell is empty or occupied by an obstacle. Given this, the more dense the grid is the finer the representation of the environment will be. However, the more dense the grid is the more computationally expensive is to create it. Each cell in the grid is binary i.e. it has a value, $m_i$, of either one or zero [3]. . Typically, in order to create an occupancy grid, we make the following assumptions [3] . Static environment i.e. no dynamic objects | Grid cells are independent | The vehicle state is known | . Given the gride cell value $mi_i$, we can construct a belief map meaning a map at time $t$ such that for each cell . $$bel_{t}(m_i) = P(m_i | (x,y))$$ . where $(x,y)$ denotes the vehicle state and the sensor measurements for a given cell. We can establish a threshold at which a given belief can be classified as occupied. Furthermore, we can combine measurements from different time steps to obtain a more accurate belief. In particular, using Baye&#39;s theorem we can come up with the following equation [3] . $$bel_{t}(m_i) = eta P(y_t | m_i)bel_{t-1}(m^i)$$ . where $ eta$ is a scaling or normalization constant to ensure that the $bel$ function represents a probability. . Disadvantages of occupancy grids . In occupancy grid mapping every grid cell is one of two states; occupied or empty [2]. But in some situations it makes sense for a cell to be partially filled. This may occur when only part of the grid cell is filled and the rest is empty, or when the objects that occupies the grid cell have special characteristics [2]. For example it may be that we want a grid cell occupied with vegetation to be somehow less occupied than a grid cell filled with solid rock. . Semi-transparent obstacles and Mitigation Classical occupancy grids have trouble dealing with semi-transparent obstacles such as glass and vegetation. These obstacles may return hits to the laser rangefinder about half of the time, but eventually the occupancy grid will converge to either occupied or not, both of which are incorrect. [2] . A possible solution to this problem would be to consider a more continous measure that measures the density or probability of a beam to have reflected and not passed. Perhaps the simplest approach towards this is to treat the random variable as a biased coin and for each state keep a count of the number of hits and pass throughs [2]. Thus, the only difference is that each state tracks two numbers. Then, the probability can be calculated empirically as the ratio of the hits to the sum of hits and passes. For the inverse sensor model, we either spread the high probability zone or use fractional number of hits and misses for each state [2]. . References . Bruno Siciliano, Lorenzo Sciavicco, Luigi Villani, Giuseppe Oriolo, Robotics Modelling, Planning and Control, Springer | Drew Bagnell Statistical Techniques in Robotics, Lecture notes. | Motion planning for self-driving cars |",
            "url": "https://pockerman.github.io/qubit_opus/robotics/planning/mapping/navigation/occupancy-grid/2020/09/15/occupancy-grids.html",
            "relUrl": "/robotics/planning/mapping/navigation/occupancy-grid/2020/09/15/occupancy-grids.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Gradient Descent",
            "content": "Gradient Descent . Perhaps the simplest algorithm for uncostrained optimization is gradient descent also known as steepest descent. Consider the following function [1] . $$f( theta_1, theta_2) = frac{1}{2}( theta_{1}^2 - theta_2)^2 + frac{1}{2}( theta_1 -1)^2$$ . We are interested in finding $ theta_1, theta_2$ that minimize $f$. Gradient descent is an iterative algorithm that uses the gradient of the function in order to update the parameters. The update rule is . $$ boldsymbol{ theta}_k = boldsymbol{ theta}_{k-1} - eta nabla f|_{ boldsymbol{ theta}_{k-1}} $$ . $ eta$ is the so called learning rate and tunes how fast we move to the direction of the gradient. A small $ eta$ slows down convergence whilst a large value may not allow convergence of the algorithm. This is shown in the two figures below: . Figure 1. Gradient descent with eta 0.1. . Figure 2. Gradient descent with eta 0.6. . The code below is a simple implementation of the gradient descent algorithm. . import numpy as np import matplotlib.pyplot as plt . def f(theta1, theta2): return 0.5*(theta1**2 - theta2)**2 + 0.5*(theta1 -1.0)**2 . def f_grad(theta1, theta2): return (2.0*theta1*(theta1**2 - theta2) + (theta1 - 1.0), -(theta1**2 - theta2)) . def gd(eta, itrs, tol): coeffs_series = [] coeffs = [0.0, 0.0] coeffs_series.append([coeffs[0], coeffs[1]]) val_old = f(theta1=coeffs[0], theta2=coeffs[1]) for itr in range(itrs): grad = f_grad(theta1=coeffs[0], theta2=coeffs[1]) coeffs[0] -= eta*grad[0] coeffs[1] -= eta*grad[1] coeffs_series.append([coeffs[0], coeffs[1]]) val = f(theta1=coeffs[0], theta2=coeffs[1]) abs_error = np.abs(val - val_old) print(&quot;&gt;Iteration {0} absolute error {1} exit tolerance {2}&quot;.format(itr, abs_error, tol)) if abs_error &lt; tol: print(&quot;&gt;GD converged with residual {0}&quot;.format(np.abs(val - val_old))) return coeffs_series val_old = val return coeffs_series . coeffs_series = gd(eta=0.1, itrs=100, tol=1.0e-4) . &gt;Iteration 0 absolute error 0.09494999999999998 exit tolerance 0.0001 &gt;Iteration 1 absolute error 0.07622463831103915 exit tolerance 0.0001 &gt;Iteration 2 absolute error 0.05968293530998797 exit tolerance 0.0001 &gt;Iteration 3 absolute error 0.04523783343545831 exit tolerance 0.0001 &gt;Iteration 4 absolute error 0.03333771938318547 exit tolerance 0.0001 &gt;Iteration 5 absolute error 0.024254166137182898 exit tolerance 0.0001 &gt;Iteration 6 absolute error 0.01782239274907732 exit tolerance 0.0001 &gt;Iteration 7 absolute error 0.013533685475623586 exit tolerance 0.0001 &gt;Iteration 8 absolute error 0.010768584908705553 exit tolerance 0.0001 &gt;Iteration 9 absolute error 0.008983651981479004 exit tolerance 0.0001 &gt;Iteration 10 absolute error 0.007786932467621618 exit tolerance 0.0001 &gt;Iteration 11 absolute error 0.006930789314960939 exit tolerance 0.0001 &gt;Iteration 12 absolute error 0.0062723026732663945 exit tolerance 0.0001 &gt;Iteration 13 absolute error 0.005733543774474825 exit tolerance 0.0001 &gt;Iteration 14 absolute error 0.0052730560188138376 exit tolerance 0.0001 &gt;Iteration 15 absolute error 0.004868553639578624 exit tolerance 0.0001 &gt;Iteration 16 absolute error 0.00450745300901266 exit tolerance 0.0001 &gt;Iteration 17 absolute error 0.004182026970546315 exit tolerance 0.0001 &gt;Iteration 18 absolute error 0.003887028764196082 exit tolerance 0.0001 &gt;Iteration 19 absolute error 0.0036185521793817496 exit tolerance 0.0001 &gt;Iteration 20 absolute error 0.0033734842877468432 exit tolerance 0.0001 &gt;Iteration 21 absolute error 0.0031492347340679877 exit tolerance 0.0001 &gt;Iteration 22 absolute error 0.0029435928019137803 exit tolerance 0.0001 &gt;Iteration 23 absolute error 0.0027546441698343416 exit tolerance 0.0001 &gt;Iteration 24 absolute error 0.0025807166947806187 exit tolerance 0.0001 &gt;Iteration 25 absolute error 0.0024203414221908165 exit tolerance 0.0001 &gt;Iteration 26 absolute error 0.0022722224779619174 exit tolerance 0.0001 &gt;Iteration 27 absolute error 0.0021352127684441946 exit tolerance 0.0001 &gt;Iteration 28 absolute error 0.002008293861733755 exit tolerance 0.0001 &gt;Iteration 29 absolute error 0.0018905590854755572 exit tolerance 0.0001 &gt;Iteration 30 absolute error 0.0017811992002927796 exit tolerance 0.0001 &gt;Iteration 31 absolute error 0.0016794901835057927 exit tolerance 0.0001 &gt;Iteration 32 absolute error 0.0015847827649886279 exit tolerance 0.0001 &gt;Iteration 33 absolute error 0.0014964934298845774 exit tolerance 0.0001 &gt;Iteration 34 absolute error 0.0014140966564608337 exit tolerance 0.0001 &gt;Iteration 35 absolute error 0.0013371181986932233 exit tolerance 0.0001 &gt;Iteration 36 absolute error 0.0012651292559366714 exit tolerance 0.0001 &gt;Iteration 37 absolute error 0.0011977413984442034 exit tolerance 0.0001 &gt;Iteration 38 absolute error 0.001134602138991761 exit tolerance 0.0001 &gt;Iteration 39 absolute error 0.0010753910584805904 exit tolerance 0.0001 &gt;Iteration 40 absolute error 0.001019816407902937 exit tolerance 0.0001 &gt;Iteration 41 absolute error 0.0009676121210644636 exit tolerance 0.0001 &gt;Iteration 42 absolute error 0.0009185351824353497 exit tolerance 0.0001 &gt;Iteration 43 absolute error 0.0008723633028205682 exit tolerance 0.0001 &gt;Iteration 44 absolute error 0.0008288928625002738 exit tolerance 0.0001 &gt;Iteration 45 absolute error 0.0007879370873338197 exit tolerance 0.0001 &gt;Iteration 46 absolute error 0.0007493244282380136 exit tolerance 0.0001 &gt;Iteration 47 absolute error 0.0007128971186049562 exit tolerance 0.0001 &gt;Iteration 48 absolute error 0.000678509887739322 exit tolerance 0.0001 &gt;Iteration 49 absolute error 0.0006460288113815469 exit tolerance 0.0001 &gt;Iteration 50 absolute error 0.0006153302829237615 exit tolerance 0.0001 &gt;Iteration 51 absolute error 0.000586300091094321 exit tolerance 0.0001 &gt;Iteration 52 absolute error 0.0005588325917408928 exit tolerance 0.0001 &gt;Iteration 53 absolute error 0.000532829962933393 exit tolerance 0.0001 &gt;Iteration 54 absolute error 0.0005082015339742743 exit tolerance 0.0001 &gt;Iteration 55 absolute error 0.00048486318008077005 exit tolerance 0.0001 &gt;Iteration 56 absolute error 0.00046273677552067724 exit tolerance 0.0001 &gt;Iteration 57 absolute error 0.0004417496988608476 exit tolerance 0.0001 &gt;Iteration 58 absolute error 0.00042183438475075323 exit tolerance 0.0001 &gt;Iteration 59 absolute error 0.00040292791732379415 exit tolerance 0.0001 &gt;Iteration 60 absolute error 0.00038497166087553616 exit tolerance 0.0001 &gt;Iteration 61 absolute error 0.00036791092397959677 exit tolerance 0.0001 &gt;Iteration 62 absolute error 0.00035169465363989703 exit tolerance 0.0001 &gt;Iteration 63 absolute error 0.00033627515646207293 exit tolerance 0.0001 &gt;Iteration 64 absolute error 0.00032160784416228154 exit tolerance 0.0001 &gt;Iteration 65 absolute error 0.0003076510010270577 exit tolerance 0.0001 &gt;Iteration 66 absolute error 0.00029436557119745174 exit tolerance 0.0001 &gt;Iteration 67 absolute error 0.000281714963878678 exit tolerance 0.0001 &gt;Iteration 68 absolute error 0.00026966487477881555 exit tolerance 0.0001 &gt;Iteration 69 absolute error 0.000258183122257602 exit tolerance 0.0001 &gt;Iteration 70 absolute error 0.0002472394968245067 exit tolerance 0.0001 &gt;Iteration 71 absolute error 0.0002368056227645514 exit tolerance 0.0001 &gt;Iteration 72 absolute error 0.0002268548307944717 exit tolerance 0.0001 &gt;Iteration 73 absolute error 0.00021736204076207993 exit tolerance 0.0001 &gt;Iteration 74 absolute error 0.00020830365349946717 exit tolerance 0.0001 &gt;Iteration 75 absolute error 0.00019965745102808012 exit tolerance 0.0001 &gt;Iteration 76 absolute error 0.0001914025043919798 exit tolerance 0.0001 &gt;Iteration 77 absolute error 0.00018351908846457078 exit tolerance 0.0001 &gt;Iteration 78 absolute error 0.0001759886031370006 exit tolerance 0.0001 &gt;Iteration 79 absolute error 0.00016879350035171707 exit tolerance 0.0001 &gt;Iteration 80 absolute error 0.0001619172164950512 exit tolerance 0.0001 &gt;Iteration 81 absolute error 0.00015534410970717907 exit tolerance 0.0001 &gt;Iteration 82 absolute error 0.000149059401708577 exit tolerance 0.0001 &gt;Iteration 83 absolute error 0.0001430491237779689 exit tolerance 0.0001 &gt;Iteration 84 absolute error 0.00013730006654984238 exit tolerance 0.0001 &gt;Iteration 85 absolute error 0.000131799733328664 exit tolerance 0.0001 &gt;Iteration 86 absolute error 0.00012653629664396288 exit tolerance 0.0001 &gt;Iteration 87 absolute error 0.00012149855779407855 exit tolerance 0.0001 &gt;Iteration 88 absolute error 0.00011667590914838256 exit tolerance 0.0001 &gt;Iteration 89 absolute error 0.00011205829899737585 exit tolerance 0.0001 &gt;Iteration 90 absolute error 0.00010763619875779401 exit tolerance 0.0001 &gt;Iteration 91 absolute error 0.00010340057235624662 exit tolerance 0.0001 &gt;Iteration 92 absolute error 9.934284762933097e-05 exit tolerance 0.0001 &gt;GD converged with residual 9.934284762933097e-05 . coeffs_x = [] coeffs_y = [] for item in coeffs_series: coeffs_x.append(item[0]) coeffs_y.append(item[1]) . theta1 = np.linspace(0.0, 2.0, 100) theta2 = np.linspace(-0.5, 3.0, 100) . X, Y = np.meshgrid(theta1, theta2) . Z = f(X, Y) . plt.contour(X, Y, Z, 60, colors=&#39;black&#39;); plt.plot(coeffs_x, coeffs_y, &#39;r-o&#39;) plt.show() . References . Kevin P. Murphy, Machine Learning A Probabilistic Perspective, The MIT Press |",
            "url": "https://pockerman.github.io/qubit_opus/gradient-descent/unconstrained-optimization/machine-learning/algorithms/numerics/2020/06/22/gradient-descent.html",
            "relUrl": "/gradient-descent/unconstrained-optimization/machine-learning/algorithms/numerics/2020/06/22/gradient-descent.html",
            "date": " • Jun 22, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Viterbi Algorithm",
            "content": "The backward and forward algorithms can be used to compute $P(O| lambda)$. In this notebook we are interested in computing the most likely path given a sequence $O$ and a hidden Markov model $ lambda$. The Viterbi algorithm gives us a way to do so. The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states, also called the Viterbi path, that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM) [2]. The algorithm uses a maximum operation instead of the sum. The operation of Viterbi&#39;s algorithm can be visualized by means of a trellis diagram [2]. It is essentially the shortest path through this trellis. . Given a state sequence $Q=q_1q_2, cdots,q_T$ and an observation sequence $O=O_1O_2, cdots,O_T$ we dfine the variable $ delta_t(i)$ as the probability of the highest probability path at time $t$ that accounts for the first $t$ observations and ends in $S_i$ [1]: . $$ delta_t(i) = max_{Q}p(q_1q_2, cdots,q_t=S_i,O_1O_2, cdots,O_t| lambda)$$ . Then we can recursively calculate $ delta_{t+1}(i)$ and the optimal path can be read by backtracking from $T$ , choosing the most probable at each instant. The algorithm is as follows [1]: . Initialize | . $$ delta_1(i) = pi_i b_i(O_1)$$ . This is initialization is the same as in the forward algorithm. To retrieve the state sequence we also need to keep track of the argument which maximized for each $t$ and $j$. We therefore use the array $ psi$, and in the initialization step the first $ psi$ variable of every state will be equal to 0 because no specific argument coming from the initial probability maximized the value of the first state. . $$ psi_1(i) = 0$$ . Recurse | . $$ delta_t(j) = max_{i=1}^{N} delta_{t-1}(i)a_{ij}b_j(O_t)$$ . $$ psi_t(j) = argmax_{i=1}^{N} delta_{t-1}(i)a_{ij}$$ . Termination | . $$p^{*} = max_i delta_T(i)$$ . $$q^{*}_T = arg max_i delta_T(i)$$ . Path backtracking | . $$q_t{*} = psi_{t+1}(q^{*}_{t+1}), t= T-1, T-2, cdots, 1$$ . $ psi_t (j)$ keeps track of the state that maximizes $ delta_t(j)$ at time $t-1$, that is, the best previous state. The Viterbi algorithm has the same complexity with the forward phase, where instead of the sum, we take the maximum at each step [1]. . Let&#39;s see an example applying the Viterbi algorithm. The example is taken from [2]. Some coding hints from Implement Viterbi Algorithm in Hidden Markov Model using Python and R have also been used. . import numpy as np . obs_to_idx = {&#39;normal&#39;:0, &#39;cold&#39;: 1, &#39;dizzy&#39;:2} # state to index map state_to_idx = {&#39;Healthy&#39;:0, &#39;Fever&#39;:1} . pi = np.array([0.6, 0.4]) # transition probabilities A = np.array([[0.7, 0.3], [0.4, 0.6]]) # emission probabilties B = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]) . o = [&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;] . delta = np.zeros(shape=(len(o), A.shape[0])) previous = np.zeros((len(o)-1, A.shape[0])) for st in state_to_idx: state_idx = state_to_idx[st] delta[0][state_idx] = pi[state_idx] * B[state_idx][obs_to_idx[o[0]]] print(delta) . [[0.3 0.04] [0. 0. ] [0. 0. ]] . for t in range(1, len(o)): obs_idx = obs_to_idx[o[t]] for i in state_to_idx: i_st_idx = state_to_idx[i] probs=[] for j in state_to_idx: j_st_idx = state_to_idx[j] probs.append(delta[t - 1][j_st_idx]*A[j_st_idx][i_st_idx]*B[i_st_idx][obs_idx]) # This is our most probable state given previous state at time t (1) previous[t-1, i_st_idx] = np.argmax(probs) delta[t, i_st_idx] = np.max(probs) # Path Array S = np.zeros(len(o)) # Find the most probable last hidden state last_state = np.argmax(delta[len(o) - 1, :]) S[0] = last_state backtrack_index = 1 for i in range(len(o) - 2, -1, -1): S[backtrack_index] = previous[i, int(last_state)] last_state = previous[i, int(last_state)] backtrack_index += 1 # Flip the path array since we were backtracking S = np.flip(S, axis=0) # Convert numeric values to actual hidden states path = [] for s in S: if s == 0: path.append(&quot;Healthy&quot;) else: path.append(&quot;Fever&quot;) . print(&quot;Path is &quot;, path) . Path is [&#39;Healthy&#39;, &#39;Healthy&#39;, &#39;Fever&#39;] . Another way to compute the $ delta$ matrix is the following more Pythonic way, taken from Implement Viterbi Algorithm in Hidden Markov Model using Python and R. Note the use of the log function. . delta = np.zeros(shape=(len(o), A.shape[0])) previous = np.zeros((len(o)-1, A.shape[0])) for st in state_to_idx: state_idx = state_to_idx[st] delta[0, :] = np.log(pi * B[:, obs_to_idx[o[0]]]) print(delta) . [[-1.2039728 -3.21887582] [ 0. 0. ] [ 0. 0. ]] . for t in range(1, len(o)): obs_idx = obs_to_idx[o[t]] for i in state_to_idx: i_st_idx = state_to_idx[i] # Same as Forward Probability probability = delta[t - 1] + np.log(A[:, i_st_idx]) + np.log(B[i_st_idx, obs_idx]) # This is our most probable state given previous state at time t (1) previous[t - 1, i_st_idx] = np.argmax(probability) # This is the probability of the most probable state (2) delta[t, i_st_idx] = np.max(probability) # Path Array S = np.zeros(len(o)) # Find the most probable last hidden state last_state = np.argmax(delta[len(o) - 1, :]) S[0] = last_state backtrack_index = 1 for i in range(len(o) - 2, -1, -1): S[backtrack_index] = previous[i, int(last_state)] last_state = previous[i, int(last_state)] backtrack_index += 1 # Flip the path array since we were backtracking S = np.flip(S, axis=0) # Convert numeric values to actual hidden states path = [] for s in S: if s == 0: path.append(&quot;Healthy&quot;) else: path.append(&quot;Fever&quot;) . print(&quot;Path is &quot;, path) . Path is [&#39;Healthy&#39;, &#39;Healthy&#39;, &#39;Fever&#39;] . The following video provides a motivation behind the use of the Viterbi algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;MPeedE6Odj0&#39;, width=800, height=300) . The following video provides nice description of the Viterbi algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;s9dU3sFeE40&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Viterbi algorithm, Wikipedia. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20viterbi-algorithm/dynamic-programming/algorithms/numerics/2020/05/24/viterbi-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20viterbi-algorithm/dynamic-programming/algorithms/numerics/2020/05/24/viterbi-algorithm.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Backward Algorithm",
            "content": "The backward algorithm is the complement of the forward algorithm. Let&#39;s introduce the backward variable $ beta_t(i)$. This is the probability of being in $S_i$ at time $t$ abd observing the partial sequence $O_{t+1}, cdots,O_T$ [1]. This can be written as . $$ beta_t(i) = P(O_{t+1}, cdots,O_T | q_t=S_i, lambda)$$ . The backward algorithm computes this recursively . Initialize | . $$ beta_T(i) = 1$$ . Recurse | . $$ beta_t(i) = P(O_{t+1}, cdots,O_T | q_t=S_i, lambda)$$ . which can be written as, see [1], . $$ beta_{t}(i) = sum_{j}^{N}b_j(O_{t+1})a_{i,j} beta_{t+1}(j)$$ . Let&#39;s implement this as we did for the forward algorithm. . Assume a system with two states $S= {S_0, S_1 }$. Futher, assume that the observation sequence consists of elements from the following set $V= {a, b,c }$. Also let&#39;s assume the following HMM: . $$ boldsymbol{ pi}= begin{bmatrix}0.6 &amp; 0.4 end{bmatrix}$$ . $$ mathbf{A}= begin{bmatrix}0.7 &amp; 0.3 0.4 &amp; 0.6 end{bmatrix}$$ . $$ mathbf{B}= begin{bmatrix} 0.5 &amp; 0.4 &amp; 0.1 0.1 &amp; 0.3 &amp; 0.6 end{bmatrix}$$ . Assume the following sequence $V= {a, b, c }$. We introduce the $ beta$ matrix: . $$ beta = begin{bmatrix}0 &amp; 0 0 &amp; 0 0 &amp; 0 end{bmatrix}$$ . First we initialize . $$ beta = begin{bmatrix}0 &amp; 0 0 &amp; 0 1 &amp; 1 end{bmatrix}$$ . Then use the recursion formula . $$ beta_{t}(i) = sum_{j}^{N}b_j(O_{t+1})a_{i,j} beta_{t+1}(j)$$ . import numpy as np . obs_to_idx = {&#39;a&#39;:0, &#39;b&#39;: 1, &#39;c&#39;:2} # state to index map state_to_idx = {&#39;S0&#39;:0, &#39;S1&#39;:1} . pi = np.array([0.6, 0.4]) # transition probabilities A = np.array([[0.7, 0.3], [0.4, 0.6]]) # emission probabilties B = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]) . o = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] . beta = np.zeros(shape=(len(o),A.shape[0])) . beta[len(o) - 1] = np.ones(A.shape[0]) . # start at the position one before the end # proceed until t is -1. Move back one step at the time for t in range(len(o)-2, -1, -1): for j in range(A.shape[0]): # for x = np.array([1.,2.]) and # y = np.array([1.,2.])then # x*y = np.array([1., 4.]) # that is element-wise product is performed beta[t, j] = (beta[t + 1] * B[:, obs_to_idx[o[t + 1]]]).dot(A[j, :]) . print(beta) . [[0.106 0.112] [0.25 0.4 ] [1. 1. ]] . Overall the forward and backward algorithms can be used to compute $P(O| lambda)$. Indeed using the forward algorithm we have: . $$P(O| lambda) = sum_{i}^{N} alpha_{T}(i)$$ . whilst using the backward algorithm . $$P(O| lambda) = sum_{i}^{N} pi_i b_i(O_1) beta_{1}(i)$$ . The following video nicely explains the motivation behind the backward algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EbxLWGw2zJ4&#39;, width=800, height=300) . The following video nicely explains both the forward and backward algorithms. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;gYma8Gw38Os&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Forward–backward algorithm. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20backward-algorithm/dynamic-programming/algorithms/numerics/2020/05/23/backward-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20backward-algorithm/dynamic-programming/algorithms/numerics/2020/05/23/backward-algorithm.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Forward Algorithm",
            "content": "Given a hidden Markov model $ lambda$, meaning . A set of $N$ states $ mathbb{S}= {S_1, dots S_N }$ | A set of $M$ distinct observation symbols $ mathbf{V} = {v_1 dots v_M }$ | A transition probability matrix $ mathbf{A}$ | An observations probabilities matrix $ mathbf{B}$ | An initial state of probabilities $ boldsymbol{ pi}$ | . we want to be able to compute the marginal probability $P(O| lambda)$. This is the so called evaluation problem [1]; given the observation sequence $O$ calculate the probability that the sequence can occur under $ lambda$ . In theory, this can be calculated by using . $$P(O| lambda) = sum_{Q}P(O, Q| lambda)$$ . where . $$P(O, Q| lambda) = P(q_1) Pi_{t=2}^{T}P(q_t|q_{t-1}) Pi_{t=1}^{T}P(O_t|q_{t})$$ . Despite this, you should note that there are $N^T$ possible $Q$s assuming that all probabilities are nonzero [1]. Thus, the marginalization step above is rather, or can be, computationally expensive. We need another way to calculate $P(O| lambda)$. . In order to compute the probability $P(O| lambda)$ we need to know the joint probability $P(O, Q| lambda)$. The forward algorithm allows us to compute the latter without using marginalization. It does so by using recursion. We will divide the observation sequence into two parts; the first part will be $[1,t]$, the second is $[t+1, T]$ [1]. We further define the forward variable $ alpha_t(i)$. This will denote the probability of observing the partial sequence $ {O_1, cdots,O_t }$ unitl time $t$ and being in $S_i$ at time $t$ given $ lambda$: . $$ alpha_t(i) = P(O_1, cdots,O_t, q_t = S_i | lambda)$$ . This can be calculated recursively: . Initialize | . $$ alpha_t(i) = pi_ib_i(O_1)$$ . Recurse | . $$ alpha_{t+1}(j) = P(O_1, cdots,O_{t+1}, q_{t+1} = S_j | lambda)$$ . which can be written as . $$ alpha_{t+1}(j) = [ sum_{i}^{N} alpha_t(i)a_{i,j}]b_j(O_{t+1})$$ . Now $ alpha_t(i)$ explains the first $t$ observations and ends in state $S_i$. We multiply with the transition probability $a_{ij}$ in order to move to state $S_j$. since the are $N$ possible previous states we have to sum over all of them. Finally, we weight the result with $b_j(O_{t+1})$ which is the probability of observing $O_{t+1}$ at state $S_j$ at time $t+1$. . Once we know the forward variables, it is easy to calculate $P(O| lambda)$: . $$P(O| lambda) = sum_{i}^{N} alpha_{T}(i)$$ . $ alpha_{T}(i)$ is the probability of generating the full observation sequence and ending up in state $S_i$. We need to sum up over all the possible final states. The $ alpha_{t}(i)$ can be represented as a matrix of size $T times N$. Where $T$ is the size of the observation sequence and $N$ the number of states. Let&#39;s see an example. . Let&#39;s see a simple example. Assume a system with three states $S= {S_1, S_2, S_3 }$. Futher, assume that the observation sequence consists of elements from the following set $V= {a, b,c }$. Also let&#39;s assume the following HMM: . $$ boldsymbol{ pi}= begin{bmatrix}0.7 &amp; 0.15 &amp; 0.15 end{bmatrix}$$ . $$ mathbf{A}= begin{bmatrix}0.5 &amp; 0.25 &amp; 0.25 0.1 &amp; 0.8 &amp; 0.1 0.3 &amp; 0.15 &amp; 0.6 end{bmatrix}$$ . $$ mathbf{B}= begin{bmatrix} 0.16 &amp; 0.26 &amp; 0.58 0.25 &amp; 0.28 &amp; 0.47 0.2 &amp; 0.1 &amp; 0.7 end{bmatrix}$$ . We want to calculate the probability $P(O| lambda)$ where $O= {a, b, a, c, b, a }$. We will use the forward algorithm for this. We will first do the computation using pencil and paper and then write a small Python script for us. We create the matrix $ alpha$: . $$ alpha = begin{bmatrix}0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 end{bmatrix}$$ . The first step is the initialization of the matrix. We take the first observation in the sequence $O$ which &#39;a&#39;. This has to be mapped to an index. Assume that we have available such a mapping: . $$ {a:0, b:1, c:2 }$$ . Further assume that we use zero-based counting. We have . $$ alpha_{0,0} = pi_0 mathbf{B}_{0, 0} = 0.7 0.16 = 0.112$$ . $$ alpha_{0,1} = pi_1 mathbf{B}_{1, 0} = 0.15 0.25 = 0.0375 $$ . $$ alpha_{0,2} = pi_2 mathbf{B}_{2, 0} = 0.15 0.2 = 0.03$$ . we now proceed to the calculation of the probabilities of the next symbols. The symbol &#39;b&#39;, which is the next symbol in $O$ has index 1. Its probabilities are . $$ alpha_{1,0} = mathbf{B}_{0,1}( alpha_{0,0} mathbf{A}_{0,0} + alpha_{0,1} mathbf{A}_{1,0} + alpha_{0,2} mathbf{A}_{2,0}) $$ . Similarly for $ alpha_{1,1}$ and $ alpha_{1,2}$ . $$ alpha_{1,1} = mathbf{B}_{1,1}( alpha_{0,0} mathbf{A}_{0,1} + alpha_{0,1} mathbf{A}_{1,1} + alpha_{0,2} mathbf{A}_{2,1}) $$ . $$ alpha_{1,2} = mathbf{B}_{2,1}( alpha_{0,0} mathbf{A}_{0,2} + alpha_{0,1} mathbf{A}_{1,2} + alpha_{0,2} mathbf{A}_{2,2}) $$ . After filling the matrix $ alpha$ we can calculate the probability $P(O| lambda)$, This is given by the following sum: . $$P(O| lambda) = alpha_{5,0} + alpha_{5,1} + alpha_{5,2}$$ . Below is a simple Python script that performs these tedious calculations for us. . import numpy as np . obs_to_idx = {&#39;a&#39;:0, &#39;b&#39;: 1, &#39;c&#39;:2} # state to index map state_to_idx = {&#39;S1&#39;:0, &#39;S2&#39;:1, &#39;S3&#39;: 2} . pi = np.array([0.7, 0.15, 0.15]) # transition probabilities A = np.array([[0.5, 0.25, 0.25], [0.1, 0.8, 0.1], [0.3, 0.15, 0.6]]) # emission probabilties B = np.array([[0.16, 0.26, 0.58], [0.25, 0.28, 0.47], [0.2, 0.1, 0.7]]) . o = [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] . a = np.zeros(shape=(len(o),A.shape[0])) . # initialize alpha for i in range(len(state_to_idx)): # only first row the rest is zero a[0][i] = pi[i]*B[i][obs_to_idx[o[0]]] . print(&quot;Initial probabilities&quot;) print(a) . Initial probabilities [[0.112 0.0375 0.03 ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ]] . for t in range(1, len(o)): for j in range(A.shape[0]): a[t][j] = 0 # fix j = state_idx and sum over the states for i in range(A.shape[0]): a[t][j] += a[t -1][i] * A[i][j] a[t][j] *= B[j][obs_to_idx[o[i]]] print(&quot;alpha matrix: &quot;) print(a) . alpha matrix: [[1.12000000e-01 3.75000000e-02 3.00000000e-02] [1.10000000e-02 1.56250000e-02 9.95000000e-03] [1.60760000e-03 4.18562500e-03 2.05650000e-03] [2.94290000e-04 1.01471875e-03 4.10872500e-04] [5.95005800e-05 2.36744594e-04 8.43135750e-05] [1.25950115e-05 5.42294641e-05 1.78275499e-05]] . prob = 0; for i in range(A.shape[0]): prob += a[len(o)-1][i] print(&quot;Probability for sequence {0} is {1} &quot;.format(o, prob)) . Probability for sequence [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] is 8.465202543750001e-05 . Log probabilities . The forward variable, as well as the backward variable used in the backward algorithm are calculated as products of probabilities. When we have long sequences this may result in underflow [1]. In order to avoid this, we normalize $ alpha_t(i)$ by multiplying it with . $$c_t = frac{1}{ sum_{j} alpha_t(j)}$$ . After this normalization, $P(O| lambda)$ is given by . $$P(O| lambda) = frac{1}{ Pi_t c_t}$$ . or . $$logP(O| lambda) = - sum_t log c_t$$ . The motivation behind the forward algorithm is nicely presented in the following video . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EbxLWGw2zJ4&#39;, width=800, height=300) . The following video nicely explains both the forward and backward algorithms. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;gYma8Gw38Os&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Forward–backward algorithm. | Lawrence R. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20forward-algorithm/dynamic-programming/algorithms/numerics/2020/05/22/forward-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20forward-algorithm/dynamic-programming/algorithms/numerics/2020/05/22/forward-algorithm.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Hidden Markov Models",
            "content": "We know that given a sample of independent observations we can get the likelihood of the sample by forming the product of the likelihoods of the individual instances. However, there are situations in which the assumption of observation independence simply breaks down. An example of such a situation is when we consider words from the English dictionary. In this case, within a word, successive letters are dependent; in English h is very likely to follow t but not x. In such a scenario, it is better to assume that the sequence is generated by a parametric random process. Our goal is to establish the parameters of this process. Hidden Markov models are a way to model such a process. Let&#39;s see how. . A hidden Markov model or HMM, is a statistical model in which the system being modeled is assumed to be a Markov process. Let&#39;s call that process with $X$. The system can be on a series of states from a give state set. However, we don&#39;t know at each time instant the specific state the system is in. In other words, the system state is unobservable. HMM assumes that there is another process, say $Y$, whose behavior depends on $X$. The goal is to learn about $X$ by observing $Y$. . Let&#39;s assume that at any time the system or random process we model can be in any of $N$ distinct states. Let&#39;s denote this set with $ mathbb{S}$ . $$ mathbb{S} = {S_1, S_2, cdots,S_N }$$ . Furthermore, let&#39;s denote the state that the system is at time $t$ by $q_t$. . The system can move from one state to another. The probability of being at state $S_j$ at time $t$ depends on the values of the previous states. We express this mathematically using the following conditional probability: . $$P(q_t=S_j | q_{t-1}=S_i, q_{t-2}=S_k, cdots)$$ . A hidden Markov model assume that system states form a Markov chain. To be more specific we will restrict ourselves to the first-order Markov model which is quite frequent in practice. In this case, the probability above simply becomes: . $$P(q_t=S_j | q_{t-1}=S_i, q_{t-2}=S_k, cdots)=P(q_t=S_j | q_{t-1}=S_i)$$ . In words, what the first order Markov property tells us is that the state of the system depends solely on the previous state; a rather memoryless situation. . Let&#39;s move further by introducing the so-called transition probabilities $ alpha_{i,j}$: . $$ alpha_{i,j}=P(q_t=S_j | q_{t-1}=S_i)$$ . Since the $ alpha_{i,j}$&#39;s are probabilities they should satisfy the followin constraints . $$ alpha_{i,j} geq 0, ~~~ sum_{j=1}^{N} alpha_{i,j} = 1$$ . These are nothing more than the usual axioms of the definition of probability. Note that for the latter condition we keep the $i$ index fixed. . We will assume that the transition probabilities are independent of time. What this means is that going from $S_i$ to $S_j$ has the same probability regardless of when it happens (i.e. in the observation sequence see below). . We usually arrange the transition probabilities into an $N times N$ matrix, denoted here with $ mathbf{A}$ that its rows sum to one. . We now have a way, or a model, that allows us to move from one state to another. However, we cannot do much with it as the states are unknown, hidden, unobserved or any other expression that suits your needs. The point is that we cannot access them. In order to have progress, hidden Markov models assume a second process that produces observation sequences. We can use this process to infer the state of the system. . Let&#39;s denote by $ lambda$ ( we will be more specific about what $ lambda$ denotes further below) the HMM instance we are using. Let $O_T$ be an observation sequence of length $T$. We assume that $O_T$ has elements from a given discrete set $ mathbb{V}$: . $$ mathbb{V}= {v_1, v_2, cdots, v_M }$$ . The set $ mathbb{V}$ has in total $M$ elements. Also let&#39;s introduce the mechanism that characterizes the generation of a sequence given a state $S_j$. This is done via the so-called emission probability matrix $b_j(m)$: . $$b_j(m) = P(O_t = v_m|q_t = S_j)$$ . this is the probability that we observe element $v_m$ at time $t$ when the system is at state $S_j$. For example, let&#39;s assume that that we have two states and three symbols and we are given the following emission probabilities matrix . $$ mathbf{B}= begin{bmatrix} 0.16 &amp; 0.26 &amp; 0.58 0.25 &amp; 0.28 &amp; 0.47 end{bmatrix}$$ . what this tells us is that at state $S_1$ symbol $v_1$ has probability 0.16 to be observed, symbol $v_2$ will be observed 26 % and symbol $v_3$ will be observed 58 %. . Although, we cannot observe the state sequence $Q$, this can be inferred from the observation sequence $O$. Note however that in general there are many different sequences $Q$ that can generate the same observation sequence. This is however done with different probabilities. This is similar when we have an iid sample from, say, a normal distribution; there are an infinite number of $ mu, sigma$ pairs possible which can generate the sameple. Thus, we are more interested in a maximum likelihood state sequence or a sequence that has the maximum probability of generating the sequence $O$. . What is $ lambda$? . Above we used the notation $ lambda$ in order to indicate a specific HMM instance. Let&#39;s see what this $ lambda$ parameter actually imply. This is also a summary of the basic element of an HMM. Specifically, . An HMM model assumes a set of states in the model $ mathbb{S} = {S_1, S_2, cdots,S_N }$ | . An HMM model assumes a number of distinct observation symbols $ mathbb{V}= {v_1, v_2, cdots, v_M }$ | . An HMM model assumes the existence of transition probabilities $ mathbf{A}$ where $ alpha_{i,j}=P(q_t=S_j | q_{t-1}=S_i)$ | . An HMM model assumes the existence of observation probabilities $ mathbf{B}$ where $b_j(m) = P(O_t = v_m|q_t = S_j)$ | . The last thing we need to talk about, is how to initialize the model. This is done by a vector of initial probabilties $ boldsymbol{ pi}$ where each $ pi_i = P(q_1 = S_i)$ that is each $ pi_i$ is the probability that the first state of the model is $S_i$. . The $ lambda$ parameter is the triplett consisting of the matrices $ mathbf{A}$, $ mathbf{B}$ and the vector $ boldsymbol{ pi}$ . $$ lambda = { mathbf{A}, mathbf{B}, boldsymbol{ pi} }$$ . For a state set with $N$ states, $ mathbf{A}$ is $N times N$. Likewise for a set $V$ with $M$ symbols, $ mathbf{B}$ is $N times M$. Finally the vector $ boldsymbol{ pi}$ has size $N$. . Typically, when dealing with an HMM we are intersted in the following three problems [1] . Given an HMM i.e. $ lambda$ evaluate the probability of a given observation sequence: | $$P(O| lambda)$$ . Given an HMM and an observation sequence $O$ we want to find the state sequence $Q$ with the highest probability of producing $O$ i.e we want to find $Q$ such that | $$P(Q|O, lambda) ~~ text{is maximum}$$ . Given a training set of observation sequences $ mathbf{X}$ we want to learn the HMM that maximizes the probability of generating $ mathbf{X}$ that is we want to find $ lambda$ so that | $$P( mathbf{X}| lambda)~~ text{is maximum}$$ . Checkout the video below for a motivation about Hidden Markov models . from IPython.display import YouTubeVideo YouTubeVideo(&#39;PAngl8DZ8yk&#39;, width=800, height=300) . The following video explains the Markov property . from IPython.display import YouTubeVideo YouTubeVideo(&#39;J_y5hx_ySCg&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning/algorithms/numerics/2020/05/21/hidden-markov-model.html",
            "relUrl": "/hidden-markov-model/machine-learning/algorithms/numerics/2020/05/21/hidden-markov-model.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Matrix Approximation",
            "content": "Very often in numerical modeling, the data we need to work with is rather large and therefore not really handy. Moreover, the behavior under investigation can be explained by small number of features. Thus, it is desireable to be able to approximate the matrices. One way to do so is using the singular value decomposition method. . In fact SVD provides an optimal low-rank approaximation to a matrix $A$ (this is the Eckart-Young theorem). We can obtain a hierarchy of low rank matrices by just keeping the leading $k$ singular values and the corresponding eigenvectors. . Image copression is a simple example illustrating matrix approximation using SVD. We can view a grayscale image as matrix $A in mathbb{R}^{n times m}$ where $n, m$ are the number of pixels in the vertical and horizonal directions. . The Python code below computes the full SVD of the matrix representing the loaded image. We then compute approximations of the image using a range of retained singular values. We can see that as the number of retained singular values increases the quality of the image increases. . import numpy as np from matplotlib import image import matplotlib.pyplot as plt from numpy.linalg import matrix_rank . def rgb2gray(rgb): return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]) . A = image.imread(&#39;my_icons/volvo_img.png&#39;) A = np.mean(A, -1)#rgb2gray(rgb=data) # summarize shape of the pixel array print(A.shape) # display the array of pixels as an image plt.imshow(A) plt.show() . (366, 424) . print(&quot;Matrix rank &quot;, matrix_rank(A)) . Matrix rank 307 . U, S, V = np.linalg.svd(A, full_matrices=False) S = np.diag(S) . print(&quot;Shape U &quot;, U.shape) print(&quot;Shape S &quot;, S.shape) print(&quot;Shape V &quot;, V.shape) . Shape U (366, 366) Shape S (366, 366) Shape V (366, 424) . for r in [5, 20, 100]: print(&quot;Working with r&quot;, r) # construct approximate image img_approx = U[:,:r] @ S[0:r,:r] @ V[:r,:] plt.imshow(img_approx) plt.show() . Working with r 5 . Working with r 20 . Working with r 100 . plt.semilogy(np.diag(S)) plt.show() . plt.plot(np.cumsum(np.diag(S))/np.sum(np.diag(S))) plt.show() .",
            "url": "https://pockerman.github.io/qubit_opus/linear-algebra/singular-value-decomposition/matrix-approximation/algorithms/numerics/2020/03/14/matrix-approximation.html",
            "relUrl": "/linear-algebra/singular-value-decomposition/matrix-approximation/algorithms/numerics/2020/03/14/matrix-approximation.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pockerman.github.io/qubit_opus/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://pockerman.github.io/qubit_opus/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Books",
          "content": "Mathematics . Kevin P. Murphy Machine Learning. A Probabilistic Perspective | Thomas Witelski and Mark Bowen Methods of Mathematical Modelling. Continuous Systems and Differential Equations | Steven L. Brunton, J. Nathan Kutz Data-Driven Science and Engineering Machine Learning, Dynamical Systems and Control | Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction | . Physics . Leonard Susskind and George Hrabovsky, Classical Mechanics. The Theoretical Minimum | Leonard Susskind and Art Friedman, Quantum Mechanics. The Theoretical Minimum | . Computing . Len Bass, Paul Clements, Rick Kazman Software Architecture in Practice | Robert C. Martin Clean Architecture. A Craftsman&#39;s Guide to Software Architecture and Design | David Mermin Quantum Computer Science. An Introduction | . Programming . Scott Meyers Effective C++: 50 Specific Ways to Improve Your Programs And Design | Scott Meyers Effective STL: 50 Specific Ways to Improve Your Use Of The STL | Scott Meyers Effective C++: 55 Specific Ways to Improve Your Programs And Design | Scott Meyers Effective Modern C++ | Bjorn Andrist and Viktor Sehr C++ High Performance | . History . Herodotus The Histories | Max Hastings Catastrophe | Thucydides The Histroy of the Peloponnesian War | . Philosophy . Sun Tzu The Art of War | Nicolo Machiavelli The Prince | . Literature . Antoine De Saint-Exupery Ο Μικρος Πριγγιπας | Albert Camus The Fall | Albert Camus The Outsider | George Orwell 1984 | Douglas R. Hostadter Godel, Escher, Bach: An Enternal Golden Braid | Fyodor Dostoyevsky Crime and Punishment | Fyodor Dostoyevsky The Idiot | Ernest Hemingway For Whom the Bell Tolls | Ernest Hemingway The Old Man and the Sea | .",
          "url": "https://pockerman.github.io/qubit_opus/books/",
          "relUrl": "/books/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pockerman.github.io/qubit_opus/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}