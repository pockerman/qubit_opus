{
  
    
        "post0": {
            "title": "K-d trees part 2",
            "content": "Note . Under development :) :) :) . Overview . In a previous post, we looked into what k-d trees are. In this post we want to go deeper into this view and attempt to implement a k-d tree in C++. The final code can be found here. Furthermore, we will follow the implementation from the excellent book of Marcello La Rocca Advanced algorithms and data structures by Manning Publications. . K-d trees part 2 . Before starting implementing a k-d tree let&#39;s recall that this a binary search tree i.e. a hierarchical data structure. Specifically, a k-d tree is a space partitioning data structure for organizing points in a k-dimensional space [1]. In a k-d tree every node in the tree represents a k-dimensional point [2]. Furthermore, we will assume that the coordinates of k-dimensional vector can be compared with each other. . Following [2], here is the exposed API: . template&lt;typename NodeType&gt; class KDTree { public: typedef NodeType node_type; typedef typename node_type::data_type data_type; KDTree(uint_t k); template&lt;typename Iterator, typename SimilarityPolicy, typename ComparisonPolicy&gt; KDTree(uint_t k, Iterator begin, Iterator end, const SimilarityPolicy&amp; sim_policy, const ComparisonPolicy&amp; comp_policy); bool empty()const noexcept; uint_t size()const noexcept; uint_t dim()const noexcept; template&lt;typename ComparisonPolicy&gt; std::shared_ptr&lt;node_type&gt; search(const data_type&amp; data, const ComparisonPolicy&amp; comp_policy)const; template&lt;typename Iterator, typename SimilarityPolicy, typename ComparisonPolicy&gt; void build(Iterator begin, Iterator end, const SimilarityPolicy&amp; sim_policy, const ComparisonPolicy&amp; comp_policy); template&lt;typename ComparisonPolicy&gt; std::shared_ptr&lt;node_type&gt; insert(const data_type&amp; data, const ComparisonPolicy&amp; comp_policy); template&lt;typename ComparisonPolicy&gt; std::vector&lt;std::pair&lt;typename ComparisonPolicy::value_type, typename NodeType::data_type&gt;&gt; nearest_search(const data_type&amp; data, uint_t n, const ComparisonPolicy&amp; calculator)const; }; . The class above accepts the tree node as a generic parameter that exposes the type of the data to be stored. In this perspective, the KDTree is a homogeneous container. . According to the exposed API we can construct a k-d tree in two ways; by specifying the size of the space or by passing a range of data to be stored in the tree. The first construct actually creates an empty tree. We can populate this tree by calling either insert or preferably build. We will explain below why this is the preferred method. . We can see that the exposed API does not have a remove or delete method. Typically, a k-d tree is constructed as remains as is. Furthermore, removing a node may result in an unbalanced tree which implies that the fast look up will not hold any more. Although it is possible to re-balance the tree see e.g. [2], we won&#39;t pursue this path here. So let&#39;s concentrate on the rest of the methods. Perhaps the most important of which is the build method. . In our implementation we distinguish between similarity and comparison. A similarity metric, or policy, is used in order to decide whether two points are similar or close enough in the given metric. A comparison policy is used in order to compare coordinates of points. Thus, we use the similarity policy to search in the tree. And we use the comparison policy whenever strict comparison of point coordinates is needed. . Building the tree . The build function accepts a range of iterators pointing to the data, a similarity policy and a comparison policy. It delegates all its to the call_ function; a private to the outside world class that implements the nuts and bolds of building the tree. The call_ function definition is shown below . template&lt;typename NodeType&gt; template&lt;typename Iterator, typename SimilarityPolicy, typename ComparisonPolicy&gt; void KDTree&lt;NodeType&gt;::create_(Iterator begin, Iterator end, uint_t level, const SimilarityPolicy&amp; sim_policy, const ComparisonPolicy&amp; comp_policy){ auto n_points = std::distance(begin, end); // nothing to do if no points // are given if(n_points == 0){ return ; } if(n_points == 1){ auto data = *begin; // create the root root_ = std::make_shared&lt;NodeType&gt;(level, data, nullptr, nullptr); ++n_nodes_; return; } // otherwise partition the range auto [median, left, right] = detail::partiion_on_median(begin, end, level, k_, comp_policy); // create root root_ = std::make_shared&lt;NodeType&gt;(level, median, nullptr, nullptr); ++n_nodes_; // create left and right subtrees auto left_tree = do_create_(left.first, left.second, level + 1, sim_policy, comp_policy); // create left and right subtrees auto right_tree = do_create_(right.first, right.second, level + 1, sim_policy, comp_policy); root_-&gt;left = left_tree; root_-&gt;right = right_tree; } . The implementation above is fairly straightforward. However, let&#39;s go over a few details. The detail::partiion_on_median accepts a range of points the current tree level and the comp_policy and returns the median point at this level and the data that is left and right to the calculated median. . template&lt;typename Iterator, typename ComparisonPolicy&gt; std::tuple&lt;typename std::iterator_traits&lt;Iterator&gt;::value_type, std::pair&lt;Iterator, Iterator&gt;, std::pair&lt;Iterator, Iterator&gt;&gt; partiion_on_median(Iterator begin, Iterator end, uint_t level, uint_t k, const ComparisonPolicy&amp; comp_policy){ ... // the median index auto median_idx = n_points % 2 == 0 ? (n_points + 1) / 2 : n_points / 2; // how to compare the data at the given // level. We use the level % k operation to decide // which coordinate to use auto compare = [&amp;](const value_type&amp; v1, const value_type&amp; v2){ auto idx = level % k; return comp_policy(v1, v2, idx); //v1[idx] &lt; v2[idx]; }; // rearrange the elements. Do partial sorting // std::nth_element(begin, begin + median_idx, end , compare); // get the data corresponding to the median auto median = *(begin + median_idx); // create the left are right sub-trees auto left = std::make_pair&lt;Iterator, Iterator&gt;(std::forward&lt;Iterator&gt;(begin), begin + median_idx); auto right = std::make_pair&lt;Iterator, Iterator&gt;(begin + median_idx + 1, std::forward&lt;Iterator&gt;(end)); return std::make_tuple(median, left, right); } . The implementation above uses std::nth_element to partially sort the elements. According to the documentation: . _nth_element is a partial sorting algorithm that rearranges elements in [first, last) such that_ . The element pointed at by nth is changed to whatever element would occur in that position if [first, last) were sorted. | All of the elements before this new nth element are less than or equal to the elements after the new nth element. | . We then recursively build the tree using do_create_ . References . k-d tree. | Marcello La Rocca, Advanced algorithms and data structures, Manning Publications. |",
            "url": "https://pockerman.github.io/qubit_opus/multidimensional-search/kd-trees/k-nearest-search/algorithms/c++/2022/04/25/kd-trees-part-2.html",
            "relUrl": "/multidimensional-search/kd-trees/k-nearest-search/algorithms/c++/2022/04/25/kd-trees-part-2.html",
            "date": " • Apr 25, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "K-means clustering",
            "content": "Overview . With Gaussian mixture models, a cluster is represented by using three elements; mean, variance and a weight. In this method each sample belongs to all clusters with a given probability. In many situations, however, we would like to have a single cluster per sample. One way to do so, is to use hard-clustering techniques. . A well known hard-clustering algorithm is k-means clustering. The algorithm is frequently used because it is easy to interpret and implement. In k-means clustering we find the shortest distance between the sample point and all the centroids of the clusters. A data point belongs to the cluster that has the smallest distance from the cluster centroid. . K-means clustering . The k-means algorithm proceeds by first partitioning a dataset into $k$ distinct clusters and then arbitrarily selects centroids of each of these clusters. It then iteratively updates partitions by first assigning points to the closer cluster and then it updates the centroids. This process is repeated until some convergence criteria is satisfied. . Three steps . Thus, k-means can be broken into four steps. In particular, . Initialization | Classification | Recentering | Repeat steps 2 and 3 | Steps 2-4 of the algorithm above compute exact distances between points and centroids, and then update the centroids to the center of mass of each cluster. However, given the random initialization step, the algorithm qualifies also as a Monte Carlo randomized algorithm [1]. . The initialization step is crucial for the algorithm [1]. It influences heavily the final result; a bad choice can slow down convergence and will likely lead to an early stop and a poor result [1]. Furthermore, because the algorithm will remove centroids to which no points are assigned, e.g. when the initial choice consists of several centroids close to each other, this could lead to an unwanted reduction of the number of centroids in the early stages of the algorithm. Therefore, in practice, the algorithm is always used with a random-restart strategy; it is run several times on the dataset, each time with a different random initialization of the centroids, and then results are compared to choose the best clustering [1]. . The aforementioned process minimizes the total intra-cluster variation across all clusters. Mathematically, k-means clustering minimizes a loss function. For instance assuming that we minimize a Euclidean distance, . $$L = sum_{j=1}^k sum_{ mathbf{x}_i in C_j}|| mathbf{x}_i - boldsymbol{ mu}_j||^2$$ . This quantity is also called the inertia [3]. Large levels of inertia imply low cohesion. Directly, minimizing the loss function $L$ i.e. find a global minimum has exponential complexity [3]. The the k-means algorithm employs the iterative approach described above to avoid such complexity. . The algorithm can be seen as a search heuristic converging to a (local) optimum [1]. Convergence may be checked by comparing whether the computed cluster centroids have changed or if the classification of the points has not changed which of course implies the former. However, this may not be enough to guarantee convergence, so usually the algorithm is run with a maximum number of iterations. . Details . Now that we have an overview of what the k-means does, let&#39;s dive deeper into the implementation details. To start with, the algorithm expects as an input the following: . The number of clusters $k$ to partitioned the dataset | The number of maximum iterations to perform | A tolerance that indicates whether two points are the same or not in the distance metric | . As mentioned in the previous section, the first step is the initialization of the centroids $ boldsymbol{ mu}_j$. . Initialization . There are several alternatives we could use, e.g. randomly drawing each coordinate from the domain’s boundaries, or using actual points from the dataset [1]. However, a solution that has several advantages is randomly perturbing $k$ points casually drawn from the dataset [1]. This has the following advantages [1]: . If each coordinate of a centroid was generated completely at random, we would have to first scan the dataset to find the acceptable range for each coordinate. We avoid this by randomly perturbing $k$ points drawn from the dataset | By uniformly drawing points from the dataset, the centroids will be close to points in the data and centroids will be drawn with higher probability in areas with higher density of points. | Randomly perturbing the points, reduces the risk that all the generated points will be concentrated in the denser areas. | . Classification . The next step is to start the iterations and perform classification and recentering. Classification is performed by looping over over the dataset and compute the distance between $ mathbf{x}_i$ and the current $ boldsymbol{ mu}_j$. The data point $ mathbf{x}_i$ is assigned to cluster $C_j$ based on the following criterion . $$C_j = argmin_j d(, mathbf{x}_i, boldsymbol{ mu}_j) $$ . i.e. the data point is assigned to the cluster that has the smallest distance from the cluster centroid. . Recentering . Once all points have been clustered, the new centroids are computed according to . $$ boldsymbol{ mu}_j = frac{1}{N_j} sum_{ mathbf{x}_i in C_j} mathbf{x}_i, forall j $$ . $$$$ . that is the new $ boldsymbol{ mu}_j$ is the center of mass of all the points that belong in the cluster. . Issues . When the clusters have approximately a spherical shape and are linearly separable the algorithm does a pretty good job. This is shown in the figure below. Note that the algorithm is able to correctly identify clusters with different density. . . Remark . You may want to go over the following example Demonstration of k-means assumptions. This example is meant to illustrate situations where k-means will produce unintuitive and possibly unexpected clusters. . . Figure 1. Clustering produced by k-means. Image from [1]. . However, the algorithm has several weaknesses which we discuss bellow. . The first weakness of the algorithm is that it cannot detect outliers [1]. In fact, outlier points are added to the closest clusters. Recall that in general the mean is sensitive to outliers. Since centroids are computed as the centers of the mass of the clusters, the centroids of the clusters will be pulled by outliers away from the best position they could hold. Therefore, we should remove outliers from the data set before performing k-means clustering. . The second point to notice, is that the algorithm can only produce/distinguish spherical clusters [1]. However, in real data sets not all clusters are spherical. . Furthermore, if the clusters in the data set are not linearly separable, then it is difficult to approximate them with spherical clusters. As a result k-means cannot separate non-convex clusters e.g. clusters shaped as two concentric rings [1]. Thus, in these situations, the algorithm will have some hard time in order to find good solutions. . The fourth point to remember is that the algorithm cannot automatically find the number of clusters present in the data set. Instead it expects this as an input. This means that unless we have some insight deriving from domain knowledge that indicates the number of clusters, we will need to run the algorithm multiple times, trying different values for the number of centroids, and comparing the results using some kind of metric. . In fact, one way to determine the number of clusters using the so-called elbow method. In this method, we select the optimal number of clusters by fitting the model with a range of values for $k$ . If the line chart resembles an arm, then the “elbow” (the point of inflection on the curve) is a good indication that the underlying model fits best at that point [4]. . Python example . Let&#39;s explore the scikit-learn API to perform some clustering using K-means on the well-known Iris dataset. Arguably, this will not be something exciting. The example is taken from K-Means Elbow Method code for Python. . import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.cluster import KMeans from sklearn import datasets . iris = datasets.load_iris() df=pd.DataFrame(iris[&#39;data&#39;]) . kmeanModel = KMeans(n_clusters=4) kmeanModel.fit(df) . KMeans(n_clusters=4) . df[&#39;k_means&#39;]=kmeanModel.predict(df) df[&#39;target&#39;]=iris[&#39;target&#39;] fig, axes = plt.subplots(1, 2, figsize=(16,8)) axes[0].scatter(df[0], df[1], c=df[&#39;target&#39;]) axes[1].scatter(df[0], df[1], c=df[&#39;k_means&#39;], cmap=plt.cm.Set1) axes[0].set_title(&#39;Actual&#39;, fontsize=18) axes[1].set_title(&#39;K_Means&#39;, fontsize=18) plt.show() . Now that we are at it, let&#39;s also use the elbow method to determine the right number of clusters. . distortions = [] K = range(1,10) for k in K: kmeanModel = KMeans(n_clusters=k) kmeanModel.fit(df) distortions.append(kmeanModel.inertia_) . plt.figure(figsize=(16,8)) plt.plot(K, distortions, &#39;bx-&#39;) plt.xlabel(&#39;k&#39;) plt.ylabel(&#39;Distortion&#39;) plt.title(&#39;The Elbow Method showing the optimal k&#39;) plt.show() . We can see that the right number of clusters to use is $k=3$. . Summary . In this section we reviewed the k-means algorithm. K-means is a centroid-based hard-clustering method. It is a good option only for low-to-medium-dimensional (with at most around 20 dimensions) datasets where we know that clusters can be accurately approximated with hyperspheres i.e. cluster shapes are spherical and the number of clusters can be estimated a priori [1]. It also works well even if the dataset does not have a homogeneous distribution. On the other hand, the algorithm works poorly on high-dimensional data, and when clusters cannot be approximated with hyperspheres [1]. . We have not touched upon evaluation metrics in order to assess the performance of the algorithm. We will do so in a future post. . References . Marcello La Rocca, Advanced algorithms and data structures, Manning Publications. | k-means clustering | Giuseppe Bonaccorso, Mastering machine learning algorithms, Packt Publications. | Elbow method. |",
            "url": "https://pockerman.github.io/qubit_opus/unsupervised-learning/clustering/k-means/elbow-method/algorithms/2022/04/23/kmeans-clustering.html",
            "relUrl": "/unsupervised-learning/clustering/k-means/elbow-method/algorithms/2022/04/23/kmeans-clustering.html",
            "date": " • Apr 23, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "K-d trees  Part 1",
            "content": "Overview . In this post we take a look into searching in multidimensional data sets. Specifically, we consider k-d trees to facilitate searching in multidimensional data sets and the k-nearest neighbors algorithm. . In this post, we won&#39;t implement a k-d tree. We rather keep things simple and just illustrate the concept. A C++ implementation can be found here. Furthermore, a discussion of this implementation can be found in K-d trees Part 2. . K-d trees Part 1 . The k-nearest neighbor algorithm is one of the simplest algorithms performing both classification and regression modeling on data. Indeed, given a data set $D$ and a set of labels, the k-nearest neighbor algorithm does not explicitly train any particular mathematical model on $D$. Instead, it stores $D$ and every time a query comes in about the class of a point, $P$, the algorithm goes over the points in $D$ computes the $k$ nearest points to $P$ and uses a kind of majority vote in order to determine the class that $P$ should belong to. This is incredibly simple but this means that we need to compare $P$ with every point in $D$. Assuming that the metric we use is $O(1)$ time, the total time complexity is $O(|D|)$. However, this is just an ideal scenario. For a tuple with $N$ elements computing a distance requires $N$ operations. Hence, for one query point the complexity is $O(N|D|)$ and for $|D|$ query points we get quadratic behavior $O(N|D|^2)$ [4]. . So how can we improve this time complexity? The problem we face is that of searching in multidimensional data for the nearest neighbor(s) of a generic point $P$ that possibly is not in the dataset itself. K-d trees pose a way to solve this problem [3]. . Before getting into the details, recall that a balanced binary search tree with $|D|$ nodes has complexity $O(log(|D|)$ and this is the motivation behind k-d trees. The only problem seems to be how to do the splitting. . K-d trees details . A k-d tree or a k-dimensional tree is a space partitioning data structure for organizing points in a k-dimensional space [1]. Specifically, a k-d tree is a binary search tree where every node in the tree represents a k-dimensional point [3]. We will assume that the coordinates of k-dimensional vector can be compared with each other. A non-leaf node in a k-d tree, implicitly generates a splitting hyperplane that divides the space into two parts, known as half-spaces [1]. Then all points to the left of the splitting hyperplane are represented by the left subtree of that node and points to the right of the hyperplane are represented by the right subtree [1]. The hyperplane direction is chosen by associating every node in the tree with one of the $k$ dimensions. K-d trees were proposed by Jon Louis Bentley in [5]. . The above may sound a bit technical and difficult to grasp. So let&#39;s go over a simple example. In order to be able to visualize what is happening we will confine ourselves to 2D space i.e. $k=2$. . K-d tree simple example . The example we will use is taken from [2] as well as all the images used below. Let&#39;s consider the following points as shown in the figure below. . Figure 1. Data points in a Cartesian 2D space. Image from [3]. . The specific coordinates of the points is immaterial here. We want to construct a k-d tree out of these points. There are many different ways to construct k-d trees [1]. This follows from the fact that there are many possible ways we could choose in order to create axis-aligned splitting planes. . Nevertheless, the canonical method of of k-d tree construction is as follows. As we move down the tree, we cycle through the axes used to select the splitting planes. For our example, we could have the root node to have an $y-$aligned plane, root&#39;s children would both have $x-$aligned planes, the the root&#39;s grandchildren would all have again $y-$aligned planes an so on. This process is illustrated in the figure above we use point $R$ and its $x-$ coordinate to determine the first split and the two images below. . Figure 2. Splitting a Cartesian 2D space along $x$ and $y$ coordinates. Image from [3]. . In figure 1 we choose to split along the $x-$coordinate of point $R$. In figure 2, we continue by choosing point $W$ and its $y-coordinate$. Whilst in figure 3, we choose point $P$ and its $x-$coordinate. . Figure 3. Splitting a Cartesian 2D space along $x$ and $y$ coordinates. Image from [3]. . The second point of concern is how to choose the points to insert. Again in the canonical method, points are inserted by selecting the median of the points being put into the subtree, with respect to their coordinates in the axis being used to create the splitting plane. Of course the latter assumes that we know all points in advance. This method leads to a balanced k-d tree, in which each leaf node is approximately the same distance from the root [1]. Note. however, that it is not required to select the median point; in this case however, there is no guarantee that the tree will be balanced [1]. . In order to avoid coding a complex $O(n)$ median-finding algorithm or using an $O(n log n)$ sort e.g. heap-sort or merge-sort to sort all $n$ points, a popular practice is to sort a fixed number of randomly selected points, and use the median of those points to serve as the splitting plane. In practice, this technique often results in balanced trees [1]. . We now know conceptually how k-d trees work, in a following post, we will implement a simple k-d tree using c++. let&#39;s turn our attention to k-nearest search. . K-nearest search is the dominant operation behind the k-nearest neighbor algorithm. K-d trees can be used in order to avoid a brute-force search over the entire data set $D$. This would be similar to search an element in an unsorted array. k-d trees provide structural information we can utilize in order to improve the search. . However, $k-$nearest search is a bit involved so we won&#39;t discuss it in this post. You can however look it up in [3]. . Summary . In this post we have a brief overview of k-d trees. These are binary search trees that facilitate searching in multidimensional data sets. In general k-d trees work well for low-to-medium-dimensional spaces but behaves poorly in high-dimensional spaces. K-d trees can also be used to boost the k-means clustering algorithm. An alternative to k-d trees is provided by ball-trees but we will look into that in another post. . References . k-d tree. | k-nearest neighbors algorithm. | Marcello La Rocca, Advanced algorithms and data structures, Manning Publications. | Giuseppe Bonaccorso, Mastering machine learning algorithms, Packt Publications. | Jon Louis Bentley Multidimensional binary search trees used for associative searching. Communications of the ACM, 1975, Vol. 18, Issue 9, pp. 509-517. |",
            "url": "https://pockerman.github.io/qubit_opus/multidimensional-search/kd-trees/k-nearest-search/algorithms/2022/03/23/kd-trees-p1.html",
            "relUrl": "/multidimensional-search/kd-trees/k-nearest-search/algorithms/2022/03/23/kd-trees-p1.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Docker. Introduction to volumes",
            "content": "Overview . In this post, we will review how to instruct Docker to persist data. You can find the official documentation at https://docs.docker.com/storage/volumes/. . Introduction to volumes . When deleting a Docker container, all the data is lost. This is true even if the container is running a data base server like MySQL. Certainly, this is not always desirable. We need therefore to instruct Docker to persist the data for us. . The way to do so, is to instruct Docker to persist the data outside the, possibly, ephemeral container. Let&#39;s see how we can do this. . One way is to navigate to /var/lib/docker/volumes/ (you may need root privileges to do so) and execute . docker volume create your_volume_name . Let&#39;s run a container from image image_name that maps to this volume . docker run -itd -v your_volume_name:/www image_name . We can also specify the volume locations to be used in our docker-compose file using the volumes key. . Bind volumes . If we follow the procedure above, the data persistence will be in the /var/lib/docker/volumes/ directory on the Docker host. This is not what we always want. Docker allows us to specify the mount point of the volume we want to use. . docker run -itd -v your_dedicated_mount_point_name:/www image_name . We can also use the following command . docker run -itd --mount type=bind, source=your_dedicated_mount_point_name, target=/www image_name . References . Use volumes |",
            "url": "https://pockerman.github.io/qubit_opus/dev-ops/data-engineering/containers/docker/docker-volumes/2021/11/22/docker-intro-volumes.html",
            "relUrl": "/dev-ops/data-engineering/containers/docker/docker-volumes/2021/11/22/docker-intro-volumes.html",
            "date": " • Nov 22, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "AWS. S3 Buckets",
            "content": "Overview . In this notebook, we are going to have a brief view on AWS S3 storage. Concretely, we are going to discuss the following: . How to create an AWS S3 Bucket | How to upload and download items | How to do multi-part file transfer | How to generate pre-signed URLS. | How to set up bucket policies | . Moreover, we will work with AWS S3 buckets using the Boto3 Python package. . S3 Buckets . AWS S3 is an object storage system. S3 stands for Simple Storage Service. By design, S3 has 11 9&#39;s of durability and stores data for millions of applications. S3 files are referred to as objects. You can find more information about S3 here and here. . We will use the Boto3 Python package to interact with AWS S3; that is to create a bucket, upload and download files in the created bucket. . import logging import boto3 from botocore.exceptions import ClientError . Create S3 Bucket . AWS_ACCESS_KEY_ID = &#39;Use your own credentials&#39; AWS_SECRET_ACCESS_KEY = &#39;Use your own credentials&#39; . s3_client = boto3.client(&#39;s3&#39;, region_name=&#39;us-west-2&#39;, aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY) . location = {&#39;LocationConstraint&#39;: &#39;us-west-2&#39;} s3_client.create_bucket(Bucket=&#39;coursera-s3-bucket&#39;, CreateBucketConfiguration=location) . The response of the function all above is shown below: . {&#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;355VX5QNYSQBTSCM&#39;, &#39;HostId&#39;: &#39;7jXN853VP175Fw/il1Zvx8UXkfRsdQRXH3VrAFOcCYZl4y2ZTF6zNPp6tXvwnpBGlmAKTCP9RFA=&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;x-amz-id-2&#39;: &#39;7jXN853VP175Fw/il1Zvx8UXkfRsdQRXH3VrAFOcCYZl4y2ZTF6zNPp6tXvwnpBGlmAKTCP9RFA=&#39;, &#39;x-amz-request-id&#39;: &#39;355VX5QNYSQBTSCM&#39;, &#39;date&#39;: &#39;Thu, 11 Nov 2021 11:01:14 GMT&#39;, &#39;location&#39;: &#39;http://coursera-s3-bucket.s3.amazonaws.com/&#39;, &#39;server&#39;: &#39;AmazonS3&#39;, &#39;content-length&#39;: &#39;0&#39;}, &#39;RetryAttempts&#39;: 0}, &#39;Location&#39;: &#39;http://coursera-s3-bucket.s3.amazonaws.com/&#39;} . Upload and object to a bucket . Bucket policies . Retrieve the policies attached to a bucket . result = 3_client.get_bucket_policy(Bucket=&#39;bucket-name&#39;) . The call above fails because by default there are no policies set. A bucket&#39;s policy can be set by calling the put_bucket_policy method. Moreover, a policy is defined in the same JSON format as an IAM policy. . The Sid (statement ID) is an optional identifier that you provide for the policy statement. You can assign a Sid value to each statement in a statement array. . The Effect element is required and specifies whether the statement results in an allow or an explicit deny. Valid values for Effect are Allow and Deny. . By default, access to resources is denied. . Use the Principal element in a policy to specify the principal that is allowed or denied access to a resource. . You can specify any of the following principals in a policy: . AWS account and root user | IAM users | Federated users (using web identity or SAML federation) | IAM roles | Assumed-role sessions | AWS services | Anonymous users | . The Action element describes the specific action or actions that will be allowed or denied. . We specify a value using a service namespace as an action prefix (iam, ec2, sqs, sns, s3, etc.) followed by the name of the action to allow or deny. . The Resource element specifies the object or objects that the statement covers. We specify a resource using an ARN. Amazon Resource Names (ARNs) uniquely identify AWS resources. . CORS Configuration . response = s3_client.get_bucket_cors(Bucket=bucket_name) print(response[&#39;CORSRules&#39;]) . cors_configuration = { &#39;CORSRules&#39;:[{&#39;AllowHeaders&#39;:[&#39;Authorization&#39;], &#39;AllowedMethods&#39;:[&#39;GET&#39;, &#39;PUT&#39;], &#39;AllowedOrigins&#39;:[&#39;*&#39;], &#39;ExposeHeaders&#39;:[&#39;GET&#39;, &#39;PUT&#39;], &#39;MaxAgeSeconds&#39;:3000} ] } response = s3_client.put_bucket_cors(Bucket=bucket_name, CORSConfiguration=cors_configuration) . References . AWS S3 | What is Amazon S3? |",
            "url": "https://pockerman.github.io/qubit_opus/aws/s3-buckets/cloud-computing/data-storage/data-engineering/boto3/2021/11/11/aws-s3-buckets.html",
            "relUrl": "/aws/s3-buckets/cloud-computing/data-storage/data-engineering/boto3/2021/11/11/aws-s3-buckets.html",
            "date": " • Nov 11, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Apache Spark. Explore the Column API",
            "content": "Overview . In this notebook, we will explore the Column type API. The source code can be found here. . Explore the Column API . Named columns in a DataFrame are similar to named columns in pandas data frames; they describe a type of field [1]. This is also similar to an RDBMS table. In Spark, a column is an object represented by the Column type [1]. Let&#39;s explore what we can do with columns in Spark. The examples are taken from [1]. The following Scala snippet shows some of the Column type API functions in action. . package train.spark /* Explore the DataFrame Column API */ import org.apache.spark.sql.SparkSession import org.apache.spark.sql.types._ import org.apache.spark.sql.functions._ object ExploreDataFrameColumnAPI { def main(args: Array[String]) { val csvFile = &quot;/home/alex/qi3/learn_scala/scripts/spark/data/train.csv&quot; val appName: String = &quot;Spark DataFrame API Demo&quot; val spark = SparkSession .builder() .appName(appName) .getOrCreate() // specify the schema val customSchema = StructType(Array( StructField(&quot;mu-1&quot;, DoubleType, false), StructField(&quot;mu-2&quot;, DoubleType, false), StructField(&quot;label&quot;, IntegerType, false))) // read the data frame val df = spark.read.schema(customSchema).csv(csvFile) // print the schema df.printSchema() // get the columns df.columns // access a particular column by using col // it returns a Column type val colId = df.col(&quot;Id&quot;) // we can use expressions on columns df.select(expr(&quot;Hits * 2&quot;)).show(2) // compute a value df.select(col(&quot;Hits&quot;) * 2).show(2) // add a new column in the data frame df.withColumn(&quot;Big Hitters&quot;, (expr(&quot;Hits &gt; 10000&quot;))).show() // concatenate columns and create a new column df.withColumn(&quot;AuthorsId&quot;, (concat(expr(&quot;First&quot;), expr(&quot;Last&quot;), expr(&quot;Id&quot;)))) .select(col(&quot;AuthorsId&quot;)) .show(4) } } . References . Jules S. Damji, Brooke Wenig, Tathagata Das, Denny Lee, Learning Spark. Lighting-fast data analytics, O&#39;Reilly, 2nd Edition. |",
            "url": "https://pockerman.github.io/qubit_opus/spark-dataframe/spark/scala/big-data/data-engineering/data-analysis/2021/11/05/dataframe-column-api-spark-scala.html",
            "relUrl": "/spark-dataframe/spark/scala/big-data/data-engineering/data-analysis/2021/11/05/dataframe-column-api-spark-scala.html",
            "date": " • Nov 5, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Data Engineering. Data anonymity with k-anonymity",
            "content": "Overview . In this post, we take a look at the k-anonymity algorithm. We will also look at how to use ARX for anonymizing a dataset. . Data anonymity with k-anonymity . Let&#39;s assume we have at our disposal a dataset $D$. We would like to be able to modify $D$ such that sensitive information about individuals within $D$ is not leaked. Let&#39;s denote the modified dataset $D$ with $ hat{D}$. Modification of $D$ cannot be done recklessly because $ hat{D}$ will not have any value in terms of performing analytics on it. Thus, anonymization, is about striking a balance between privacy and utility of the resulting anonymized set. . With $k-$anonymity a dataset $D$ is transformed so that it is difficult for an intruder to determine the identity of the individuals in $D$ [1]. When a dataset is anonymized using $k-$anonymity, it has the property that each record is similar to at least $k-1$ other records on the potentially identifying variables. . Common implementations of the algorithm, use various transformation techniques such as [1]: . Generalization | Global recoding | Suppression | . Any record in a $k-$anonymized $D$ has a maximum probability $1/k$ of being re-identified [1]. . The algorithm was developed to protect against two types of attacks [1]: . Re-identification of a specific individual | Re-identification of an arbitrary individual | . In the first type of attack, the intruder would know that a particular individual exists in $ hat{D}$ and wants to discover the record that belongs to that individual. In the second type of attack, the intruder is not interested in a specific individual but rather is interested in that re-identification per se can be done [1]. . In most cases, the algorithm is capable of preventing identity disclosure i.e. a record in a $k-$anonymized $D$ cannot be connected to the corresponding record in the non-anonymized dataset. However, it may fail to protect against attribute disclosure [2]. Approaches such as l-diversity and t-closeness have been proposed to overcome the limitations of $k-$anonymity. . package example_2 import org.deidentifier.arx.ARXPopulationModel.Region import java.io.File import java.nio.charset.Charset import java.text.DecimalFormat import collection.JavaConverters.* import collection.mutable.ArrayBuffer import org.deidentifier.arx.{ARXAnonymizer, ARXConfiguration, ARXPopulationModel, ARXResult, AttributeType, Data, DataHandle, DataType} import org.deidentifier.arx.criteria.KAnonymity import org.deidentifier.arx.criteria.EqualDistanceTCloseness import org.deidentifier.arx.criteria.HierarchicalDistanceTCloseness import org.deidentifier.arx.criteria.DistinctLDiversity import org.deidentifier.arx.Data import org.deidentifier.arx.Data.DefaultData import org.deidentifier.arx.AttributeType.Hierarchy import org.deidentifier.arx.AttributeType.Hierarchy.DefaultHierarchy import postprocessor.ResultPrinter.{printResult, printHandle} /** * Example1: Load data to ARX */ object KAnonymityARX { def createData: Data = { // Define data val data = Data.create data.add(&quot;age&quot;, &quot;gender&quot;, &quot;zipcode&quot;) data.add(&quot;34&quot;, &quot;male&quot;, &quot;81667&quot;) data.add(&quot;45&quot;, &quot;female&quot;, &quot;81675&quot;) data.add(&quot;66&quot;, &quot;male&quot;, &quot;81925&quot;) data.add(&quot;70&quot;, &quot;female&quot;, &quot;81931&quot;) data.add(&quot;34&quot;, &quot;female&quot;, &quot;81931&quot;) data.add(&quot;70&quot;, &quot;male&quot;, &quot;81931&quot;) data.add(&quot;45&quot;, &quot;male&quot;, &quot;81931&quot;) data } def main(args: Array[String]): Unit ={ System.out.println(&quot;Running example 2...&quot;) val data = createData // check the columns val nCols = data.getHandle.getNumColumns println(s&quot;Number of columns ${nCols}&quot;) val nRows = data.getHandle.getNumRows println(s&quot;Number of rows ${nRows}&quot;) // define hierarchies val age = Hierarchy.create age.add(&quot;34&quot;, &quot;&lt;50&quot;, &quot;*&quot;) age.add(&quot;45&quot;, &quot;&lt;50&quot;, &quot;*&quot;) age.add(&quot;66&quot;, &quot;&gt;=50&quot;, &quot;*&quot;) age.add(&quot;70&quot;, &quot;&gt;=50&quot;, &quot;*&quot;) val gender = Hierarchy.create gender.add(&quot;male&quot;, &quot;*&quot;) gender.add(&quot;female&quot;, &quot;*&quot;) // Only excerpts for readability val zipcode = Hierarchy.create zipcode.add(&quot;81667&quot;, &quot;8166*&quot;, &quot;816**&quot;, &quot;81***&quot;, &quot;8****&quot;, &quot;*****&quot;) zipcode.add(&quot;81675&quot;, &quot;8167*&quot;, &quot;816**&quot;, &quot;81***&quot;, &quot;8****&quot;, &quot;*****&quot;) zipcode.add(&quot;81925&quot;, &quot;8192*&quot;, &quot;819**&quot;, &quot;81***&quot;, &quot;8****&quot;, &quot;*****&quot;) zipcode.add(&quot;81931&quot;, &quot;8193*&quot;, &quot;819**&quot;, &quot;81***&quot;, &quot;8****&quot;, &quot;*****&quot;) data.getDefinition.setAttributeType(&quot;age&quot;, age) data.getDefinition.setAttributeType(&quot;gender&quot;, gender) data.getDefinition.setAttributeType(&quot;zipcode&quot;, zipcode) System.out.println(&quot;Number of sensitive variables=&quot; + data.getHandle.getDefinition.getSensitiveAttributes.size) // Create an instance of the anonymizer val anonymizer = new ARXAnonymizer val config = ARXConfiguration.create config.addPrivacyModel(new KAnonymity(3)) config.setSuppressionLimit(0d) val result = anonymizer.anonymize(data, config) // Print info printResult(result, data) // Process results System.out.println(&quot; - Transformed data:&quot;) printHandle(handle = result.getOutput(false)) System.out.println(&quot;Done!&quot;) } } . References . Khaled El, Fiad Kamal Dankar, Protecting privacy using k-anonymity, Journal of American Medical Informatics Association, vol 15, pp. 627-637, 2008. | Ismini Psychoula et al, A deep learning approach for privacy preservation in assisted living. |",
            "url": "https://pockerman.github.io/qubit_opus/k-anonymity/anonymization/big-data/data-engineering/2021/11/03/data-eng-k-anonymity.html",
            "relUrl": "/k-anonymity/anonymization/big-data/data-engineering/2021/11/03/data-eng-k-anonymity.html",
            "date": " • Nov 3, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Reinforcement Learning. Deep Q-networks",
            "content": "Overview . In this post, we will introduce Deep Q-networks (DQN). . Deep Q-networks . Online Q-learning can experience instabilities during training. This is because by using experience sampled sequentially from the environment leads to highly correlated gradient steps. Deep Q-networks (DQN) made deep reinforcement learning a viable approach to complex sequential control problems. In this section, we introduce the vanilla DQN algorithm. . Deep q-networks were introduced in the seminal work of Mnih et al. in [2]. They demonstrated that a single DQN can achive human level performance in many Atari games without any feature engineerign. A DQN modifies online Q-learing in two ways [1, 2] . Introducing experience replay | Introducing target network | . Both these two features greatly stabilize the learning. In particular, a DQN stores the experience tuples $(s_i, alpha_i, r_i, s_{i, NEW})$ in an experience buffer. During training, the samples are drawn from the buffer uniformly. This approach eliminates the correlations between the samples used in training the neural network and gives i.i.d. samples. . The second feature that a DQN introduces is the target network. When bootstraping with function approximations, in a sense we create a moving target to learn from. Attempting to train a neural network via such a route is more likely bound to fail. The key idea is to create a copy of the neural network that is only used to generate the Q-value estimates used in sampled Bellman updates. That is the target value for sample $i$ is obtained as . $$y_i = r_i + gamma max_{ alpha_i} Q_{ theta_{TN}}(s_i, alpha_i)$$ . Note that in the update rule above, $ theta_{TN}$ denotes the parameters of a target network. These are updated every $C$ steps by setting them equal to the parameters $ theta$ i.e. $ theta_{TN} = theta$. Such an update rule, creates a lag in updating the target network which may make the action-value estimates that it generates a bit stale compared to the original network. What we gain, however, is that the target values become stable and the original network is trainable. . Loss function . In the DQN algorithm, we, typically, use the following loss function . $$L = begin{cases} left( Q(s, alpha) - (r + gamma max_{ alpha in mathbb{A}} hat{Q}(s_{NEW}, alpha) right)^2, text{if step is not at the end of the episode} left( Q(s, alpha) - r right)^2, text{otherwise} end{cases}$$ . The following section summarizes the DQN algorithm. . DQN algorithm . Let&#39;s now walk over the steps of the DQN algorithm as these are described in [1]. These are: . Initialize $ theta$ and the replay buffer with a fixed capacity. Set $ theta_{TN}= theta$ | Set the policy $ pi$ to be an $ epsilon-$greedy with respect to $q_{ theta}$ | Until some condition is met do . 3.1 Sample an action $ alpha$ from the policy $ pi$ . 3.2 Take the action $ alpha$ and observe $r$ and $s_{NEW}$. Add the transition $(s, alpha, r, s_{ NEW})$ in the replay buffer. If $|D|&gt;M$ eject the oldest transition from the buffer . 3.3 If the experience buffer has reached the indicated capacity, unfiromly sample a random minibatch of $N$ transitions from $D$ else return to 3.1 above. . 3.4 Obtain the target values $y_i = r_i + gamma max_{ alpha_i} q_{ theta_{TN}}(s_i, alpha_i)$. . 3.5 Take the gradient step to update $ theta$ . 3.6 Every $C$ steps update the target network parameters . | References . Enes Bilgin, Mastering Reinforcement Learning with Python. Build next-generation, self-learning models using reinforcement learning techniques and best practices. | Mnih V. et al. Human level control through deep reinforcement learning, Nature, v. 518, pp. 529-533, 2015 | Maxim Lapan, Deep Reinforcement Learning Hands-on, Packt |",
            "url": "https://pockerman.github.io/qubit_opus/reinforcement-learning/deep-reinforcement-learning/dqn/python/pytorch/2021/10/18/rl-dqn.html",
            "relUrl": "/reinforcement-learning/deep-reinforcement-learning/dqn/python/pytorch/2021/10/18/rl-dqn.html",
            "date": " • Oct 18, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "C++ Programming. Using the auto keyword",
            "content": "Overview . The auto keyword changed its semantics starting from the C++11 standard. In this notebook, we will review its new semantics and examine its new flavors. . Using the auto keyword . The auto keyword changed its semantics starting from the C++11 standard. From C++11 onwards, the semantic of the keyword is automatic type deduction. In fact, from C++11 onwards, we have the following different flavors of the keyword [1]: . auto | const auto&amp; | auto&amp; | auto&amp;&amp; | . Furthermore, we have decltype(auto). This post is a short guide on how to use auto and its various flavors. . Automatic type deduction . The first thing to note about auto is that it ise used for automatic type deduction. This means that we can write code like the following: . ... // before c++11 int x = 5; //from c++11 auto x = 5; ... . Thus, using auto may help us to write cleaner and less cluttered code. This is emphasized particularly when we consider function signatures. Compare the code snippet below (example taken from [1]): . class Foo { int value()const{..} const int&amp; cref_value()const{...} int&amp; ref_value(){...} }; . with the following code snippet . class Foo { auto value()const{..} auto&amp; cref_value()const{...} auto&amp; ref_value(){...} }; . The latter API is obviously cleaner and simpler.The type returned is deduced by the compiler for us. . . Remark . Althgough to a large extent using auto simplifies our code, overusing it can have the opposite result. . . One other advantage of using auto is that we cannot leave the variable uninitialized. That is the following fails to compile. . ... auto x; ... . This is reasonable as the compiler uses the right hand side value to deduce the type and hence the memory size it has to allocate. If there isn&#39;t a value there is nothing to deduce from. Thus, uninitialized variables are not allowed when using auto and the good news are that the compilers let us know the exact line number in our code that this occurs. . const reference . const auto&amp; has the ability to bind to anything [1]. The original object cannot be mutated via such a reference. Note that if the const reference is bound to a temporary object, the lifetime of the temporary will be extended to the lifetime of the reference [1]. . Although we may be using auto&amp; , it is possible that we end up with a const reference. For example . auto foo = Foo{}; auto&amp; cref = cref_value(); . We should however strive to be more explicit and for such cases simply use const auto&amp; and use auto&amp; to only denote mutable references [1]. . Forwarding reference . Similar to auto&amp;, auto&amp;&amp; can bind to anything. auto&amp;&amp; is called a forwardin or universal reference [1]. Similar to const auto&amp;, auto&amp;&amp; extends the lifetime of a temporary. However, in contrast to const auto&amp;, auto&amp;&amp; allows us to mutate the objects it references including the temporaries. . . Remark . Note that auto&amp;&amp; and T&amp;&amp; are interpreted as forwarding references only when used in a function template where T is a template parameter of that function. However, using &amp;&amp; with an explicit type e.g. std::vector&lt;double&gt;&amp;&amp; denotes an rvalue reference and does not have the properties of a forwarding reference [1]. . . References . Bjorn Andrist, Viktor Sehr, C++ High Performance, 2nd Edition, Packt Publishing |",
            "url": "https://pockerman.github.io/qubit_opus/programming/c++/2021/10/17/cpp-program-using-auto.html",
            "relUrl": "/programming/c++/2021/10/17/cpp-program-using-auto.html",
            "date": " • Oct 17, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "PyTorch with C++. Linear regression model",
            "content": "Overview . Linear regression model . #include &lt;torch/torch.h&gt; #include &lt;iostream&gt; #include &lt;vector&gt; namespace example { typedef std::size_t uint_t; class Net: public torch::nn::Module { public: // Net(uint_t input_size, uint_t output_size); // forward torch::Tensor forward(torch::Tensor input); private: torch::nn::Linear linear; torch::Tensor bias_; }; Net::Net(uint_t input_size, uint_t output_size) : linear(register_module(&quot;linear&quot;, torch::nn::Linear(input_size, output_size))), bias_() { bias_ = register_parameter(&quot;b&quot;, torch::randn(output_size)); } torch::Tensor Net::forward(torch::Tensor input){ return linear(input) + bias_; } } int main() { using namespace example; if(torch::cuda::is_available()){ std::cout&lt;&lt;&quot;CUDA is available on this machine&quot;&lt;&lt;std::endl; } else{ std::cout&lt;&lt;&quot;CUDA is not available on this machine&quot;&lt;&lt;std::endl; } // create data std::vector&lt;double&gt; x_train(11, 0.0); std::vector&lt;double&gt; y_train(11, 0.0); for(uint_t i=0; i&lt;x_train.size(); ++i){ x_train[i] = static_cast&lt;double&gt;(i); y_train[i] = 2*static_cast&lt;double&gt;(i) + 1; } auto x_tensor = torch::from_blob(x_train.data(), {int(y_train.size()), int(x_train.size()/y_train.size())}); auto y_tensor = torch::from_blob(y_train.data(), {int(y_train.size()), 1}); Net net(1, 1); for (const auto&amp; p : net.parameters()) { std::cout &lt;&lt; p &lt;&lt; std::endl; } torch::nn::MSELoss mse; torch::optim::SGD sgd(net.parameters(), 0.01); for(uint_t e=0; e&lt;100; ++e){ sgd.zero_grad(); auto outputs = net.forward(x_tensor); auto loss = mse(outputs, y_tensor); // get gradients w.r.t to parameters loss.backward(); // update parameters sgd.step(); std::cout&lt;&lt;&quot;Epoch=&quot;&lt;&lt;e&lt;&lt;&quot; loss=&quot;&lt;&lt;loss&lt;&lt;std::endl; } return 0; } . CUDA is not available on this machine 0.01 * 1.7510 [ CPUFloatType{1} ] -0.4187 [ CPUFloatType{1,1} ] 0.01 * -1.6082 [ CPUFloatType{1} ] Epoch=0 loss=4.09091 [ CPUFloatType{} ] Epoch=1 loss=3.72361 [ CPUFloatType{} ] Epoch=2 loss=3.39557 [ CPUFloatType{} ] Epoch=3 loss=3.10247 [ CPUFloatType{} ] Epoch=4 loss=2.84047 [ CPUFloatType{} ] Epoch=5 loss=2.60615 [ CPUFloatType{} ] Epoch=6 loss=2.39647 [ CPUFloatType{} ] Epoch=7 loss=2.20874 [ CPUFloatType{} ] Epoch=8 loss=2.04054 [ CPUFloatType{} ] Epoch=9 loss=1.88976 [ CPUFloatType{} ] Epoch=10 loss=1.75448 [ CPUFloatType{} ] Epoch=11 loss=1.63302 [ CPUFloatType{} ] Epoch=12 loss=1.52387 [ CPUFloatType{} ] Epoch=13 loss=1.42571 [ CPUFloatType{} ] ... . References .",
            "url": "https://pockerman.github.io/qubit_opus/pytorch/c++/api/machine-learning/linear-regression/2021/10/15/pytorch_cpp_linear_regression.html",
            "relUrl": "/pytorch/c++/api/machine-learning/linear-regression/2021/10/15/pytorch_cpp_linear_regression.html",
            "date": " • Oct 15, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Apache Spark. Create a  DataFrame with Scala",
            "content": "Overview . In this post, we will review how to create a DataFrame in Spark using the Scala API. . Create a DataFrame with Scala . The Spark DataFrame is inspired by the equivalent pandas DataFrame. A DataFrame is Spark is like a distributed in-memory table with named columns and schemas [1]. Similar to an RDD, a DataFrame is also immutable and Spark keeps a lineage of all the transformations. Thus, when we add or change the names and types of the columns, a new DataFrame is actually created. . package train.spark import org.apache.spark.sql.SparkSession object CreateDataFrame { def main(args: Array[String]) { val spark = SparkSession .builder() .appName(&quot;Spark DataFrame Demo&quot;) .getOrCreate() val csvFile = &quot;/home/alex/qi3/learn_scala/scripts/spark/data/train.csv&quot; val df = spark.read.csv(csvFile) // print the schema df.printSchema() } } . The output is . 21/10/15 15:22:10 WARN Utils: Your hostname, LT-2R0620-101 resolves to a loopback address: 127.0.1.1; using 192.168.0.71 instead (on interface wlp58s0) 21/10/15 15:22:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/alex/MySoftware/spark-3.0.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int) WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release 21/10/15 15:22:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable root |-- _c0: string (nullable = true) |-- _c1: string (nullable = true) |-- _c2: string (nullable = true) |-- _c3: string (nullable = true) |-- _c4: string (nullable = true) . Specify a schema . In the previous example, we saw how to create a DataFrame from a CSV file. However, in the returned DataFrame all columns are of type string which is not actual the case. One neat way to fix this is by declaring the underlying schema to be used. A schema in Spark defines the column names and the associated data types for a DataFrame [1]. There are certain advantages that we get when we define the schema up front [1]: . Spark does not have to infer the data types | We can detect errors early if the data doesn&#39;t match the specified schema | Spark does not have to create an extra job in order to read a portion of the file to ascertain the schema | . Let&#39;s see how we can specify a schema. We have two way for doing so [1]: . Define it programmatically | Use a data definition language or DDL | . The code snippet below shows the first approach. . package train.spark import org.apache.spark.sql.SparkSession import org.apache.spark.sql.SQLContext import org.apache.spark.sql.types._ import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf object CreateDataFrame { def main(args: Array[String]) { val csvFile = &quot;/home/alex/qi3/learn_scala/scripts/spark/data/train.csv&quot; val appName: String = &quot;Spark DataFrame Demo&quot; val conf = new SparkConf().setAppName(appName) val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val customSchema = StructType(Array( StructField(&quot;mu-1&quot;, DoubleType, false), StructField(&quot;mu-2&quot;, DoubleType, false), StructField(&quot;label&quot;, IntegerType, false))) // specify a schema val df_schema = sqlContext.read.format(&quot;csv&quot;) .option(&quot;delimiter&quot;,&quot;,&quot;) .schema(customSchema) .load(csvFile) df_schema.printSchema() df_schema.groupBy(&quot;label&quot;).count().show() df_schema.show(5) } } . root |-- mu-1: double (nullable = true) |-- mu-2: double (nullable = true) |-- label: integer (nullable = true) +--+--+ |label|count| +--+--+ | null| 1| | 1| 185| | 3| 185| | 4| 185| | 2| 185| | 0| 185| +--+--+ +--+--+--+ | mu-1| mu-2|label| +--+--+--+ | null| null| null| |22.91|28.54| 0| |17.26|30.72| 0| |17.05|31.08| 0| |24.05|26.27| 0| +--+--+--+ only showing top 5 rows . References . Jules S. Damji, Brooke Wenig, Tathagata Das, Denny Lee, Learning Spark. Lighting-fast data analytics, O&#39;Reilly, 2nd Edition. |",
            "url": "https://pockerman.github.io/qubit_opus/spark-dataframe/spark/scala/big-data/data-engineering/data-analysis/2021/10/15/create-dataframe-spark-scala.html",
            "relUrl": "/spark-dataframe/spark/scala/big-data/data-engineering/data-analysis/2021/10/15/create-dataframe-spark-scala.html",
            "date": " • Oct 15, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Apache Spark. RDD operations",
            "content": "Overview . In this post, we will review the allowed operations on an RDD. . Acknowledgements . The content of this notebooks is to a large extent edited from [1]. . RDD operations with Scala . A Spark RDD provides two types of operations . Transformations | Actions | . Transformations . A transformation operation creates a new RDD from an existing RDD. Moreover, we can apply a chain of transformations once the data is loaded into memory. Below are some examples of transformations we can apply on an RDD. . map(function): It returns a new data set by operating on each element of the source RDD. | flatMap(function): Similar to map, but each item can be mapped to zero, one, or more items. | mapPartitions(function): Similar to map, but works on the partition level. | mapPartitionsWithIndex(function): Similar to mapPartitions, but provides a function with an Int value to indicate the index position of the partition. . | filter(function): It returns a new RDD that contains only elements that satisfy the predicate. . | union(otherDataset): It returns a new data set that contains the elements of the source RDD and the otherDataset RDD. Note that the participating RDDs should be of the same data type. . | intersection(otherDataset): It returns a new data set that contains the intersection of elements from the source RDD and the argument RDD. . | . Spark transformations are lazy evaluated. What this means that a transformation is applied only when an action is called. Examples of actions are collect and count . Actions . Transformations in Spark are lazy evaluated. What this means that a transformation is applied only when an action is called. Let&#39;s see some examples of actions. . collect(): Returns all the elements of the data set are returned as an array to the driver program. | count(): Returns the number of elements in the data set. | reduce(function): It returns a data set by aggregating the elements of the RDD it is applied on. The aggregation is done by using the user provided function argument. The function should take two arguments and returns a single argument. Moreover it should be commutative and associative so that it can be operated in parallel. . | first(): Returns the first element in the data set. . | take(n): Returns the first n elements in the data set as an array. | takeOrdered(n, [ordering]): Return the first n elements of the RDD using either their natural order or a custom comparator. | takeSample(withReplacement, num, [seed]): Returns an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed. | saveAsTextFile(path): Write the elements of the RDD as a text file in the local file system, HDFS, or any another supported storage system. | foreach(function): Applies the function argument on each element in the RDD. | . References . Subhashini Chellappan, Dharanitharan Ganesan, Practical Apache Spark. Using the Scala API, Apress |",
            "url": "https://pockerman.github.io/qubit_opus/spark/scala/api/data-analysis/big-data/2021/10/01/rdd-ops-scala.html",
            "relUrl": "/spark/scala/api/data-analysis/big-data/2021/10/01/rdd-ops-scala.html",
            "date": " • Oct 1, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Machine learning notes. Hyperparameter tuning",
            "content": "Overview . Machine learning models typically involve a parameter set that it is learnt during the training process. However, machine learning models also contain parameters that are not learnable and must be specified before the training process begins. In this notebook, we will review some commonly used methods to establish good hyperparameters values. In particular, we will review the following approaches . Grid search | Random search | Informed search | . Hyperparameter tuning . Machine learning models typically consist of a set of parameters that define their learnability. It is these parameters that somehow we try to estimate during training. However, most machine learning models also contain parameters that we need to establish before training begins. These parameters affect the learning process but they are not learnable themselves. We call these parameters as hyperparameters. . Hyperparameters are typically set before the modeling process begins. For example the number of clusters is a hyperparameter that the application needs to establish before running the algorithm. Thus, the crucial elemet that distinguishes parameters from hyperparameters is that the former are learnt by the model whilst the latter are set by the application. . In the sequel, we will differentiate hyperparameters into two categories. Namely, parameters that affect the model performance and parameters that do not. an example of the latter category is the number of cpu cores that we want to use when training the model. Although, this impacts the training time, it does not impact how the model performs on unseen data or in other words the model&#39;s performance. . Setting and hyperparameter values . Now that we have the needed definitions out of the way, we turn our attention to the main topic of this notebook. Namely, how do we set the optimal values for the model hyperparameters. Unfortunately, there is not a clear answer to this question. Hyperparameters are specific to each algorithm. However, there are available some general guidelines and tips that we can follow. Let&#39;s review some of the top tips. . First, we need to identify which hyperparameters values are in conflict. For example, if we are using sklearn&#39;s logistic regression model, the solver and penalty parameters have options that may be in conflict; the elasticnet penalty is only supported by the saga solver. . Another point to be aware of is that some hyperparameter values are simply silly. For example, setting the number of clusters equal to one when performing K-means clustering or equal to the number of points in the dataset, does not sound very meaningful. Similarly, setting the number of neighbors in a kNN algorithm equal to one is not very wise. . Below, we will review the following methods for hyperparamter tuning . Grid search | Random search | Informed search | . Let&#39;s start with grid search. . Grid search . Grid search performs an exhaustive search over a specified parameter values for an estimator. We can visualize this as a two dimensional grid. At each point of the grid, a different combination of parameters is examined. For example, consider an artificial model with three hyperparamters $a, b$ and $c$. Each of these parameters has the following values sets; $a in [a_1, a_2, a_3], b in [b_1, b_2], c in [c_1, c_2, c_3]$. Grid search performs exhaustive search by forming all the possible triplets and fitting the model using the identified values. Let&#39;s see how to perform grid search in sklearn. Overall the steps of using grid search in sklearn are as follows . Choose the algorithm to tune the hyperparameters (estimator) | Define which hyperparameters to tune (param_grid) | Define the range of values for each hyperparameter | Decide of the cross-validation scheme to use (cv) | Define the score function to be used when deciding which model is the best (scoring) | . The following example, taken from here, shows how to use grid search . from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.metrics import classification_report from sklearn.svm import SVC . digits = datasets.load_digits() # To apply an classifier on this data, we need to flatten the image, to # turn the data in a (samples, feature) matrix: n_samples = len(digits.images) X = digits.images.reshape((n_samples, -1)) y = digits.target # Split the dataset in two equal parts X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.5, random_state=0) . tuned_parameters = [{&#39;kernel&#39;: [&#39;rbf&#39;], &#39;gamma&#39;: [1e-3, 1e-4], &#39;C&#39;: [1, 10, 100, 1000]}, {&#39;kernel&#39;: [&#39;linear&#39;], &#39;C&#39;: [1, 10, 100, 1000]},] . clf = GridSearchCV(estimator=SVC(), param_grid=tuned_parameters, scoring=&#39;precision_macro&#39;) clf.fit(X_train, y_train) . GridSearchCV(estimator=SVC(), param_grid=[{&#39;C&#39;: [1, 10, 100, 1000], &#39;gamma&#39;: [0.001, 0.0001], &#39;kernel&#39;: [&#39;rbf&#39;]}, {&#39;C&#39;: [1, 10, 100, 1000], &#39;kernel&#39;: [&#39;linear&#39;]}], scoring=&#39;precision_macro&#39;) . print(&quot;Best parameters set found on development set:&quot;) print() print(clf.best_params_) print() print(&quot;Grid scores on development set:&quot;) print() means = clf.cv_results_[&#39;mean_test_score&#39;] stds = clf.cv_results_[&#39;std_test_score&#39;] for mean, std, params in zip(means, stds, clf.cv_results_[&#39;params&#39;]): print(&quot;%0.3f (+/-%0.03f) for %r&quot; % (mean, std * 2, params)) print() print(&quot;Detailed classification report:&quot;) print() print(&quot;The model is trained on the full development set.&quot;) print(&quot;The scores are computed on the full evaluation set.&quot;) print() y_true, y_pred = y_test, clf.predict(X_test) print(classification_report(y_true, y_pred)) print() . Best parameters set found on development set: {&#39;C&#39;: 10, &#39;gamma&#39;: 0.001, &#39;kernel&#39;: &#39;rbf&#39;} Grid scores on development set: 0.986 (+/-0.016) for {&#39;C&#39;: 1, &#39;gamma&#39;: 0.001, &#39;kernel&#39;: &#39;rbf&#39;} 0.959 (+/-0.028) for {&#39;C&#39;: 1, &#39;gamma&#39;: 0.0001, &#39;kernel&#39;: &#39;rbf&#39;} 0.988 (+/-0.017) for {&#39;C&#39;: 10, &#39;gamma&#39;: 0.001, &#39;kernel&#39;: &#39;rbf&#39;} 0.982 (+/-0.026) for {&#39;C&#39;: 10, &#39;gamma&#39;: 0.0001, &#39;kernel&#39;: &#39;rbf&#39;} 0.988 (+/-0.017) for {&#39;C&#39;: 100, &#39;gamma&#39;: 0.001, &#39;kernel&#39;: &#39;rbf&#39;} 0.983 (+/-0.026) for {&#39;C&#39;: 100, &#39;gamma&#39;: 0.0001, &#39;kernel&#39;: &#39;rbf&#39;} 0.988 (+/-0.017) for {&#39;C&#39;: 1000, &#39;gamma&#39;: 0.001, &#39;kernel&#39;: &#39;rbf&#39;} 0.983 (+/-0.026) for {&#39;C&#39;: 1000, &#39;gamma&#39;: 0.0001, &#39;kernel&#39;: &#39;rbf&#39;} 0.974 (+/-0.012) for {&#39;C&#39;: 1, &#39;kernel&#39;: &#39;linear&#39;} 0.974 (+/-0.012) for {&#39;C&#39;: 10, &#39;kernel&#39;: &#39;linear&#39;} 0.974 (+/-0.012) for {&#39;C&#39;: 100, &#39;kernel&#39;: &#39;linear&#39;} 0.974 (+/-0.012) for {&#39;C&#39;: 1000, &#39;kernel&#39;: &#39;linear&#39;} Detailed classification report: The model is trained on the full development set. The scores are computed on the full evaluation set. precision recall f1-score support 0 1.00 1.00 1.00 89 1 0.97 1.00 0.98 90 2 0.99 0.98 0.98 92 3 1.00 0.99 0.99 93 4 1.00 1.00 1.00 76 5 0.99 0.98 0.99 108 6 0.99 1.00 0.99 89 7 0.99 1.00 0.99 78 8 1.00 0.98 0.99 92 9 0.99 0.99 0.99 92 accuracy 0.99 899 macro avg 0.99 0.99 0.99 899 weighted avg 0.99 0.99 0.99 899 . The output from GridSearchCV can be categorized into three different groups . Results log: cv_results_ | Best results: best_index_, best_params_ and best_score_ | Other extra information such as refit_time_, and scorer_ | . Random search . The next method we will review is random search. According to wikipedia, random search (RS) is a family of numerical optimization methods that do not require the gradient of the problem to be optimized, and RS can hence be used on functions that are not continuous or differentiable. Such optimization methods are also known as direct-search, derivative-free, or black-box methods. . In general, random search is similar to the grid search approach we reviewed above. In particular, . We have to define an estimator | The parameters to be tuned and their range of values | Establish a cross-validation scheme | Establish a scoring functon | . We won&#39;t go into the details of why random search works. Instead let&#39;s see how to perform random search with scikit-learn. The following example, is a copy verbatim from scikit-learn . from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.model_selection import RandomizedSearchCV from scipy.stats import uniform . iris = load_iris() logistic = LogisticRegression(solver=&#39;saga&#39;, tol=1e-2, max_iter=2, random_state=0) distributions = dict(C=uniform(loc=0, scale=4), penalty=[&#39;l2&#39;, &#39;l1&#39;]) clf = RandomizedSearchCV(logistic, distributions, random_state=0) search = clf.fit(iris.data, iris.target) . /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; /home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn(&#34;The max_iter was reached which means &#34; . search.best_params_ . {&#39;C&#39;: 2.195254015709299, &#39;penalty&#39;: &#39;l1&#39;} . Grid search performs exhaustive search by trying out all possible combinations. On the other hand, random search selects a subset of combinations. Given this, it requires that we establish a sampling methodology. Grid search is more computationally expensive than random search. However, it is guaranteed to find the best score in the sample space. Random search is not guaranteed to find the best score, but it is likely to find a good one faster than grid search. . Informed search . Both grid and random search algorithms are uninformed search algorithms. What this means is that algorithms do not use any form of information in order to improve the searching. In this section, we will review some informed search algorithms . Coarse to fine tuning . In this approach, we start with a rough estimate and iteratively we refine our search.This approach utilizes both grid and random search. Here are the general steps we can follow towards this direction. . Start with random search | Find the areas in the sampling space that look promising | Do a grid search in these smaller areas | Continue until a good or optimal score is achieved | Other approaches to informed search include Bayesian statistics and genetic algorithms. However, we won&#39;t go into these directions as they can easily form the subject of whole books on their own. You can check the article A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning to get a conceptual view about the subject. . References . Hyperparameter tuning in Python course from Datacamp | Hyperparameter optimization | Random search |",
            "url": "https://pockerman.github.io/qubit_opus/machine-learning/hyperparameters/sklearn/grid-search/random-search/informed-search/2021/09/20/mln-hyperparameter-tuning.html",
            "relUrl": "/machine-learning/hyperparameters/sklearn/grid-search/random-search/informed-search/2021/09/20/mln-hyperparameter-tuning.html",
            "date": " • Sep 20, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "PyTorch with C++ 1",
            "content": "Overview . PyTorch is one of the well established libraries for modeling deep neural networks. The exposed Python API is the most commonly used one. However, the library also exposes bindings for C++. In this series of notebooks, I will try to demonstrate how to use the latter. I will be following to a large extent the documentation for the C++ frontend. . PyTorch with C++ 1 . I will start by performing some basic manipulations with the Tensor class. Instances of this class are used around the library to perform numerics. Whilst at this I will also show how to link against the C++ bindings. . Install and link against libtorch . To start with, download the binaries from here: https://pytorch.org/get-started/locally/. I use the pre-build binaries. Also make sure that you download the C++11 ABI. . Let&#39;s create now the first C++ PyTorch program. The following program is taken from the official documentation. . #include &lt;torch/torch.h&gt; #include &lt;iostream&gt; int main() { torch::Tensor tensor = torch::eye(3); std::cout &lt;&lt; tensor &lt;&lt; std::endl; return 0; } . I use the following CMakeLists.txt file to generate the needed Makefile . cmake_minimum_required(VERSION 3.0 FATAL_ERROR) PROJECT(example_1 VERSION 1.0.0 LANGUAGES CXX) SET(SOURCE example_1.cpp) SET(EXECUTABLE example_1) # default optionsSET(BUILD_SHARED_LIBS ON) SET(CMAKE_BUILD_TYPE &quot;Debug&quot;) SET(CMAKE_CXX_COMPILER g++) SET(CMAKE_CXX_STANDARD 20) SET(CMAKE_CXX_STANDARD_REQUIRED True) SET(CMAKE_C_COMPILER gcc) SET(CMAKE_LINKER_FLAGS &quot;-pthread&quot;) LIST(APPEND CMAKE_PREFIX_PATH /home/alex/MySoftware/libtorch) FIND_PACKAGE(Torch REQUIRED CONFIG) MESSAGE(STATUS &quot;TORCH Include directory ${TORCH_INCLUDE_DIRS}&quot;) MESSAGE(STATUS &quot;Build type: ${CMAKE_BUILD_TYPE}&quot;) MESSAGE(STATUS &quot;C++ Compiler: ${CMAKE_CXX_COMPILER}&quot;) MESSAGE(STATUS &quot;C Compiler: ${CMAKE_C_COMPILER}&quot;) INCLUDE_DIRECTORIES(${TORCH_INCLUDE_DIRS}) ADD_EXECUTABLE(${EXECUTABLE} ${SOURCE}) TARGET_LINK_LIBRARIES(${EXECUTABLE} ${TORCH_LIBRARIES}) . The program above produces the following output when built and executed: . 1 0 0 0 1 0 0 0 1 [ CPUFloatType{3,3} ] . I will now extend the example above to check on the provided API. Still, this is very elementary. Here is the updated example . #include &lt;torch/torch.h&gt; #include &lt;iostream&gt; #include &lt;vector&gt; int main() { if(torch::cuda::is_available()){ std::cout&lt;&lt;&quot;CUDA is available on this machine&quot;&lt;&lt;std::endl; } else{ std::cout&lt;&lt;&quot;CUDA is not available on this machine&quot;&lt;&lt;std::endl; } torch::Tensor tensor = torch::eye(3); std::cout &lt;&lt; tensor &lt;&lt; std::endl; std::vector&lt;double&gt; data(3, 2.0); auto tensor_from_data_1 = torch::tensor(data); std::cout &lt;&lt; tensor_from_data_1 &lt;&lt; std::endl; data[0] = data[1] = data[2] = 1.0; auto tensor_from_data_2 = torch::tensor(data); std::cout &lt;&lt; tensor_from_data_2 &lt;&lt; std::endl; auto sum = tensor_from_data_2 + tensor_from_data_1; std::cout &lt;&lt; sum &lt;&lt; std::endl; if(torch::cuda::is_available()){ // create a tensor and send it to the GPU auto cuda_tensor = torch::tensor({1.0, 2.0, 3.0}).to(&quot;cuda&quot;); } // compute element-wise product auto tensor1 = torch::tensor({1.0, 2.0, 3.0}); auto product = tensor1 * tensor1; std::cout &lt;&lt; product &lt;&lt; std::endl; return 0; } . The output of the program above is shown below . CUDA is not available on this machine 1 0 0 0 1 0 0 0 1 [ CPUFloatType{3,3} ] 2 2 2 [ CPUFloatType{3} ] 1 1 1 [ CPUFloatType{3} ] 3 3 3 [ CPUFloatType{3} ] 1 4 9 [ CPUFloatType{3} ] . The driver code above can be found in this github repository. . References . PyTorch | Using the PyTorch C++ Frontend | .",
            "url": "https://pockerman.github.io/qubit_opus/pytorch/deep-neural-networks/api/c++/numerics/2021/08/29/pytorch-with-cpp-1.html",
            "relUrl": "/pytorch/deep-neural-networks/api/c++/numerics/2021/08/29/pytorch-with-cpp-1.html",
            "date": " • Aug 29, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Machine learning with Scala Spark linear regression",
            "content": "Overview . In a previous post I developed a trivial Scala application that performs linear regression with only one feature. In this post, I want to go a bit further, I want to use Spark&#39;s MLlib to develop a linear regression model using two features this time. . Machine learning with Scala Spark linear regression . The first thing I need to do in order to use MLlib in my Scala application is to update the dependencies in the build.sbt script. These should now look as . libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-core_2.12&quot; % &quot;3.0.1&quot; libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-sql_2.12&quot; % &quot;3.0.1&quot; libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-mllib_2.12&quot; % &quot;3.0.1&quot; . package train.spark import org.apache.spark.ml.regression.LinearRegression import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.apache.spark.ml.feature.VectorAssembler import org.apache.spark.ml.linalg.Vectors import org.apache.spark.sql.types.DoubleType object LinearRegressionApp { def main(args: Array[String]) { val conf = new SparkConf().setAppName(&quot;Linear regression Spark&quot;) val sc = new SparkContext(conf) val session = SparkSession.builder().appName(&quot;Linear regression Spark&quot;).master(&quot;local[4]&quot;).getOrCreate() // Should be some file on your system val csvFile = &quot;/home/alex/qi3/spark_scala/data/spark_regression.csv&quot; val inputTrainigSet = session.read.format(&quot;csv&quot;).load(csvFile) println(&quot;Number of Partitions: &quot;+inputTrainigSet.rdd.getNumPartitions) println(&quot;Action: First element: &quot;+inputTrainigSet.rdd.first()) val analysisData = inputTrainigSet.withColumn(&quot;x1&quot;, inputTrainigSet(&quot;_c0&quot;).cast(DoubleType)) .withColumn(&quot;x2&quot;, inputTrainigSet(&quot;_c1&quot;).cast(DoubleType)) .withColumn(&quot;y&quot;, inputTrainigSet(&quot;_c2&quot;).cast(DoubleType)) .drop(&quot;_c0&quot;) .drop(&quot;_c1&quot;) .drop(&quot;_c2&quot;) //creating features column val assembler = new VectorAssembler() .setInputCols(Array(&quot;x1&quot;,&quot;x2&quot;)) .setOutputCol(&quot;features&quot;) // create the model val lr = new LinearRegression() .setMaxIter(10) .setRegParam(0.3) .setElasticNetParam(0.8) .setFeaturesCol(&quot;features&quot;) .setLabelCol(&quot;y&quot;) val trainigSet = assembler.transform(analysisData) // Fit the model val lrModel = lr.fit(trainigSet) // Print the coefficients and intercept for linear regression println(s&quot;Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}&quot;) // Summarize the model over the training set and print out some metrics val trainingSummary = lrModel.summary println(s&quot;numIterations: ${trainingSummary.totalIterations}&quot;) // there is sth wrong with my scala/spark version and this // throws an excpetion //println(s&quot;objectiveHistory: [${trainingSummary.objectiveHistory.mkString(&quot;,&quot;)}]&quot;) trainingSummary.residuals.show() println(s&quot;RMSE: ${trainingSummary.rootMeanSquaredError}&quot;) println(s&quot;r2: ${trainingSummary.r2}&quot;) } } . 21/08/25 12:36:15 WARN Utils: Your hostname, LT-2R0620-101 resolves to a loopback address: 127.0.1.1; using 192.168.0.71 instead (on interface wlp58s0) 21/08/25 12:36:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/alex/MySoftware/spark-3.0.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int) WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release 21/08/25 12:36:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 21/08/25 12:36:17 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect. Number of Partitions: 1 Action: First element: [0.0,4.0,4.357400305044133] 21/08/25 12:36:22 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS 21/08/25 12:36:22 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS Coefficients: [1.2545846367230242,0.7527507820338242] Intercept: 1.305736977601481 numIterations: 3 +--+ | residuals| +--+ | 0.04066019930735543| | -0.6631570819021908| | 0.8844468485401586| |-0.27725408848247746| | 1.523792089069631| | 0.9081058052618962| | 0.6154843963633212| | -1.5426210882366824| | -1.116750516169644| | -0.5438006575317718| |-0.41191237820348237| |-0.10423573938951769| | -0.7720329729420263| | -0.5175509972153742| | 0.5066514385552212| | 0.28386941829179424| | -1.7266735995448794| | -0.7963013580643907| | -0.8306208671329927| | -0.7913153349720496| +--+ only showing top 20 rows RMSE: 1.0241722775198268 r2: 0.8486882566011 .",
            "url": "https://pockerman.github.io/qubit_opus/spark/scala/api/data-analysis/machine-learning/2021/08/25/ml-scala-spark-linear-regression.html",
            "relUrl": "/spark/scala/api/data-analysis/machine-learning/2021/08/25/ml-scala-spark-linear-regression.html",
            "date": " • Aug 25, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Django with Docker and Celery",
            "content": "Acknowledgements . This post is basically a mixture of these two post from testdriven.io; Dockerizing Django with Postgres, Gunicorn, and Nginx and Handling Periodic Tasks in Django with Celery and Docker. You should check them out if you want more details on what is happening. . Overview . In a previous post we saw how to use docker to containerize a small Django application. In this post we will add Celery into the mix. The code files for this post can be found at demo_django_docker_celery . . Django with Docker and Celery . In order to improve user experience, long-running processes should be run outside the normal HTTP request/response flow in a background process. Thus, the goal of this post is to develop a Django application that works in conjunction with Celery to handle long-running processes outside the normal request/response cycle. In particular, we will develop an application with the following cycle: . The user starts a new task via a POST request to the server | The server creates a new a task that is added to the queue and returns the tak&#39;s id back to the client. | The client continues to poll the server to check the status of the task while the task itself is running in the background. | To get started, let&#39;s check the docker/docker-compose versions. . docker --version docker-compose --version . On my machine, these two return with the following information . Docker version 20.10.10, build b485636 docker-compose version 1.25.0, build unknown . We will extend the Django project from the previous post by adding a new application called my_app. The app directory structure now looks like the following . app/ Dockerfile hello_world_django manage.py my_app requirements.txt templates . That is we added two new directories my_app and templates. Furthermore, we need to update the requirements.txt file to . Django==3.0.7 mysqlclient==2.0.3 redis==3.5.3 celery==4.4.7 amqp==2.6.1 . The next changes concern docker-compose.yml file in order to account for the new containers. This is shown below . version: &#39;3.3&#39; services: web: build: ./app container_name: my_django_app_django_container command: python manage.py runserver 0.0.0.0:8000 volumes: - ./app/:/usr/src/app/ ports: - 8000:8000 env_file: - ./.env.dev depends_on: - db db: image: mysql:5.7 container_name: my_django_app_mysql_container ports: - &#39;3306:3306&#39; environment: MYSQL_DB: &#39;django_app_demo&#39; MYSQL_PASSWORD: &#39;password&#39; MYSQL_ROOT_PASSWORD: &#39;password&#39; celery: container_name: my_django_app_celery_container build: ./app command: celery worker --app=hello_world_django --loglevel=info volumes: - ./app/:/usr/src/app/ env_file: - ./.env.dev depends_on: - web - redis redis: image: redis:6-alpine container_name: my_django_app_redis_container . Finally, we need to update the .env.dev file . DEBUG=1 SECRET_KEY=foo DJANGO_ALLOWED_HOSTS=127.0.0.1 0.0.0.0 localhost [::1] SQL_ENGINE=django.db.backends.mysql SQL_DATABASE=django_app_demo SQL_USER=root SQL_PASSWORD=password SQL_HOST=db SQL_PORT=3306 CELERY_BROKER_URL=redis://redis:6379/0 CELERY_BACKEND_URL=redis://redis:6379/0 . Notice that the SQL_HOST has the name of the MySQL service. Also we need to postfix with _URL the variables related to Celery. We can now build and run the containers . docker-compose up --build . However, the above will fail as we haven&#39;t created yet the database. To do so we can log on the MySQL container using . sudo docker exec -it my_django_app_mysql_container /bin/bash . whilst in the container, issue . mysql -u root -p . In order to access the MySQL server running on the container and create the data base django_app_demo and grant access to the user root. Once this is done, we need to run the Django migrations. In order to do so, log on the web application container . docker exec -it my_django_app_django_container /bin/bash . We are now ready to go. Navigate at http://127.0.0.1:8000/ to view the application. Launch a task and poll the application for the result. A container&#39;s logs can be viewed with . docker my_django_app_django_container logs . References . Dockerizing a Python Django Web Application | Django Development with Docker Compose and Machine | Dockerize a Flask, Celery, and Redis Application with Docker Compose | Dockerizing Django with Postgres, Gunicorn, and Nginx |",
            "url": "https://pockerman.github.io/qubit_opus/programming/django/docker/celery/containers/python/2021/08/24/django-docker-celery.html",
            "relUrl": "/programming/django/docker/celery/containers/python/2021/08/24/django-docker-celery.html",
            "date": " • Aug 24, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Apache Spark. Create an RDD with Scala",
            "content": "Overview . In post Apache Spark. Application concepts we went over some basic but core concepts associated with Spark. In this post, we will introduce the most basic abstraction in Spark namely the RDD (or the Resilient Distributed Dataset) [2]. Although, modern applications most likely will be using the DataFrame and/or DataSet APIs, still the RDD data structure is what lies underneath the latter two and therefore always useful to know. Moreover, in this post we will see how to create a Spark RDD within a Scala application. As we will see, there are various methods to create an RDD in Spark. The following example is taken for Spark by {Examples}. . You can find the example snippets at Computational Statistics with Scala. . The RDD abstraction . The RDD is perhaps the most basic abstraction in Spark. An RDD is an immutable collection of objects that can be distributed across a cluster of computers. An RDD collection is divided into a number of partitions so that each node on a Spark cluster can independently perform computations. There are three concepts associated with an RDD [2]: . Dependencies | Partitions | Compute function | . Partitions provide the ability to split the work and therefore to parallelize computation across executors. The compute function produces the data that will be stored in the RDD. Finally the dependencies, inform Spark how an RDD is constructed. This allows for RDD resiliency as Spark, if needed, is able to recreate the RDD from the dependencies [2]. . Now that we have a very simplified overview of what an RDD is, let&#39;s how we can create one. . Create Spark RDD with Scala . There are two main methods available in Spark to create an RDD: . SparkContext.parallelize method | Read from a file | . The first method is illustrated in the code listing example below . package train.spark import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf object CreateRDD { def main(args: Array[String]) { val conf = new SparkConf().setAppName(&quot;Hello Spark RDD&quot;) val sc = new SparkContext(conf) val data = Array(1,2,3,4,5,6,7,8,9,10) val rdd = sc.parallelize(data) rdd.foreach(println) println(&quot;Number of Partitions: &quot;+rdd.getNumPartitions) println(&quot;Action: First element: &quot;+rdd.first()) } } . Running the application produces something like the following . 3 6 1 8 9 2 7 4 5 10 Number of Partitions: 4 Action: First element: 1 . Note the the output may be different as it depends on which thread is accessing the standard output first. Note that the application above has to create a SparkContext first before we are able to create an RDD. . . Remark . Creating a SparkContext is not necessary when we use the Spark shell as one such object is already created for us. . . The second method is to read a file from disk. This is also shown in the snippet below. . package train.spark import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf object CreateRDDFile { def main(args: Array[String]) { val conf = new SparkConf().setAppName(&quot;Hello Spark RDD&quot;) val sc = new SparkContext(conf) // Should be some file on your system val csvFile = &quot;/home/alex/qi3/learn_scala/scripts/spark/data/train.csv&quot; val csvRDD = sc.textFile(csvFile) println(&quot;Number of Partitions: &quot;+csvRDD.getNumPartitions) // prints the header of the file println(&quot;Action: First element: &quot;+csvRDD.first()) } } . Upon executing this code, we get . Number of Partitions: 2 Action: First element: #Duplicate: 0, Delete: 1, Normal-1: 2, TUF: 3, Normal-2: 4 . However, we are interested in converting the contents of the file into floating point numbers so that we can feed them to a machine learning algorithm. We can do this as follows. we can use the map() function to convert the RDD[String] into an RDD[Array[Double]] . val doubleRDD = csvRDD.map(line =&gt; {line.split(&quot;,&quot;)}) .map( arrString =&gt; {Try(Array(arrString(0).toDouble, arrString(1).toDouble, arrString(2).toDouble))}) .map(_ match {case Success(res) =&gt; res case Failure(res) =&gt; Array(-100, -100, -100)}) . We can also use a schema in order to let Spark know the type of the data but this requires that we use a DataFrame instead and not an RDD. . Note also that Spark divides by default data into two partitions and distributes them across a cluster. The number of partitions can be specified while creating an RDD as shown below. . object CreateRDDFile { def main(args: Array[String]) { ... // Should be some file on your system val csvFile = &quot;/home/alex/qi3/learn_scala/scripts/spark/data/train.csv&quot; val csvRDD = sc.textFile(csvFile, 4) ... } } . Other methods . As an aside, we can create an RDD by using the following also: . JDBC | Cassandra | HBase | Elasticsearch | . References . RDD Programming Guide | Jules S. Damji, Brooke Wenig, Tathagata Das, Deny Lee, Learning Spark. Lighting-fasts data analytics, 2nd Edition, O&#39;Reilly. |",
            "url": "https://pockerman.github.io/qubit_opus/spark/scala/big-data/data-engineering/data-analysis/2021/08/19/spark-rdd-scala.html",
            "relUrl": "/spark/scala/big-data/data-engineering/data-analysis/2021/08/19/spark-rdd-scala.html",
            "date": " • Aug 19, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Apache Spark. Submit a self-contained Scala application",
            "content": "Overview . In post Apache Spark. Application concepts we went over some basic but core concepts associated with Spark. In this series of posts, we will be using Scala as the main programming language. Thus, in this post, we describe the steps you need to take in order to submit a Scala application to be executed by Spark. You should also check the official documentation on Self-contained Applications. The code snippets can be found Computational Statistics with Scala. . Submit a self-contained Scala application . I will assume that the environment is already set. Meaning, Scala is already installed, in this post I use version 2.13.3, Spark is also installed, for this post I use version 3.0.1. Finally, I am using SBT for building and packaging the application. . The Scala application simply informs us about the version of Spark we are using, the name of the master node and whether we run in local or distributed mode. It is shown below . //HelloSpark.scala package spark import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf object HelloSpark { def main(args: Array[String]) { val conf = new SparkConf().setAppName(&quot;Hello Scala Spark&quot;) val sc = new SparkContext(conf) println(&quot;Spark version: &quot; + sc.version) println(&quot;Spark master: &quot; + sc.master) println(&quot;Spark running &#39;locally&#39;?: &quot; + sc.isLocal) } } . Notice, that I placed the application under the spark package. This will be used when submitting the class to Spark. For the moment we need to structure properly our code for SBT to work. In particular, we need a file structure as follows . build.sbt +src/ +main/ +scala/ +spark/HelloSpark.scala . The build.sbt script is shown below . name := &quot;Hello Spark&quot; version := &quot;0.0.1&quot; scalaVersion := &quot;2.13.3&quot; libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-core_2.12&quot; % &quot;3.0.1&quot; libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-sql_2.12&quot; % &quot;3.0.1&quot; . As a side not, observe that I don&#39;t use double percentages. The reason why is explained here. . In order to compile our code, call sbt at the level where the build.sbt script is located. This will bring up the sbt console. Once in the console, type compile to build the project. When the compilation finishes and still in the sbt console, type package to create the application .jar file. The whole process for this application should not take long. Once finished, we can submit our application to Spark for execution. I use the following bash shell script for convenience. . /home/alex/MySoftware/spark-3.0.1-bin-hadoop2.7/bin/spark-submit --class &quot;spark.HelloSpark&quot; --master local[4] target/scala-2.13/hello-spark_2.13-0.0.1.jar . Notice how I specify the class to execute by prefixing it with the package name it belongs to. Upon execution of the script you should see something similar to what follows . Spark version: 3.0.1 Spark master: local[4] Spark running &#39;locally&#39;?: true . According to the official documentation, applications should define a main() method instead of extending scala.App, as the latter may not work correctly. . Summary . In this post, I described the steps I need to take in order to build and submit a Scala application to Spark. .",
            "url": "https://pockerman.github.io/qubit_opus/spark/scala/api/data-analysis/2021/08/06/apache-spark-scala.html",
            "relUrl": "/spark/scala/api/data-analysis/2021/08/06/apache-spark-scala.html",
            "date": " • Aug 6, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Django with Docker",
            "content": "Overview . Deploying applications is never ease regardless of the provisioning one may take. Containers solve many of the problems of application deployment. In this post, I want to describe how to containerize a minimal django application with docker. I will assume that docker is already installed on the machine. If not, checkout the official docker documentation. The code for this notebook can be found at this repository. . Acknowledgements . This post is basically edited from the testdriven.io; Dockerizing Django with Postgres, Gunicorn, and Nginx post. You should check this article out if you want more details on what is happening. . Django with docker . Assuming that docker is already installed on the host machine, I can check the version of docker and docker-compose by typing in the terminal. . docker --version docker-compose --version . The application won&#39;t do something really great as my goal here is to understand how to make these components work together. Thus, the application I will be looking at has two main components in terms of infrastructure. Namely, . It uses Django to support HTTP requests/responses | It uses MySQL for persistence | . Django project . Creating a simple Django project is fairly easy. Checkout how to do so here. Let&#39;s create a hello_world_django project. I will have the project files in the app directory. So . mkdir app &amp;&amp; cd app django-admin startproject hello_world_django . . The above creates the app directory and within that directory it creates the hello_world_django. . ├── hello_world_django │ ├── __init__.py │ ├── asgi.py │ ├── settings.py │ ├── urls.py │ └── wsgi.py ├── manage.py . Let&#39;s create a requirements.txt file in the app directory with the following contents . Django==3.0.7 . In order to containerize the hello_world_django project, we need to have a Dockerfile. A Dockerfile specifies overall how our application is to be built. So in the app directory, create a Dockerfile with the following contents . # pull official base image FROM python:3.8.3-alpine # set work directory WORKDIR /usr/src/app # set environment variables ENV PYTHONDONTWRITEBYTECODE 1 ENV PYTHONUNBUFFERED 1 # install dependencies RUN pip install --upgrade pip COPY ./requirements.txt . RUN pip install -r requirements.txt # copy project COPY . . . The Dockerfile above, starts with an Alpine-based Docker image for Python 3.8.3. It then sets a working directory along with two environment variables: . PYTHONDONTWRITEBYTECODE: Prevents Python from writing pyc files to disc (equivalent to python -B option) | PYTHONUNBUFFERED: Prevents Python from buffering stdout and stderr (equivalent to python -u option) | . Finally, it updates pip, copies over the requirements.txt file, installed the dependencies, and copied over the Django project itself. Although we can use docker build to build our image, I will use docker-compose to do so. In the source directory, create a file called docker-compose.yml with the following contents . version: &#39;3.8&#39; services: web: build: ./app command: python manage.py runserver 0.0.0.0:8000 volumes: - ./app/:/usr/src/app/ ports: - 8000:8000 env_file: - ./.env.dev . We also need one more file, namely the .env.dev file that contains the following . DEBUG=1 SECRET_KEY=foo DJANGO_ALLOWED_HOSTS=localhost 127.0.0.1 0.0.0.0 [::1] . The file should also be placed at the root directory where the docker-compose.yml is located. We also need to update the settings.py file so that we can retrieve these from the environment under which the application is running. . SECRET_KEY = os.environ.get(&quot;SECRET_KEY&quot;) DEBUG = int(os.environ.get(&quot;DEBUG&quot;, default=0)) # &#39;DJANGO_ALLOWED_HOSTS&#39; should be a single string of hosts with a space between each. # For example: &#39;DJANGO_ALLOWED_HOSTS=localhost 127.0.0.1 [::1]&#39; ALLOWED_HOSTS = os.environ.get(&quot;DJANGO_ALLOWED_HOSTS&quot;).split(&quot; &quot;) . Let&#39;s now build the image and check if everything works as described above. We can do so . docker-compose build . Start the container by using . docker-compose up -d . We can view the application at http://0.0.0.0:8000/. This should display django&#39;s default landing page. So far so good. Let&#39;s now try to integrate MySQL into the mix. . Configure MySQL . Adding MySQL into the mix, we just need to add a new service into docker-compose.yml. This is shown below . version: &#39;3.7&#39; services: web: build: ./app command: python manage.py runserver 0.0.0.0:8000 volumes: - ./app/:/usr/src/app/ ports: - 8000:8000 env_file: - ./.env.dev db: image: mysql:5.7 container_name: mysql_my_django_app ports: - &#39;3306:3306&#39; environment: MYSQL_DATABASE: &#39;django_app_demo&#39; MYSQL_PASSWORD: &#39;password&#39; MYSQL_ROOT_PASSWORD: &#39;password&#39; volumes: mysql_data: . To persist the data beyond the life of the container we configured a volume. This config will bind mysql_data to the &quot;/var/lib/mysql/data/&quot; directory in the container. . Note that since the default database in django is sqlite3, we need to update the DATABASES entry in the settings.py file according to . DATABASES = { &#39;default&#39;: { &#39;ENGINE&#39;: os.environ.get(&quot;SQL_ENGINE&quot;, &quot;django.db.backends.sqlite3&quot;), #&#39;django.db.backends.mysql&#39;, &#39;NAME&#39;: os.environ.get(&quot;SQL_DATABASE&quot;, BASE_DIR / &quot;db.sqlite3&quot;), #&#39;django_app_demo&#39;, &#39;USER&#39;: os.environ.get(&quot;SQL_USER&quot;, &quot;user&quot;), #&#39;root&#39;, &#39;PASSWORD&#39;: os.environ.get(&quot;SQL_PASSWORD&quot;, &quot;password&quot;), #&#39;password&#39;, &#39;HOST&#39;: os.environ.get(&quot;SQL_HOST&quot;, &quot;localhost&quot;), #&#39;db&#39;, &#39;PORT&#39;: os.environ.get(&quot;SQL_PORT&quot;, &quot;3306&quot;), #3306, } } . Similarly, we update the .env.dev file now looking like . DEBUG=1 SECRET_KEY=foo DJANGO_ALLOWED_HOSTS=localhost 127.0.0.1 0.0.0.0 [::1] SQL_ENGINE=django.db.backends.mysql SQL_DATABASE=django_app_demo SQL_USER=root SQL_PASSWORD=password SQL_HOST=db SQL_PORT=3306 . We also need to to install mysqlclient otherwise we get a django exception django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module. We add this in the requirements file. So the requirements.txt is now as follows . Django==3.0.7 mysqlclient==2.0.3 . We now have two containers. Let&#39;s build the new image and spin the two containers . docker-compose up -d --build . Once again we can access the application at http://0.0.0.0:8000/. . Migrations . In order to be able to persist data, we need to create the database tables. Django uses the notion of migrations in order to build and monitor the database tables. Let&#39;s instruct Django to run any migrations. Typically, we don&#39;t want to do that every time we fire up the container, so I just use a manual approach . docker-compose exec django_app python manage.py migrate . Summary . In this post, I described how to containerize a minimal Django-based web application. Specifically, I used the following docker commands . References . Dockerizing a Python Django Web Application | Django Development with Docker Compose and Machine | Dockerize a Flask, Celery, and Redis Application with Docker Compose | Dockerizing Django with Postgres, Gunicorn, and Nginx |",
            "url": "https://pockerman.github.io/qubit_opus/programming/django/docker/containers/python/2021/07/29/django-with-docker.html",
            "relUrl": "/programming/django/docker/containers/python/2021/07/29/django-with-docker.html",
            "date": " • Jul 29, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Use OpenAI Gym environments from C++",
            "content": "Overview . OpenAI-Gym is one of the most commonly used of Python packages used when developing reinforcement learning algorithms. In this post, we use the boost::python library to interact with an OpenAI-Gym environment from a C++ program. . Use OpenAI Gym environments from C++ . In this post we use the boost::python library in order to interact with an OpenAI-Gym environment. Specifically, we will interact with the FrozenLake-v0 environment. The exposition here is meant to be minimal rather than exhaustive. . In order to use boost::python we need to include it. . #include &lt;boost/python.hpp&gt; . Before starting the interaction with the Python interpreter, we need to initialize it. This is done by calling . Py_Initialize() . We next import the module of interest that is gym. We obtain a namespace so that we can use it to obtain the created environment in the interpreter. This is done in the following line . auto gym_namespace = gym_module.attr(&quot;__dict__&quot;); . We can extract various attributes. For instance, the module name as shown below . std::cout&lt;&lt;&quot;Module name &quot;&lt;&lt;boost::python::extract&lt;const char*&gt;(gym_namespace[&quot;__name__&quot;])&lt;&lt;std::endl; . The line of interest is where we create the environment in the interpreter and get a pointer back in the C++ program . // create an environment auto ignored = boost::python::exec(&quot;import gym n&quot; &quot;world = gym.make(&#39;FrozenLake-v0&#39;, is_slippery=True) n&quot; &quot;world = world.unwrapped&quot;, gym_namespace); // get the created world auto world = boost::python::extract&lt;boost::python::api::object&gt;(gym_namespace[&quot;world&quot;]); . Observe how we access the environment as an entry in the gym_namespace we created above. Once we have an instance of the environment, we can query it as shown below . auto world_dict = boost::python::extract&lt;boost::python::dict&gt;(world().attr(&quot;__dict__&quot;)); auto observation_space = boost::python::extract&lt;boost::python::api::object&gt;(world_dict()[&quot;observation_space&quot;]); std::cout&lt;&lt;&quot;Number of states &quot;&lt;&lt;boost::python::extract&lt;int&gt;(observation_space().attr(&quot;__dict__&quot;)[&quot;n&quot;]) &lt;&lt;std::endl; auto action_space = boost::python::extract&lt;boost::python::api::object&gt;(world_dict()[&quot;action_space&quot;]); std::cout&lt;&lt;&quot;Number of actions &quot;&lt;&lt;boost::python::extract&lt;int&gt;(action_space().attr(&quot;__dict__&quot;)[&quot;n&quot;])&lt;&lt;std::endl; . or execute an action . // create an environment boost::python::exec(&quot;observation = world.reset()&quot;, gym_namespace); // the observation auto observation = boost::python::extract&lt;int&gt;(gym_namespace[&quot;observation&quot;]); std::cout&lt;&lt;&quot;Observation after reset=&quot;&lt;&lt;observation&lt;&lt;std::endl; . The full driver program is shown below. . #include &lt;boost/python.hpp&gt; #include &lt;iostream&gt; int main(){ try { std::cout&lt;&lt;&quot;Starting the interpreter...&quot;&lt;&lt;std::endl; Py_Initialize(); std::cout&lt;&lt;&quot;Importing module...&quot;&lt;&lt;std::endl; auto gym_module = boost::python::import(&quot;gym&quot;); auto gym_namespace = gym_module.attr(&quot;__dict__&quot;); std::cout&lt;&lt;&quot;Module name &quot;&lt;&lt;boost::python::extract&lt;const char*&gt;(gym_namespace[&quot;__name__&quot;])&lt;&lt;std::endl; std::cout&lt;&lt;&quot;Creating the environment...&quot;&lt;&lt;std::endl; // create an environment auto ignored = boost::python::exec(&quot;import gym n&quot; &quot;world = gym.make(&#39;FrozenLake-v0&#39;, is_slippery=True) n&quot; &quot;world = world.unwrapped&quot;, gym_namespace); // get the created world auto world = boost::python::extract&lt;boost::python::api::object&gt;(gym_namespace[&quot;world&quot;]); auto world_dict = boost::python::extract&lt;boost::python::dict&gt;(world().attr(&quot;__dict__&quot;)); // uncomment this to see the attributes /*auto keys = boost::python::list(world_dict().keys()); for(auto i=0; i&lt;boost::python::len(keys); ++i){ std::cout&lt;&lt;boost::python::extract&lt;std::string&gt;(boost::python::object(keys[i]))()&lt;&lt;std::endl;; }*/ auto observation_space = boost::python::extract&lt;boost::python::api::object&gt;(world_dict()[&quot;observation_space&quot;]); std::cout&lt;&lt;&quot;Number of states &quot;&lt;&lt;boost::python::extract&lt;int&gt;(observation_space().attr(&quot;__dict__&quot;)[&quot;n&quot;])&lt;&lt;std::endl; auto action_space = boost::python::extract&lt;boost::python::api::object&gt;(world_dict()[&quot;action_space&quot;]); std::cout&lt;&lt;&quot;Number of actions &quot;&lt;&lt;boost::python::extract&lt;int&gt;(action_space().attr(&quot;__dict__&quot;)[&quot;n&quot;])&lt;&lt;std::endl; // create an environment boost::python::exec(&quot;observation = world.reset()&quot;, gym_namespace); // the observation auto observation = boost::python::extract&lt;int&gt;(gym_namespace[&quot;observation&quot;]); std::cout&lt;&lt;&quot;Observation after reset=&quot;&lt;&lt;observation&lt;&lt;std::endl; } catch(boost::python::error_already_set const &amp;) { PyErr_Print(); } std::cout&lt;&lt;&quot;Finilize...&quot;&lt;&lt;std::endl; return 0; } . Running the program gives the following output . Starting the interpreter... Importing module... Module name gym Creating the environment... Number of states 16 Number of actions 4 Observation after reset=0 Finilize... . Although boost::python handles a lot of the low level details needed for interacting with Python, the above program is rather dense and for more complicated scenarios, e.g. implementing A2C on an Atari environment, things will definitely get more complicated. One way to handle this is to write own wrappers that hide most of the boilerplate code. . Buidling the program with CMake . As an aside here is the CMakeLists.txt to use in order to build the program above . CMAKE_MINIMUM_REQUIRED(VERSION 3.6) SET(SOURCE example_1.cpp) SET(EXECUTABLE example_1) # find Boost FIND_PACKAGE(Boost 1.65.0 REQUIRED COMPONENTS python system) if(Boost_FOUND) if(Boost_LIBRARY_DIR) MESSAGE( STATUS &quot;Boost_LIBRARY_DIR not empty using it: ${Boost_LIBRARY_DIR}&quot; ) elseif(BOOST_LIBRARYDIR) MESSAGE( STATUS &quot;Boost_LIBRARY_DIR empty, but BOOST_LIBRARYDIR is set. Setting Boost_LIBRARY_DIR to: ${BOOST_LIBRARYDIR}&quot; ) set(Boost_LIBRARY_DIR ${BOOST_LIBRARYDIR}) elseif(Boost_LIBRARY_DIRS) MESSAGE( STATUS &quot;Boost_LIBRARY_DIR empty, but Boost_LIBRARY_DIRS is set. Setting Boost_LIBRARY_DIR to: ${Boost_LIBRARY_DIRS}&quot; ) set(Boost_LIBRARY_DIR ${Boost_LIBRARY_DIRS}) elseif(Boost_LIBRARY_DIR_RELEASE) MESSAGE( STATUS &quot;Boost_LIBRARY_DIR empty, but Boost_LIBRARY_DIR_RELEASE is set. Setting Boost_LIBRARY_DIR to: ${Boost_LIBRARY_DIR_RELEASE}&quot; ) set(Boost_LIBRARY_DIR ${Boost_LIBRARY_DIR_RELEASE}) elseif(Boost_LIBRARY_DIR_DEBUG) MESSAGE( STATUS &quot;Boost_LIBRARY_DIR empty, but Boost_LIBRARY_DIR_DEBUG is set. Setting Boost_LIBRARY_DIR to: ${Boost_LIBRARY_DIR_RELEASE}&quot; ) set(Boost_LIBRARY_DIR ${Boost_LIBRARY_DIR_DEBUG}) else() MESSAGE( WARNING &quot;Boost_LIBRARY_DIR empty, BOOST_LIBRARYDIR empty, Boost_LIBRARY_DIRS empty: might miss libraries at linking&quot; ) endif() else() MESSAGE( FATAL_ERROR &quot;Boost was not found!&quot;) endif() INCLUDE_DIRECTORIES(${Boost_INCLUDE_DIRS}) # use c++20 standard SET(CMAKE_CXX_COMPILER /usr/bin/g++-10) SET(CMAKE_C_COMPILER /usr/bin/gcc-10) SET(CMAKE_CXX_STANDARD 20) SET(CMAKE_CXX_STANDARD_REQUIRED True) SET(CMAKE_CXX_FLAGS &quot;-g -Wall -Wextra&quot;) SET(CMAKE_LINKER_FLAGS &quot;-pthread&quot;) # use the Boost link directories LINK_DIRECTORIES(${Boost_LIBRARY_DIR}) # this may be different... LINK_DIRECTORIES(/usr/lib/python3.8/config-3.8-x86_64-linux-gnu/) ADD_EXECUTABLE(${EXECUTABLE} ${SOURCE}) TARGET_LINK_LIBRARIES(${EXECUTABLE} python3.8) TARGET_LINK_LIBRARIES(${EXECUTABLE} boost_python38) TARGET_LINK_LIBRARIES(${EXECUTABLE} boost_system) .",
            "url": "https://pockerman.github.io/qubit_opus/programming/openai-gym/reinforcement-learning/c++/python/boost-python/2021/07/21/use-openai-gym-cpp.html",
            "relUrl": "/programming/openai-gym/reinforcement-learning/c++/python/boost-python/2021/07/21/use-openai-gym-cpp.html",
            "date": " • Jul 21, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Use Django with Apache",
            "content": "Overview . Recently, I had to serve an application developed with Django on a LAMP infrastructure. I will describe in this post the steps I followed in order to do so. You can find the application here. . Use Django with Apache . To start with, the Django offcial documentation has most of the information you need here. The suggested way is by using mod_wsgi. The problem that I had with that was due to a problem with SQLite that the project was initially using. I had to create a virtual environment on the server and install everything under the virtual environment. However, mod_wsgi only works with the version of Python it was compiled against. So if this is the case you may have to install the package in your environment. . In the latter scenario, you need to configure the modules loaded by Apache such that it points to your installation and not the system-wide one. You can run a find command on the directory you have your virtual environments: . find /path/to/your/envs/ -name &quot;mod_wsgi*.so&quot; . Then you need to update loadmodule.conf (which is typically located at /local/apache2/etc/) to point to the path given by find. . You then need to update the httpd.conf file according to your needs. You will need to provide as a minimum the following . Alias /robots.txt /path/to/mysite.com/static/robots.txt Alias /favicon.ico /path/to/mysite.com/static/favicon.ico Alias /media/ /path/to/mysite.com/media/ Alias /static/ /path/to/mysite.com/static/ &lt;Directory /path/to/mysite.com/static&gt; Require all granted &lt;/Directory&gt; &lt;Directory /path/to/mysite.com/media&gt; Require all granted &lt;/Directory&gt; WSGIDaemonProcess django_app_name python-home=/path/to/virtual/env/ python-path=/path/to/django/app/ WSGIProcessGroup django_app_name WSGIScriptAlias / /path/to/django/app/wsgi.py process-group=django_app_name WSGIApplicationGroup %{GLOBAL} &lt;Directory /path/to/mysite.com/mysite&gt; &lt;Files wsgi.py&gt; Require all granted &lt;/Files&gt; &lt;/Directory&gt; . It turns out that the process is not overly complicated but it may take some time to figure out some things. .",
            "url": "https://pockerman.github.io/qubit_opus/programming/django/python/apache/web-development/2021/07/15/django-apache.html",
            "relUrl": "/programming/django/python/apache/web-development/2021/07/15/django-apache.html",
            "date": " • Jul 15, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "MPI P2P Communication Modes",
            "content": "Overview . In the previous post, we saw the standard communication mode that is used under the hoods with MPI_Send. Here, we describe a few more communication modes supported by the MPI standard. . MPI P2P communication modes . MPI has three additional modes for P2P communication [1]: . Buffered | Synchronous | Ready | . In the buffered mode, the sending operation is always locally blocking and just like with standard communication mode, it will return as soon as the message is copied to a buffer. The difference here is that the buffer is user-provided [1]. . The synchronous mode is a globally blocking operation [1]. In this mode, the sending operation will return only when the retrival of the message has been initiated by the receiving process. However, the message receiving may not be complete [1]. . . Remark . The buffered and synchronous modes constitute two symmetrical endpoints. In the buffered mode we trade the waiting with memory whilst in the synchronous mode we don&#39;t mind o wait for the message to reach the destination. . . In the ready mode, the send operation will succeed only if a matching receive operation has been initiated already [1]. Otherwise, the function returns with an error code. The purpose of this mode is to reduce the overhead of handshaking operations [1]. . So how can we distinguish between these different commnunication modes? This is done by prefixing the initial letter of each mode before the Send [1]. Thus, we have . MPI_Bsend | MPI_Ssend | MPI_Rsend | . The resr of the functions signatures is the same as that of MPI_Send [1] . int [ MPI_Bsend | MPI_Ssend | MPI_Rsend ] (void∗ buf , int count , MPI_Datatype datatype , int dest , int tag , MPI_Comm comm ) ; . . Remark . Bear in mind that blocking sends can be matched with non blocking receives, and vice versa [1]. However, the tuple (communicator, rank, message tag) should match in order to do so. . . Summary . In this post, we introduced three more communication modes supported by MPI for P2P message exchange. The fact that we have in our disposal different means for P2P communucation means that we can adjust the application to better suit the hardware it is running on. The interafces of the supplied functions are the same with that of MPI_Send. This greatly facilitates development. We can, for example, create an array of function pointers so that we group these functions in one place and call the specified function based on some given configuration parameter. . References . Gerassimos Barlas, Multicore and GPU Programming An Integrated Approach, Morgan Kaufmann |",
            "url": "https://pockerman.github.io/qubit_opus/programming/mpi/parallel-computing/c++/distributed-computing/2021/07/08/mpi-comm-modes.html",
            "relUrl": "/programming/mpi/parallel-computing/c++/distributed-computing/2021/07/08/mpi-comm-modes.html",
            "date": " • Jul 8, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "MPI Basic Point-to-Point Communication",
            "content": "Overview . When two processes communicate with each other, we call this communication pattern as point-to-point communication [3]. MPI allows for easy information exchange between processes or nodes although the resulting interfaces may be quite overwhelming. In this notebook, we introduce the two most basic point-to-point communication functions in MPI namely MPI_Send (doc) and MPI_Recv (doc). . Basic point-to-point Communication . MPI_Send performs a blocking send; that is the function call may block until the message is received by the destination process [1]. An MPI_Send must be matched with a receive operation. MPI_Recv (doc) performs a blocking receive [2]. . . Remark . Note that MPI_Send may return before the message is delivered. MPI_Send uses the so called standard communication mode [3]. Behind the scenes, MPI decides whether to block or not based on the size of the message. The blocking lasts until the the destination process collects the message. Thus, if the message is small MPI_Send returns as soon as the message is copied to a local MPI buffer [3]. This copy is needed in order to release the buffer used by the source process for subsequent operations, because with this form of send, there is no way for the sender process to know when the message has been delivered [3]. . . MPI_Send sends a buffer of data of a certain type to another process. It requires the following arguments. . A pointer to a data buffer | The datatype contained in the specified data buffer | How many elements are contained in the buffer | A message tag (sort of the id of the message) which should be a non-negative integer | The receiving process id wihin the communicator | The communicator used | . The datatype must correspond precisely to the data stored in the buffer. For this, MPI has predefined types that can be used. MPI has most of the usual C types. Furthermore, the standard has made provisions for creating and communicating user defined types as well. . Note also that MPI_Send returns an error value code. If this value is 0 (or the symbolic constant MPI_SUCCESS ), no error has occurred [3]. . . Remark . The default behaviour when a fatal error occurs in any of the participating processes is to abort the whole execution. In a sense, the default MPI behaviour when an error occurs is not fault tolerant. . . MPI_Recv has a very similar signature with MPI_Send. The exception is that there is no destination id parameter but the id of the process from the process receives. Note also that the buffer set aside must be at least as large as the number or elements expected to be received. . Specification of the sent/received datatype is required so that machines wiht different endianness or machines with different memory types (32-bit, 64-bit, 128-bit) to be able to communicate. . Simple example . Below is a simple example of how to use MPI_Send and MPI_Recv. You can also find the example here. . #include &lt;mpi.h&gt; #include &lt;iostream&gt; int main(int argc, char** argv){ int rank; int n_procs; // initialize MPI. No MPI calls // prior to this point should be made MPI_Init(&amp;argc, &amp;argv); // what&#39;s my rank MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); // how may procs MPI_Comm_size(MPI_COMM_WORLD, &amp;n_procs); MPI_Status status; if(rank == 0){ std::cout&lt;&lt;&quot;Hello from process &quot;&lt;&lt;rank&lt;&lt;&quot; of &quot;&lt;&lt;n_procs&lt;&lt;std::endl; int num = 2; // send a number to the worker MPI_Send(&amp;num, 1, MPI_INT, 1, 0, MPI_COMM_WORLD); // recv the answer int ans = -1; MPI_Recv(&amp;ans, 1, MPI_INT, 1, 1, MPI_COMM_WORLD, &amp;status); if(ans == 0){ std::cout&lt;&lt;&quot;Number &quot;&lt;&lt;num&lt;&lt;&quot; is odd&quot;&lt;&lt;std::endl; } else{ std::cout&lt;&lt;&quot;Number &quot;&lt;&lt;num&lt;&lt;&quot; is even&quot;&lt;&lt;std::endl; } } else if(rank == 1){ // receive int data = -1; MPI_Recv(&amp;data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &amp;status); if(data % 2 == 0){ data = 1; MPI_Send(&amp;data, 1, MPI_INT, 0, 1, MPI_COMM_WORLD); } else{ data = 0; MPI_Send(&amp;data, 1, MPI_INT, 0, 1, MPI_COMM_WORLD); } } MPI_Finalize(); // No MPI calls beyond this point return 0; } . Note the following . The tag can be any integer between 0-32767 | MPI Recv may use for the tag the wildcard MPI_ANY_TAG. This allows an MPI_Recv to receive from a send using any tag. | MPI_Send cannot use the wildcard MPI_ANY_TAG. A speciﬁc tag must be speciﬁed. | MPI_Recv may use for the source the wildcard MPI_ANY_SOURCE. This allows an MPI_Recv to receive from a send from any source. | MPI_Send must specify the process rank of the destination. No wildcard exists. | . Summary . In this section, we introduced the two most basic point-to-point communication functions available in MPI. Namly we saw, MPI_Send and MPI_Recv. Although these functions, and MPI in general, hide much of the boilerplate code needed so that two processes can communicate, still the resulting program is rather verbose. This is something that we would like to hide as much as possible both for application maintenance as well as for development and performance considerations. . References . MPI_Send | MPI_Recv | Gerassimos Barlas, Multicore and GPU Programming An Integrated Approach, Morgan Kaufmann |",
            "url": "https://pockerman.github.io/qubit_opus/programming/mpi/parallel-computing/c++/2021/07/07/mpi-basic-point-to-point-communication.html",
            "relUrl": "/programming/mpi/parallel-computing/c++/2021/07/07/mpi-basic-point-to-point-communication.html",
            "date": " • Jul 7, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Apache Spark. Application concepts",
            "content": "Overview . In this post I want to go over some Spark application concepts. These concepts allow us to better understand what Spark is doing behind the scenes when its executing our programs. The material in this post is taken from the excellent book Learning Spark. Lighting-fasts data analytics. Moreover, you can find a complete set of notes on Spark at Statistics, Probability &amp; Machine Learning Notes and code snippets at Computational Statistics with Scala. . Some application concepts for Apache Spark . Let&#39;s begin by introducing the following terms [1] . Application | SparkSession | Job | Stage | Task | . An application is a user&#39;s program that is built using Spark&#39;s APIs. It consists of a driver program and executors on the cluster. A job consists, possibly, of many stages that depend on each other. This dependency is organized into a Direct Acyclic Graph or DAG. Each node in the DAG represents a stage [1]. Furthermore, each stage is composed of tasks. A task is the unit of work in Spark that is executed by an executor. An executor is typically a computing node on the cluster that Spark is deployed. A computing node may be a multicore machine. Thus, each task is mapped to a single core and works on a single partition of the data. The following article Spark Basics : RDDs,Stages,Tasks and DAG describes nicely these concepts. The last term is SparkSession. It provides an entry point for interacting with the underlying functionality that Spark offers. . A SparkSession is either created automatically for us, this will be the case when using the shell, or the application needs to instantiate one. The following snippet shows this . import org.apache.spark.sql.SparkSession class MySparkApp{ def main(args: Array[String]) { ... val appName: String = &quot;MySparkApp&quot; val spark = SparkSession .builder() .appName(appName) .getOrCreate() ... } } . Note that there can be only one SparkSession per JVM. . Let&#39;s now turn into what operations can we execute. Operations on a distributed data set can be classified into two types; transformations and actions [1]. You can find more information on these two operations in RDD Programming Guide. Below we just give a brief overview of what each operation entails. . Transformations . Transformations in Spark transform a DataFrame into a new one. This is done without altering the original data. Hence a transformation is an immutable operation as far as the original data is concerned. Some examples of transformations are listed below . orderBy() | groupBy() | filter() | join() | . All transformations in Spark are evaluated lazily [1] (see Lazy evaluation). What this means is that their results are not computed immediately; instead a transformation in encoded as a lineage. This allows Spark to rearrange certain transformations, coalesce them or perform certain optimizations for more efficient execution (e.g. by joining or pipelining some operations and assign them to a stage) [1]. . Wide and narrow transformations . We can classify transformations according to the dependencies they has as transformations with narrow dependencies or transformations with wide dependencies [1]. A narrow transformation is a transformation that can be computed from a single input. In particular, a narrow transformation does not need to exchange data with other partitions (of the data) in order to compute the result. Wide transformations read data from other partitions in order to compute the result. groupBy or orderBy are two transformations that instruct Spark to perform wide transformations [1]. Evidently, we should avoid wide transformations if possible. . Actions . An action triggers the lazy evaluation of all the recorded transformations [1]. A list of actions is given below. . show() | take() | count() | collect() | . . Remark . Lazy evaluation allows Spark to optimize our queries. Lineage and data immutability allow for fault tolerance [1]. Since Spark records all transformations in its lineage and the DataFrames are immutable between the transformations, it can reproduce the origin data by simply replaying the recorded lineage [1]. . . Query plan . Both actions and transformations contribute to a query plan in Spark 1. Nothing in this plan is executed until an action is invoked [1]. . Summary . In this post we went briefly onto some of the basic but core concepts in Spark. Specifically, we saw what a job, a stage, and a task are. And how these are used to organize computation in Spark. Furthermore, we touched upon the SparkSession construct. This entity gives us access to all the functionality provided by Spark. Finally, we saw the two types of operations that we can apply of an RDD. Namely transformations and actions. We will come back to these topics as these an occurring theme when working with Spark. . In Apache Spark. Submit a self-contained Scala application we describe how to submit a standalone Scala application to Spark for execution. . References . Jules S. Damji, Brooke Wenig, Tathagata Das, Deny Lee, Learning Spark. Lighting-fasts data analytics, 2nd Edition, O&#39;Reilly. |",
            "url": "https://pockerman.github.io/qubit_opus/spark/scala/big-data/data-engineering/data-analysis/2021/07/06/spark-application-concepts.html",
            "relUrl": "/spark/scala/big-data/data-engineering/data-analysis/2021/07/06/spark-application-concepts.html",
            "date": " • Jul 6, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Machine Learning with Scala Logistic Regression",
            "content": "Overview . In the post Machine Learning with Scala Linear Regression we saw how to develop a simple linear regressor with the aide of the Breeze library. In this post, we see how to develop a logistic regressor classifier for two class classification. . Machine Learning with Scala Logistic Regression . Logistic regression is a linear classifier that is the decision boundary is a line or a hyperplane. The logistic regression algorithm is to a large extent similar to linear regression with two notable differences . We filter the result of the linear regression so that it is mapped in the range $[0, 1]$. Thus, the immediate output of logistic regression can be interpreted as a probability | The loss function that we minimize is not the MSE | . Other than that the algorithm is the same. Hence, we use a linear model of the form . $$ hat{y}_i = a x_i + b$$ . and we filter it via function so that the ouput is mapped bewteen $[0, 1]$. The sigmoid function . $$ phi(x) = frac{1}{1 + e^{-x}}$$ . can be used for such a filtering. . The loss function has the following form . $$L( mathbf{w}) = sum_{i}^N -y_i log( hat{y}_i) + (1 - y_i)(1 - log( hat{y}_i))$$ . where $ mathbf{w}$ is the parameters coefficients with $ mathbf{w} = [a, b]$. . We first import some useful packages . import breeze.linalg.{DenseMatrix, DenseVector} import breeze.linalg._ import breeze.numerics.{exp, log1p, sigmoid} import breeze.optimize.{DiffFunction, minimize} . We wrap the loss function and its gradient calculation into an object class . object LogisticRegression{ def L(x: DenseMatrix[Double], y: DenseVector[Double], parameters: DenseVector[Double]): Double = { val xBeta = x * parameters val expXBeta = exp(xBeta) val targets_time = y *:* xBeta -sum(targets_time - log1p(expXBeta)) } def gradL(x: DenseMatrix[Double], y: DenseVector[Double], parameters: DenseVector[Double]): DenseVector[Double]={ val xBeta = x * parameters val probs = sigmoid(xBeta) x.t * (probs - y) } } . This is the class that wraps the linear regression model. . class LogisticRegression { // The model parameters var parameters: DenseVector[Double] = null // Flag indicating if the interception term is used var useIntecept: Boolean=true; // auxiliary constructor def this(numFeatures: Int, useIntercept: Boolean=true){ this() init(numFeatures = numFeatures, useIntercept = useIntercept) } // initialize the underlying data def init(numFeatures: Int, useIntercept: Boolean=true): Unit = { val totalFeatures = if(useIntercept) numFeatures + 1 else numFeatures this.parameters = DenseVector.zeros[Double](totalFeatures) this.useIntecept = useIntercept } // train the model def train(x: DenseMatrix[Double], y: DenseVector[Double])={ // set up the optimization val f = new DiffFunction[DenseVector[Double]] { def calculate(parameters: DenseVector[Double]) = (LogisticRegression.L(x, y, parameters=parameters), LogisticRegression.gradL(x, y, parameters = parameters)) } this.parameters = minimize(f, this.parameters) } // predict the class of the given point def predict(x: DenseVector[Double]): Double = { require(parameters != null) if(!useIntecept){ require(x.size == parameters.size) sum(parameters * x) } else{ require(x.size == parameters.size -1 ) sum(parameters.slice(0, x.size) * x) + parameters(0) } } } . Let&#39;s put this into action with a simple example. . import breeze.linalg._ import breeze.numerics._ import breeze.optimize._ import breeze.stats._ import engine.models.LogisticRegression import engine.utils.{CSVDataSetLoader, VectorUtils} import spire.algebra.NormedVectorSpace.InnerProductSpaceIsNormedVectorSpace import spire.implicits.rightModuleOps object LogisticRegression_Exe extends App{ println(s&quot;Starting application: ${LogisticRegression_Exe.getClass.getName}&quot;) // load the data val data = CSVDataSetLoader.loadRepHeightWeightsFullData val recaledHeights = VectorUtils.standardize(data.heights); val rescaledWeights = VectorUtils.standardize(data.weights); val rescaledHeightsAsMatrix = recaledHeights.toDenseMatrix.t val rescaledWeightsAsMatrix = rescaledWeights.toDenseMatrix.t val featureMatrix = DenseMatrix.horzcat(DenseMatrix.ones[Double](rescaledHeightsAsMatrix.rows, 1), rescaledHeightsAsMatrix, rescaledWeightsAsMatrix) println(s&quot;Feature matrix shape (${featureMatrix.rows}, ${featureMatrix.cols})&quot;) val targets = data.genders.values.map{gender =&gt; if(gender == &#39;M&#39;) 1.0 else 0.0} println(s&quot;Targets vector shape (${targets.size}, )&quot;) // logistic regression model val lr = new LogisticRegression; // initialize the model lr.init(numFeatures=2) lr.train(x=featureMatrix, y=targets) val optimalParams = lr.parameters println(s&quot;Optimal parameters ${optimalParams}&quot;) println(&quot;Done...&quot;) } . You can find the complete example in this repo. . Summary . In this post we looked into how to develop a simple linear regression model with Scala. The Scala numerics library Breeze greatly simplifies the development. . References . Logistic regression | Pascal Bugnion, Patric R. Nicolas, Alex Kozlov, Scala: Applied Machine Learning |",
            "url": "https://pockerman.github.io/qubit_opus/machine-learning/scala/logistic-regression/2021/06/28/ml-with-scala-logistic-regression.html",
            "relUrl": "/machine-learning/scala/logistic-regression/2021/06/28/ml-with-scala-logistic-regression.html",
            "date": " • Jun 28, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Machine Learning with Scala Linear Regression",
            "content": "Overview . Python at the time of writing is the defacto language for prototyping and developing machine learning algorithms. In this post, we will be using Scala to develop a simple linear regressor model. We will do this with the help of the Scala numerics library Breeze. . Machine Learning with Scala Linear Regression . As it is well known, the linear regression model assumes the following functional form for the predictor $ hat{y}$ . $$ hat{y}_i = a x_i + b$$ . The loss function has the following form . $$L( mathbf{w}) = sum_{i}^N (y_i - hat{y}_i)^2 = sum_{i}^N (y_i - (a x_i + b))^2$$ . where $ mathbf{w}$ is the parameters coefficients with $ mathbf{w} = [a, b]$. The gradient of the loss function with respect to the parameters is as follows . $$ frac{ partial L}{ partial a} = -2 sum_{i}^N (y_i - hat{y}_i) x_i$$ . $$ frac{ partial L}{ partial b} = -2 sum_{i}^N (y_i - hat{y}_i)$$ . The term . $$SSE = sum_{i}^N (y_i - hat{y}_i)^2$$ . is called the sum of squared errors or SSE. If we divide with the number of training examples, $N$, then we get the so-called mean squared error or MSE . $$MSE = frac{1}{N} sum_{i}^N (y_i - hat{y}_i)^2$$ . We first import some useful packages . import breeze.linalg._ import breeze.optimize.{DiffFunction, minimize} . We wrap the loss function and its gradient calculation into an object class . object LinearRegression { def L(x: DenseMatrix[Double], y: DenseVector[Double], parameters: DenseVector[Double]): Double = { val yHat = x * parameters var value = 0.0 for( i &lt;- 0 until yHat.size){ val diff = y(i) - yHat(i) value += diff * diff } value } def gradL(x: DenseMatrix[Double], y: DenseVector[Double], parameters: DenseVector[Double]): DenseVector[Double]={ val yHat = x * parameters // we have as many components as columns val gradients = DenseVector.zeros[Double](x.cols) for( i &lt;- 0 until yHat.size){ var diff = y(i) - yHat(i) for( c &lt;- 0 until gradients.size){ diff *= x(i, c) gradients(c) += diff } } -2.0 * gradients } } . This is the class that wraps the linear regression model. . class LinearRegression{ // The model parameters var parameters: DenseVector[Double] = null // Flag indicating if the interception term is used var useIntecept: Boolean=true; // constructor def this(numFeatures: Int, useIntercept: Boolean=true){ this() init(numFeatures = numFeatures, useIntercept = useIntercept) } // train the model def train(x: DenseMatrix[Double], y: DenseVector[Double])={ // set up the optimization val f = new DiffFunction[DenseVector[Double]] { def calculate(parameters: DenseVector[Double]) = (LinearRegression.L(x, y, parameters=parameters), LinearRegression.gradL(x, y, parameters = parameters)) } this.parameters = minimize(f, this.parameters) } // the initialization function def init(numFeatures: Int, useIntercept: Boolean=true): Unit = { val totalFeatures = if(useIntercept) numFeatures + 1 else numFeatures this.parameters = DenseVector.zeros[Double](totalFeatures) this.useIntecept = useIntercept } } . Let&#39;s put this into action with a simple example. . object LinearRegressionExe_1 { def main(args: Array[String]):Unit={ // data set val x = LineDataSetLoader.lineSplit(0.0, 10.0, 100) System.out.println(&quot;Number of training examples: &quot; + x.size) val coeffs = Array[Double](1.0, 2.0) val poly = new Polynomial(coeffs) val y = poly.values(x) // the feature matrix val featureMatrix = DenseMatrix.horzcat(DenseMatrix.ones[Double](x.size, 1), x.toDenseMatrix.t) // model val model = new LinearRegression(numFeatures = 1, useIntercept = true) model.train(x=featureMatrix,y=y) println(s&quot;Polynomial coeffs ${poly.getCoeffsAsDenseVector}&quot;) println(s&quot;Linear regressor coeffs ${model.getParameters}&quot;) } } . You can find the complete example in this repo. . Summary . In this post we looked into how to develop a simple linear regression model with Scala. The Scala numerics library Breeze greatly simplifies the development. . References . Linear regression | Pascal Bugnion, Patric R. Nicolas, Alex Kozlov, Scala: Applied Machine Learning |",
            "url": "https://pockerman.github.io/qubit_opus/machine-learning/scala/linear-regression/2021/06/27/ml-with-scala-linear-regression.html",
            "relUrl": "/machine-learning/scala/linear-regression/2021/06/27/ml-with-scala-linear-regression.html",
            "date": " • Jun 27, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "MPI Object Communication 1",
            "content": "Overview . In object oriented code bases, data is typically organized into classes that wrap functionality, hide information and expose an API so that client code can utilize them. Thus, frequently, we end up in the situation where we have an object that we need to send across. MPI offers various posibilities to do so. In this post we will see MPI_Type_create_struct. . MPI Object Communication 1 . MPI communication functions such as MPI_Send/Recv need as an input the type of the data that is to be communicated [1]. When dealing with primitive types like integers and floats MPI has got us covered so there isn&#39;t much we should do. . However, frequently we want to communicate structures or objects. Sure, we can break up the structures that need to be communicated into individual elements or arrays of elements and send these in a series of send operations. However, this costly and rather counter productive; it breaks data encapsulation to start with. . Why it is costly, can be understood by considerin the so-called start-up latency [1]. This is the fixed cost we need to accept that includes the activation of multiple OS layers, the network interface, and so on [1]. The result is that although the actual over-the-wire times may be identical, the accumulation of the extra start-up latencies makes such an approach expensive to use. . MPI has two main mechanisms that we can use to communicate structures between heterogeneous machines [1] . MPI derived datatypes | Packing/unpacking data | . In this post, we will look into how to construct MPI derived datatypes using MPI_Type_create_struct and leave the second approach for another post. . Derived Datatypes . The memory layout of the same data structure differs from machine to machine. MPI, in order to successfully transfer and translate an instance of a structure from one machine to another, it requires the following information [1]: . The number and types of all the data members/fields. | The relative offset of the fields from the beginning of the structure (where to deposit data). | The total memory occupied by a structure, including any padding necessary to align it to specific boundaries. This is needed so that arrays of structures can be communicated. | . MPI provides utilities for describing the information above for a generatl datatype. Once a derived datatype is defined, a reference to this object can be used in any communication function that requires a datatype specification parameter [1]. . . Remark . Derived datatypes must be declared individually/locally in all the processes that will employ them [1]. . . Two of the most commonly used functions for creating derived datatypes are [1]: . MPI_Type_vector | MPI_Type_create_struct | . MPI_Type_vector is useful for extracting blocks of data from single or multidimensional arrays of a single datatype e.g. a vector. MPI_Type_create_struct is the most generic of the available functions, allowing the use of blocks made of different datatypes [1]. . Regardless of the approach used, each specification of a derived datatype must be followed by a call to the MPI_Type_commit function for having MPI store the specification internally. Once a datatype is committed, it can be used repeatedly in communication functions. MPI_Type_commit takes just a single parameter, which is a reference to the MPI_Datatype object [1]. . The following example shows how to use MPI_Type_create_struct. . Example . #include &lt;mpi.h&gt; #include &lt;iostream&gt; struct Point { unsigned int id; double x; double y; }; . As already mentioned MPI_Type_create_struct is rather involved so we group everything in the following function . void create_mpi_point( MPI_Datatype* t){ Point p; // the types the struct has MPI_Datatype types [3]; types[0] = MPI_UNSIGNED; types[1] = MPI_DOUBLE; types[2] = MPI_DOUBLE; // get the addresses MPI_Aint displ[3]; MPI_Aint off; MPI_Aint base; displ [0] = 0 ; MPI_Get_address (&amp;(p.id) , &amp;base ) ; MPI_Get_address (&amp;(p.x) , &amp;off ) ; displ [1] = off- base ; MPI_Get_address (&amp;(p.y) , &amp;off ) ; displ [2] = off - base; int blklen [3] = {1, 1, 1} ; // create the type MPI_Type_create_struct( 3 , blklen , displ , types , t); // commit it MPI_Type_commit ( t ) ; } . Here is the main function . int main(int argc, char** argv){ int rank; int n_procs; // initialize MPI. No MPI calls // prior to this point should be made MPI_Init(&amp;argc, &amp;argv); // what&#39;s my rank MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); // how may procs MPI_Comm_size(MPI_COMM_WORLD, &amp;n_procs); if(n_procs &gt; 2){ std::cout&lt;&lt;&quot;Application should be run with 2 processes.&quot;&lt;&lt;std::endl; MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE); } // status on the receive side MPI_Status status; // all processes must commit the Point type MPI_Datatype mpi_point_type; // create the mpi point create_mpi_point(&amp;mpi_point_type); if(rank == 0){ std::cout&lt;&lt;&quot;Hello from process &quot;&lt;&lt;rank&lt;&lt;&quot; of &quot;&lt;&lt;n_procs&lt;&lt;std::endl; Point p = {10, 0.5, 1.5}; std::cout&lt;&lt;&quot;Process &quot;&lt;&lt;rank&lt;&lt;&quot; sending point &quot; &lt;&lt;p.id &lt;&lt;&quot;, &quot; &lt;&lt;p.x &lt;&lt;&quot;, &quot; &lt;&lt;p.y &lt;&lt;std::endl; // send a number to the worker MPI_Send(&amp;p, 1, mpi_point_type, 1, 0, MPI_COMM_WORLD); } else if(rank == 1){ // receive Point p_recv; MPI_Recv(&amp;p_recv, 1, mpi_point_type, 0, 0, MPI_COMM_WORLD, &amp;status); std::cout&lt;&lt;&quot;Process &quot;&lt;&lt;rank&lt;&lt;&quot; received point &quot; &lt;&lt;p_recv.id &lt;&lt;&quot;, &quot; &lt;&lt;p_recv.x &lt;&lt;&quot;, &quot; &lt;&lt;p_recv.y &lt;&lt;std::endl; } MPI_Finalize(); // No MPI calls beyond this point return 0; } . Summary . In summary, this post breifly touched on the issued of communicating user defined datatypes with MPI. These are usually in the form of classes or structs. Although, we could align such types with primitive types and comunicate the ensuing arrays, this is not a viable approach due to start-up latency. Furthermore, it will certainly lead to an error prone and complex code base. . MPI provides various utilites in order to address such a situation. In this post, we saw MPI_Type_create_struct. This is the most generic of the available functions, allowing the use of blocks made of different datatypes [1]. . References . Gerassimos Barlas, Multicore and GPU Programming. An Integrated Approach. |",
            "url": "https://pockerman.github.io/qubit_opus/programming/mpi/parallel-computing/c++/2021/06/24/mpi-object-communication.html",
            "relUrl": "/programming/mpi/parallel-computing/c++/2021/06/24/mpi-object-communication.html",
            "date": " • Jun 24, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "Simple linear regression",
            "content": "Overview . The two dominant models in supervised learning are regression and classification modeling. In the former, the output of the model is a real value whilst for the latter its a class index. More generally, in regression modeling we associate an input vector $ mathbf{x} in mathbb{R}^N$ with a continuous variable $y in mathbb{R}$ whilst in classification modeling the vector $ mathbf{x}$ is associated with a $y in mathbb{N}$. There are various models we can use for regression modeling e.g. k-nearest neighbors or decision trees. In this section however, we restrict attention to a more humble approach and perhaps one of the most known statistical modeling approaches i.e. the linear regression model. You can find out more on this and other topics at Statistics, Probability and Machine Learning Notes. . Linear regression . In Statistics, linear regression represents a linear model of the relationship between a scalar target variable $y$ and one or more explanatory variables $x_1, dots, x_k $ [1]. The variables $y$ and $x_i$ are also known as dependent and independent variables respectively [1]. When only one explanatory variable is used the method is also called simple linear regression. When more than one explanatory variables are involved, then the process is called multiple linear regression [1]. . The generic regression function is given by [4] . begin{equation} r(x) = E left[Y|X=x right] = int y f(y|x) dy end{equation} One way to estimate $r$ is by using a data set. In this section we fix attention to parametric linear models. In particular, a linear regression model using $k$ features has the following form . $$y = w_0 + w_1x_1+ dots + w_{k}x_k + epsilon$$ . Note that the term linear refers to the weights and not to the functional form of the features. . . Remark . Given what we said above, the following is also under the umbrella of the linear regression model . $$y = w_0 + w_1x_{1}^2+ dots + w_{k}x_{k}^2 + epsilon$$ . although this is not a linear model with respect to the features. . . $ epsilon$ represents some error term with $E left[ epsilon |X right]=0$ and $Var left[ epsilon |X right]= sigma^2$. Notice that we can say something about the error of the model given the data $X$. . . Generalized linear models . Linear regression falls under the umbrella of generalized linear models. A GLM models the relationship between $ mathbf{x} in mathbb{R}^N$ and $y$ as . $$y = mathbf{w}^T mathbf{x} + epsilon$$ . . The term $w_0$ is the bias or interception term. The rest of the weights represent the amount that $y$ changes when the variable associated with that specific weight changes given that the rest of the variables remain the same. . Given the assumed functional form among the independent variables, we need to estimate the weights $w_i$. This is done by fitting the model in the available data. The following image shows a line fitted in a one dimensional data set . Figure 1. Linear regression model. Image from [1]. . Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications [1]. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine [1]. . Fitting the model . We now come to the question how can we fit the model above in the data? Linear regression models are often fitted using the least squares approach. However, they may also be fitted in other ways, such as by minimizing the &quot;lack of fit&quot; in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty) [1]. Let&#39;s describe how the least squares method works . Least Squares . Perhaps the most common way to estimate the parameters of a statistical model is compute the maximum likelihood estimate or MLE [2]. Given a model on a data set $ mathbf{X}_{n times m}$ and parameters $ mathbf{w}$, the MLE estimate is defined as [2] . $$ mathbf{ hat{w}} = argmax_{ mathbf{w}} log p( mathbf{X}| mathbf{w})$$ . Let&#39;s consider the simple one dimensional model . $$ hat{y}_i = w_0 + w_1x_i + epsilon_i$$ . For each observation, we commit an error by using the model above. Let $ hat{y}_i$ be the model prediction and $y_i$ be the actual value. We define the error associated with $ hat{y}_i$ as $ epsilon_i$ . $$ epsilon_i = hat{y}_i - y_i$$ . The total error then will be . $$ epsilon = sum_i epsilon_i$$ . Ordinary least squares minimize the sum of the squared errors i.e. . $$ L = sum_i epsilon_{i}^{2}$$ . . Remark . Why do we take a square of the errors? The reason is that if we do not take the squares of the errors, the positive and negative terms can cancel each other. . . The solution to this minimization problem is called ordinary least-squares solution and it is given by the following expression [2]: . . Ordinary least squares solution . $$ mathbf{ hat{w}}_{OLS} = left( mathbf{X}^T mathbf{X} right)^{-1} mathbf{X}^T mathbf{y}$$ . The equations implied by the solution above i.e. . $$ mathbf{X}^T mathbf{X} mathbf{ hat{w}} = mathbf{X}^T mathbf{y}$$ . are known as the normal equations. . . This solution is unique, provided that the matrix $ mathbf{X}^T mathbf{X}$ is invertible which means that $det( mathbf{X}^T mathbf{X})$ should not be equal to zero. This is always possible if the matrix $ mathbf{X}$ representing the data set, has full rank with respect to the columns. specifically, if $rank( mathbf{X}) = m$, then the features are linearly independent and thus $ mathbf{X}^T mathbf{X}$ has no columns or rows that are proportional to others (something that would lead to $det( mathbf{X}^T mathbf{X})=0)$. Another viewpoint about uniqueness is provided by the fact that the loss function $L$ is a quadratic function thus is should have a global minimum point. . Thus the $ hat{ mathbf{w}}_{OLS}$ is the least squares estimate that minimizes $L$. . . Least squares estimate and MLE . Above we assumed that $ epsilon sim N(0, sigma^2)$. In this case, the least squares estimator is also the maximum likelihood estimator [4]. . . Example . Let&#39;s see an example using scikit-learn. . %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns; sns.set() import numpy as np from sklearn.metrics import r2_score from sklearn.linear_model import LinearRegression . rng = np.random.RandomState(1) x = 10 * rng.rand(50) y = 2 * x - 5 + rng.randn(50) plt.scatter(x, y); . model = LinearRegression(fit_intercept=True) model.fit(x[:, np.newaxis], y) xfit = np.linspace(0, 10, 50) yfit = model.predict(xfit[:, np.newaxis]) plt.scatter(x, y) plt.plot(xfit, yfit); . print(&quot;Model slope: &quot;, model.coef_[0]) print(&quot;Model intercept:&quot;, model.intercept_) . Model slope: 2.0272088103606953 Model intercept: -4.998577085553204 . The coefficient of determination, see the section Evaluating the model, is . print(&quot;Model r^2={0}&quot;.format(r2_score(y, yfit))) . Model r^2=-0.58735989521677 . Let&#39;s verify that the loss function is indeed quadratic. We do so without accounting for the bias term. . intersepts = np.linspace(-6.0, 6.0, 1000) loss = [] for w in intersepts: yfit = w * xfit loss.append(sum((y - yfit)**2)) plt.scatter(intersepts, loss) plt.xlabel(&#39;w&#39;) plt.ylabel(&#39;L&#39;) plt.show() . Evaluating the model . Ok we fit the model to the data but how good is that fit? In other words, we need some way(s) to assess the goodness of fit of the model. One such metric, is the **coefficient of determination** $r^2$ [3]. . The coefficient of determination indicates the percent of variance in $y$ that is explained or accounted for by the independent variable $x$. For linear regression involving just one predictor or independent variable, the coefficient of determination $r^2$ is just the Pearson correlation coefficient between $y$ and $x$ [3] i.e. . $$r^2 = ( rho(y,x))^2$$ . The value of $r^2$ ranges between 0 and 1. . A value of $r^2=0$ means that there is no fit at all between the model and the data, i.e. that no percentage of the variance in $y$ is explained by $x$. This means that knowing $x$, and using a linear model, tells us nothing about $y$ [3]. . A value of $r^2=1$ means a perfect fit of the model and the data and $100 %$ of the variance in $y$ is explained by $x$ or if we know $x$ then we fully know $y$. . . Remark . The coefficient of determination belongs to a family of statistics called variance accounted for indices. Other notable members of this family are the $R^2$ coefficient for multiple regression and the $ eta^2$ coefficient from ANOVA [3]. When we see any member of this family, we should think percent improvement in predictive accuracy [3]. . . We are also interested in the expected value, the variance as well as confidence intervals for the obtained least squares estimates. These are given below, see also [4], . begin{equation} E left[ hat{ mathbf{w}}_{OLS}|X right] = mathbf{w} end{equation} i.e. $ hat{ mathbf{w}}_{OLS}$ is an unbiased estimate of $ mathbf{w}$. The variance of the estimate is [4] . begin{equation} Var left[ hat{ mathbf{w}}_{OLS}|X right] = frac{ sigma^2}{Ns^{2}_{X}} begin{pmatrix} frac{1}{N} sum x_{i}^2 &amp; - bar{x} - bar{x} &amp; 1 end{pmatrix} end{equation} Model assumptions . Mathematical models, being simplifications of phenomena, make certain assumptions and linear regression is not an exception. In this subsection we want to go over these assumptions. The assumptions below are taken from [3] and they should be met whenever we want to use bivariate regression for modeling purposes. These are . Linearity | Independence | Homogeneity of variance | Normality | Bivariate normality | . Let&#39;s briefly explain what each of these assumptions means in practice. Linearity means that $y$ should be a linear function of $x$ or at least a linear model is a good approximation of their dependence. Pearson&#39;s coefficient $ rho$ and a scatter plot can tells us quantitatively and qualitatively whether this is the case or not. Independence, means that each observation $x_i$ should be drawn independently from the population of interest. For example, the researcher should take repeated measures on the same variable from the same participant [3]. Homogeneity of variance (or homoscedasticity) means that the variance of the dependent variable $Y$ should remain fairly constant at all values of $X$. The normality assumption refers to how the residuals are distributed. Specifically, these should be normally distributed [3]. However, violation of this assumption is only serious with relatively small samples. The final assumption is bivariate normality assumption. This refers to the fact that any specific score on one of the variables, scores on the other variable should follow the normal distribution. . Summary . In this section, we reviewed the linear regression model. . References . Linear regression model | Kevin P. Murphy, Machine Learning A Probabilistic Perspective, The MIT Press. | Larry Hatcher, Advanced statistics in research, Shadow Finch Media. | Larry Wasserman, All of Statistics: A concise course in statistical inference, Springer. |",
            "url": "https://pockerman.github.io/qubit_opus/statistics/statistical-modeling/data-analysis/supervised-learning/machine-learning/linear-regression/2021/06/14/linear-regression.html",
            "relUrl": "/statistics/statistical-modeling/data-analysis/supervised-learning/machine-learning/linear-regression/2021/06/14/linear-regression.html",
            "date": " • Jun 14, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "MPI Hello World",
            "content": "Overview . The Message Passage Interface, or MPI for short, is perhaps the defacto standard used in nowadays scientific distributed computing. It provides interfaces for both point-to-point and collective communication. In this series, we will go over basic and intermediate usage of the MPI standard. . MPI Hello World . MPI is a well-established standard that includes [1]: . Process creation | Process management | Point-to-point communication | Collective communication | One-sided communication | External interfaces | Topologies | . Language bindings exist for C and Fortran. Newer MPI standards are trying to better support the scalability in future extreme-scale computing systems, because currently, the only feasible option for increasingthe computing power is to increase the number of cooperating processors [1]. . The following code snippet is the Hello World equivalent for MPI. It demonstrates basic usage of the standard. . Code . // example_1.cpp #include &lt;mpi.h&gt; #include &lt;iostream&gt; int main(int argc, char** argv){ int rank; int n_procs; // initialize MPI. No MPI calls // prior to this point should be made MPI_Init(&amp;argc, &amp;argv); // what&#39;s my rank MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); // how may procs MPI_Comm_size(MPI_COMM_WORLD, &amp;n_procs); std::cout&lt;&lt;&quot;Hello from process &quot;&lt;&lt;rank&lt;&lt;&quot; of &quot;&lt;&lt;n_procs&lt;&lt;std::endl; MPI_Finalize(); // No MPI calls beyond this point } . We can compile the code by using the following . mpicxx example_1.cpp -o example_1 . Executing using four processes . mpirun -np 4 example_1 . produces . Hello from process 0 of 4 Hello from process 1 of 4 Hello from process 3 of 4 Hello from process 2 of 4 . . Remark . The mpirun command is also known as mpiexec on some implementations. It makesl that copies of the executable can be run on every machine. . . A few things to notice are: . In order to access any MPI related functionality we need to inlude the header file mpi.h` | . MPI related functions start with the prefix MPI_ | . As commented in the code, all MPI related calls should occur within MPI_Init/MPI_Finalize. | . In the code above, every process executes the same instructions. We can use if/else statements to differentiate what each process will execute. The variable MPI_COMM_WORLD is a predefined intra-communicator, i.e., it serves communications taking place between processes belonging to its own group of processes [1]. . The code above also calculates the rank of the calling process via MPI_Comm_rank, and the total number of processes in the comunicator MPI_Comm_size. . References . Roman Trobec et al., Introduction to parallel computing. From algorithms to programming on state-of-the-art platforms, Springer. |",
            "url": "https://pockerman.github.io/qubit_opus/programming/mpi/parallel-computing/c++/2021/06/13/mpi-hello-world.html",
            "relUrl": "/programming/mpi/parallel-computing/c++/2021/06/13/mpi-hello-world.html",
            "date": " • Jun 13, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "Value Iteration With Scala 2",
            "content": "Overview . In a previous post we saw how to implement the value iteration algorithm in Scala. This is particularly easy algorithm to implement. In that post, we used a dummy environment to check on the algorithm. In this post we extend our implementation by integrating ScalaPy in our implementation. This way we can interact with OpenAI Gym library which is written in Python. However, this way we have access to various environments to test our reinforcement learning algorithms. . Value iteration . Recall that one drawback of policy iteration is that each of its iterations involves a policy evaluation. This, however, may itself be an iterative computation; therefore requiring multiple sweeps through the state set [1]. Furthermore, if the evaluation is done iteratively, then convergence to $V_{ pi}$ occurs only in the limit [1]. . Luckily, the policy evaluation step of policy iteration can truncated without loosing the convergence gurantees of the method. Moreover, this can be done in several different ways [1]. . In particular, when policy evaluation is stopped after just one update of each state, the algorithm is called value iteration. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps [1] . $$V_{k+1}(s) = max_{ alpha} sum_{s^*, r}p(s^*, r | s, alpha) left[r + gamma V_{k}(s^*) right], ~~ forall s in mathbb{S}$$ . Figure 1: Value iteration algorithm. Image from [1]. . The value iteration is obtained simply by turning the Bellman optimality equation into an update rule [1]. It requires the maximum to be taken over all the actions. Furthermore, the algorithm terminates by checking the amount of change of the value function. . Code . OpenAI Gym provides a large collection of environments to test on reinforcement learning algorithms. The library is written in Python. Hence, we cannot use here as is. Fortunately, ScalaPy allows us to use Python libraries from Scala code. Check the ScalaPy how to install it on your machine. . Let&#39;s see how to implement a simple wrapper over the FrozenLake-v0 environment. . import scala.collection.mutable.ArrayBuffer import scala.util.control.Breaks._ import scala.math.max . import scala.collection.mutable.ArrayBuffer import scala.util.control.Breaks._ import scala.math.max . package engine.worlds import me.shadaj.scalapy.py class FrozenLake(val version: String) { // load the gym module private val gym = py.module(&quot;gym&quot;) private var env: me.shadaj.scalapy.py.Dynamic = null // Make the environment def make: Unit = this.env = gym.make(this.name) // Reset the environment def reset: Int = { val state = this.env.reset() state.as[Int] } // Returns the name of the environment def name: String = {&quot;FrozenLake-&quot; + version} // Returns the number of states def nStates: Int = this.env.observation_space.n.as[Int] // Returns the number of actions def nActions: Int = this.env.action_space.n.as[Int]; // def getDynamics(state: Int, action: Int): Seq[(Double, Int, Double, Boolean)] = { val P = py.Dynamic.global.dict(this.env.P) val dynamicsTuple = P.bracketAccess(state) val result = dynamicsTuple.bracketAccess(action).as[Seq[Tuple4[Double, Int,Double, Boolean]]] result } } . This is very simple wrapper suitable though for our needs. It hides all the boilerplate code we need to have in order to interface with OpenAI Gym. Let&#39;s also change the implementation of the ValueIteration class. The class has two implementation depending on the trainMode value defined below . package engine.rl object TrainMode extends Enumeration { val DEFAULT = Value(0, name = &quot;DEFAULT&quot;) val STOCHASTIC = Value(1, name = &quot;STOCHASTIC&quot;) } . The STOCHASTIC mode walks in the environment by randomly selecting an action. The DEFAULT implements the algorithm as we implemented in the previous related post. . package engine.rl import scala.collection.mutable import breeze.linalg.{DenseVector, max} import engine.worlds.DiscreteEnvironment import scala.util.control.Breaks.{break, breakable} class ValueIteration(env: DiscreteEnvironment, gamma: Double, maxIterations: Int, tolerance: Double, trainMode: TrainMode.Value=TrainMode.DEFAULT) { val rewards = new mutable.HashMap[Tuple3[Int, Int, Int], Double]() val transits = new mutable.HashMap[Tuple3[Int, Int, Int], Int] var stateValues = DenseVector.zeros[Double](env.nStates) var residual = 1.0 def train: Unit = { this.rewards.clear() this.transits.clear() this.stateValues = DenseVector.zeros[Double](env.nStates) breakable { for(itr &lt;- Range(0, maxIterations)){ println(&quot;&gt; Learning iteration &quot; + itr) println(&quot;&gt; Learning residual &quot; + residual) step if(residual &lt; tolerance) break; } } } def step: Unit ={ // stop condition var delta = 0.0 // update each state for( s &lt;- 0 until env.nStates){ // Do a one-step lookahead to find the best action val value = oneStepLookahead(state=s) val bestActionValue = breeze.linalg.max(value) delta = math.max(delta, math.abs(bestActionValue - stateValues(s))) stateValues(s) = bestActionValue } residual = delta } def defaultStep: Unit = { } def stochasticStep: Unit = { } protected def oneStepLookahead(state: Int): DenseVector[Double] = { val values = DenseVector.zeros[Double](env.nActions) for(action &lt;- 0 until env.nActions) { val dynamics = env.getDynamics(state = state, action = action) for(item &lt;- dynamics.indices){ val prob = dynamics(item)._1 val next_state = dynamics(item)._2 val reward = dynamics(item)._3 values(action) += prob * (reward + gamma * stateValues(next_state)) if(rewards.contains((state, action, next_state))){ rewards.update((state, action, next_state), reward) transits.update((state, action, next_state), 1) } else{ rewards.addOne((state, action, next_state), reward) transits.addOne((state, action, next_state), 1) } } } values } } . References . Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. |",
            "url": "https://pockerman.github.io/qubit_opus/scala/reinforcement-learning/algorithms/dynamic-programming/2021/06/06/scala-value-iteration-2.html",
            "relUrl": "/scala/reinforcement-learning/algorithms/dynamic-programming/2021/06/06/scala-value-iteration-2.html",
            "date": " • Jun 6, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "Scala Inheritance 2",
            "content": "Scala inheritance 2 . As in Java or C++, you can declare a ﬁeld or method as protected . Such a member is accessible from any subclass, but not from other locations. Unlike in Java, a protected member is not visible throughout the package to which the class belongs. (If you want this visibility, you can use a package modiﬁer). . class Element{ protected def speak = println(&quot;Hi from element&quot;) } . defined class Element . Recall that a class has one primary constructor and any number of auxiliary constructors, and that all auxiliary constructors must start with a call to a preceding auxiliary constructor or the primary constructor. As a consequence, an auxiliary constructor can never invoke a superclass constructor directly. . The auxiliary constructors of the subclass eventually call the primary constructor of the subclass. Only the primary constructor can call a superclass constructor. Recall that the primary constructor is intertwined with the class deﬁnition. The call to the superclass constructor is similarly intertwined. Here is an example . Overriding fields . A ﬁeld in Scala consists of a private ﬁeld and accessor/mutator methods. You can override a val (or a parameterless def ) with another val ﬁeld of the same name. The subclass has a private ﬁeld and a public getter, and the getter overrides the superclass getter (or method) . class Item(name: String){ override def toString = getClass.getName + &quot; &quot; + name } . defined class Item . class Milk extends Item(&quot;Milk&quot;) . defined class Milk . val milk = new Milk println(milk.toString) . ammonite.$sess.cmd2$Helper$Milk Milk . milk: Milk = ammonite.$sess.cmd2$Helper$Milk Milk . class Bullet extends Item(&quot;Bullet&quot;){ override val name =&quot;Flower&quot; } . cmd4.sc:2: value name overrides nothing override val name =&#34;Flower&#34; ^Compilation Failed . Compilation Failed . A def can only override another def . | A val can only override another val or a parameterless def . | A var can only override an abstract var | . Anonymous subclasses . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/inheritance/programming/oop/2021/05/24/scala-inheritance-2.html",
            "relUrl": "/scala/inheritance/programming/oop/2021/05/24/scala-inheritance-2.html",
            "date": " • May 24, 2021"
        }
        
    
  
    
        ,"post31": {
            "title": "Scala Inheritance 1",
            "content": "Overview . Inheritance is a cornerstone of OOP and Scala has support for this concept. A Scala class can extend one and only one other class. This is similar to Java. We can do this using the extends keyword. . Inheritance 1 . Class inheritance is the common way for achiving polymorphism. A Scala class can extend one and only one other class. This is similar to Java. We can do this using the extends keyword. . class Element(id: Int){ } . defined class Element . class Triangle(id: Int) extends Element(id){ def elementType: String = &quot;TRI&quot; } . defined class Triangle . Similar to Java, we can declare a class final so that it cannot be extended. However, unlike Java, you can also declare individual methods or fields final so that they cannot be overridden [1]. . Overriding methods . Child classes can customize the behaviour of methods from the parent classes (provided of course these are not decaled final). This however should be told explicitly using the override modifier . class Element(id: Int){ def elementType: String = &quot;INVALID&quot;; } . defined class Element . class Triangle(id: Int) extends Element(id){ def elementType: String = &quot;TRI&quot; } . cmd3.sc:2: `override` modifier required to override concrete member: def elementType: String (defined in class Element) def elementType: String = &#34;TRI&#34; ^Compilation Failed . Compilation Failed . class Triangle(id: Int) extends Element(id){ override def elementType: String = &quot;TRI&quot; } . defined class Triangle . val tri = new Triangle(10) . tri: Triangle = ammonite.$sess.cmd3$Helper$Triangle@1266c3c3 . print(tri.elementType) . TRI . This requirement can potentially be very useful [1]: . When we misspell the name of the method that we are overriding | When we accidentally provide a wrong parameter type in the overriding method | When we introduce a new method in a superclass that clashes with a subclass method | . As a final note, we can invoke a superclass method in Scala works in the same way we do in Java that is using the keyword super . class TriQuad(id: Int) extends Triangle(id){ override def elementType: String = super.elementType + &quot;_QUAD&quot; } . defined class TriQuad . val quad = new TriQuad(10) . quad: TriQuad = ammonite.$sess.cmd6$Helper$TriQuad@69fd55e1 . print(quad.elementType) . TRI_QUAD . isInstanceOf . Frequently, we want to know is an object is an instance of a class. To do so we can use the isInstanceOf method [1] . val q1 = new TriQuad(1) . q1: TriQuad = ammonite.$sess.cmd6$Helper$TriQuad@31d67ba6 . if(q1.isInstanceOf[TriQuad]) println(&quot;This is TRI_QUAD&quot;) . This is TRI_QUAD . The q1.isInstanceOf[TriQuad] will succeed if q1 refers to an object of class TriQuad. Since TriQuad is a subclass of Element the following also succeeds . if(q1.isInstanceOf[Element]) println(&quot;This is TRI_QUAD&quot;) . This is TRI_QUAD . inst.isInstanceOf[Type] returns false if inst is null: . val q2 = null . q2: Null = null . if(q2.isInstanceOf[Element]) println(&quot;This is TRI_QUAD&quot;) else println(&quot;Object is null&quot;) . Object is null . We can further use the asInstanceOf to convert a reference to a subclass reference: . val q2 = q1.asInstanceOf[Element] . q2: Element = ammonite.$sess.cmd6$Helper$TriQuad@31d67ba6 . print(q2.elementType) . TRI_QUAD . If q2 is not an Element , then we get an exception . class SomeClass{} . defined class SomeClass . val q2 = new SomeClass . q2: SomeClass = ammonite.$sess.cmd16$Helper$SomeClass@3f85fd8 . val q3 = q2.asInstanceOf[Element] . java.lang.ClassCastException: class ammonite.$sess.cmd16$Helper$SomeClass cannot be cast to class ammonite.$sess.cmd2$Helper$Element (ammonite.$sess.cmd16$Helper$SomeClass and ammonite.$sess.cmd2$Helper$Element are in unnamed module of loader ammonite.runtime.SpecialClassLoader @7bd4937b) ammonite.$sess.cmd18$Helper.&lt;init&gt;(cmd18.sc:1) ammonite.$sess.cmd18$.&lt;clinit&gt;(cmd18.sc:7) . If we want to test whether an instance refers to a specific class, but not a subclass, we can use classOf [1] . if (p.getClass == classOf[Element]) . The classOf method is defined in the scala.Predef object that is always imported. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/inheritance/programming/oop/2021/05/17/scala-inheritance-1.html",
            "relUrl": "/scala/inheritance/programming/oop/2021/05/17/scala-inheritance-1.html",
            "date": " • May 17, 2021"
        }
        
    
  
    
        ,"post32": {
            "title": "Prototypical Networks",
            "content": "",
            "url": "https://pockerman.github.io/qubit_opus/2021/05/17/prototypical-networks.html",
            "relUrl": "/2021/05/17/prototypical-networks.html",
            "date": " • May 17, 2021"
        }
        
    
  
    
        ,"post33": {
            "title": "Scala Enumerations",
            "content": "Overview . Enumerations can be very useful when we want to create discrete set of items. Often these items help us to differentiate a run time instances having the same base class. Scala provides an Enumeration helper class that we can use to create enumerations[1]. . Enumerations . Scala does not have enumerated types [1]. In contrast, it provides the Enumeration helper class to help us create enumerations. This is shown in the code snippet below . object Element extends Enumeration{ val QUAD, TRI, HEX, TET = Value } . defined object Element . Above we defined an enumerated type with four fields. The above initialization is equivalent to [1] . ... val QUAD = Value val TRI = Value val HEX = Value val TET = Value ... . println(Element.QUAD) println(Element.TRI) . QUAD TRI . Each call to the Value method returns a new instance of an inner class, also called Value [1]. We can also initialize the enumeration fields with ids, names or both as shown below . object Element_2 extends Enumeration{ val QUAD = Value(0, &quot;QUAD&quot;) val TRI = Value(1, &quot;TRI3&quot;) } . defined object Element_2 . println(Element_2.QUAD) println(Element_2.TRI) . QUAD TRI3 . If not specified, the id is one more than the previously assigned one, starting with zero and the default name is the field name [1]. . Note that the type of the enumeration is Element.Value and not just Element. The latter is just the type of the object holding the values. We can use aliases to disambiguate this [1] . object Element_3 extends Enumeration{ type Element_3 = Value val QUAD = Value(0, &quot;QUAD&quot;) val TRI = Value(1, &quot;TRI3&quot;) } . defined object Element_3 . Now the type of the enumeration is Element_3.Element_3 [1]. . for( e &lt;- Element_3.values) println(e.id + &quot;:&quot; + e) . 0:QUAD 1:TRI3 . Finally, you can look up an enumeration value by its id or name [1]. Both of the following yield the object Element.HEX : . println(Element(2)) println(Element.withName(&quot;HEX&quot;)) . HEX HEX . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/enumerations/programming/2021/05/04/scala-enumerations.html",
            "relUrl": "/scala/enumerations/programming/2021/05/04/scala-enumerations.html",
            "date": " • May 4, 2021"
        }
        
    
  
    
        ,"post34": {
            "title": "Testing for Controllability",
            "content": "Overview . We have introduced controllability and observability for linear time-invariant systems. We saw the conditions for such a system is controllalble and observable. Now we turn to the question how can we test is controllable and observable. . Testing for controllability . Recall that we deal with linear systems of the form . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . We can understand whether the linear system above is controllable or not by examing the controllability matrix $ mathbf{ cal{C}}$. In particular, the column space of that matrix. This matrix is defined as follows . $$ mathbf{ cal{C}} = begin{bmatrix} mathbf{B} &amp;&amp; mathbf{AB} &amp;&amp; mathbf{A}^2 mathbf{B} &amp;&amp; dots &amp;&amp; mathbf{A}^{n-1} mathbf{B} end{bmatrix}$$ . Where $n$ is the number of state variables. If the controllability matrix has $n$ linearly independent columns, then the system under consideration is controllable [1]. Note that this does not mean that the columns of $ mathbf{ cal{C}}$ should be linearly independent. All that we require, is that we can find $n$ linearly independent columns (see example 2 below). Let&#39;s see two examples taken from [1]. . The Popov-Belevich-Hautus or PBH is one of the most useful tests to determine whether or not a system is contollable [1]. The test says that the pair $( mathbf{A}, mathbf{B})$ is controllable if and only if the column rank of the matrix . $$ begin{bmatrix} mathbf{A} - lambda mathbf{I} &amp;&amp; mathbf{B} end{bmatrix}$$ . is equal to $n$ $ forall lambda in mathbb{C}$. Indeed the rank of $ mathbf{A} - lambda mathbf{I}$ is $n$ only when $ lambda$ is an eigenvalue of $ mathbf{A}$ [1]. Given that the $ mathbf{A} - lambda mathbf{I}$ is only rank deficient for the eigenvalues $ lambda$, then the kernel or null-space of $ mathbf{A} - lambda mathbf{I}$ is given by the span of the eigenvectors corresponding to $ lambda$ [1]. Therefore, for the matrix . $$ begin{bmatrix} mathbf{A} - lambda mathbf{I} &amp;&amp; mathbf{B} end{bmatrix}$$ . to have rank $n$, the columns in $ mathbf{B}$ must have some component in each of the eigenvector directions of $ mathbf{A}$ so that to complment the null space $ mathbf{A} - lambda mathbf{I}$ [1] . If $ mathbf{A}$ has $n$ distinct eigenvalues, then the system will be controllable with a single actuation input. In this call the matrix $ mathbf{A} - lambda mathbf{I}$ has at most one eigenvector direction in the null-space [1]. We can choose $ mathbf{B}$ as the sum of all $n$ linearly independent eigenvectors. This will guarantee to have some component in each direction. . Obviously, cases exist where we have degenerate eigenvalues with multiplicity greater than 2. The actuation input i.e. matrix $ mathbf{B}$ must then have multiple columns [1]. One more case where more it may be helpful to have multiple actuators is when we need better control of the system or when dealing with systes with large transient growth [1]. . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/robotics/autonomous-vehicles/controllability/dynamical-systems/controllability-test/2021/05/02/testing-for-controllability.html",
            "relUrl": "/robotics/autonomous-vehicles/controllability/dynamical-systems/controllability-test/2021/05/02/testing-for-controllability.html",
            "date": " • May 2, 2021"
        }
        
    
  
    
        ,"post35": {
            "title": "The Scala apply Method",
            "content": "Overview . The apply method is called in expression of the form Object(arg1,...,argN) [1]. . The apply method . We have seen that we can write both experssions . val arr1 = new Array[Int](10) . arr1: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) . val arr2 = Array(10) . arr2: Array[Int] = Array(10) . The first expression creates an Array of length 10. In this expression the constructor is called. In contrast, in the second expression, the apply method is called. And an Array instance is created having length one and value . println(arr2(0)) . 10 . Not having the new keyword is handy for nested expressions, like the one below . val arr3 = Array(Array(1, 7), Array(2, 9)) . arr3: Array[Array[Int]] = Array(Array(1, 7), Array(2, 9)) . In order to be able to use expressions like the above, the apply method must be defined [1]. We can do this as shown below [1] . class MyCls (val idx: Int, val value: Double){ } object MyCls{ def apply(idx: Int, value: Double) = new MyCls(idx, value) } . defined class MyCls defined object MyCls . val cls = MyCls(1, 20) . cls: MyCls = ammonite.$sess.cmd5$Helper$MyCls@5e0ad6c6 . println(cls.idx) println(cls.value) . 1 20.0 . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/apply-method/programming/2021/04/30/scala-apply-method.html",
            "relUrl": "/scala/classes/apply-method/programming/2021/04/30/scala-apply-method.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post36": {
            "title": "More On Scala object",
            "content": "Overview . We have seen how we can create singletons and companion objects using object. In this notebook we introduce more things we can do with object. . More on Scala object . An object can extend one class. However, it can extend one or more traits [1]. This results in an object that has all of the features specified in the object definition [1]. One utilization of this pattern is to specify default objects as shown below. . abstract class Element(val idx: Int){ def nFaces: Int; def nVertices: Int; } . defined class Element . object DummyElement extends Element(-1){ override def nFaces: Int = -1 override def nVertices: Int = -1 } . defined object DummyElement . Whenever we want to use an Element that makes no sense but anyway it is needed we can use DummyElement. . Application objects . Just like Java and C++, a Scala application starts with a main method which has the following signature [1] . def main(args: Array[String]): Unit . We can wrap that in a companion object . class Hello{ def showMsg() = println(&quot;Hello...&quot;) } object Hello{ def main(args: Array[String]){ val msg = new Hello msg.showMsg() } } . defined class Hello defined object Hello . Note that we can also extend the App trait and place the program code into the constructor body [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/companion-object/programming/2021/04/29/scala-object.html",
            "relUrl": "/scala/classes/companion-object/programming/2021/04/29/scala-object.html",
            "date": " • Apr 29, 2021"
        }
        
    
  
    
        ,"post37": {
            "title": "Scala Companion Objects",
            "content": "Overview . Scala does not support static functions. We saw that object classes can be used to implement patterns like the singleton pattern. Moreover, often it make more sense that a function is a class function. We can do this using companion objects. . Companion objects . A companion object has the same name as the class it refers to. It is implemented using the object keyword . object MyClass{ private var currentIndex = 0 def getNewIndex : Int = {currentIndex +=1; currentIndex} } . defined object MyClass . class MyClass{ val id = MyClass.getNewIndex def getIdx: Int = id } . defined class MyClass . val cls = new MyClass println(cls.getIdx) . 1 . cls: MyClass = ammonite.$sess.cmd5$Helper$MyClass@38c61f45 . Both the class and its companion object can access each other’s private features. Furthermore, they must be located in the same source file [1]. . Note that the companion object of a class is accessible, but it is not in scope [1]. This means that in the example above we need to use MyClass.getNewIndex and not just getNewIndex to invoke the method of the companion object [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/companion-object/programming/2021/04/27/scala-companion-objects.html",
            "relUrl": "/scala/classes/companion-object/programming/2021/04/27/scala-companion-objects.html",
            "date": " • Apr 27, 2021"
        }
        
    
  
    
        ,"post38": {
            "title": "Scala Singletons",
            "content": "Overview . Often in software modeling we need to represent an entity that it does not make sense to have more than one instances throughout program execution. We call these objects singletons. Typically, singletons are modeled using static functions. Scala does not support static functions. Instead we use the object construct [1]. . Singletons . An object defines a single instance of a class with the features we want. For example . object Counter{ private var theCounter = 0 def getNewCounter : Int = {theCounter +=1; theCounter} } . defined object Counter . When the application requires a new counter, simply calls Counter.getNewCounter . println(&quot;New counter &quot; + Counter.getNewCounter) . New counter 1 . The constructor of an object is executed when the object is first used [1]. If an object is never used, its constructor is, obviously, not executed [1]. . An object can have all the features of a class, including extending other classes or traits [1]. However, an object cannot have a constructor with parameters. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/singletons/programming/2021/04/26/scala-singletons.html",
            "relUrl": "/scala/classes/singletons/programming/2021/04/26/scala-singletons.html",
            "date": " • Apr 26, 2021"
        }
        
    
  
    
        ,"post39": {
            "title": "Observability",
            "content": "Overview . Using full state feedback i.e. $ mathbf{u} = - mathbf{K} mathbf{x}$, we can modify the behavior of a controllable system. However, it is not always possible to have full-state measurements of the state vector $ mathbf{x}$. In this case, we have to estimat it. This is only possible when the system observable [1]. In this post we will have a brief view of observability. . Observability . Recall that we deal with linear systems of the form . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . In this case, observability is similar to controlability [1]. Briefly, a system is observable if it is possible to estimate any state $ boldsymbol{ xi} in mathbb{R}^n$ from a history of measurements $ mathbf{y}(t)$ [1]. The observability matrix $ mathbf{ cal{O}}$ allows us to determin entirely whether a system is observable or not [1]. It is defined as . $$ mathcal{ cal{O}} = begin{bmatrix} mathbf{C} mathbf{CA} mathbf{C} mathbf{A}^2 vdots mathbf{C} mathbf{A}^{n-1} end{bmatrix}$$ . Where $n$ is the number of state variables. Specifically, if the rows of the matrix span $ mathbb{R}^n$ then it is possible to estimate any full-dimensional state vector $ mathbf{x} in mathbb{R}^n$ from the time-history of $ mathbf{y}(t)$ [1]. . If a system is observable, then it is possible to design the eignevalues of the estimated dynamics to have properties such as noise attenuation and fast estimation [1]. Finally, note that observability matrix is the transpose of the controllability matrix $ mathbf{ cal{C}}$. . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/dynamical-systems/linear-systems/control/observability/2021/04/25/observability.html",
            "relUrl": "/dynamical-systems/linear-systems/control/observability/2021/04/25/observability.html",
            "date": " • Apr 25, 2021"
        }
        
    
  
    
        ,"post40": {
            "title": "Value Iteration With Scala",
            "content": "Overview . When policy evaluation is stopped after just one update of each state, the algorithm is called value iteration. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps [1]. This post looks at the value iteration algorithm. . Value iteration . One drawback of policy iteration is that each of its iterations involves a policy evaluation. This, however, may itself be an iterative computation; therefore requiring multiple sweeps through the state set [1]. Furthermore, if the evaluation is done iteratively, then convergence to $V_{ pi}$ occurs only in the limit [1]. . Given the above limitations of policy iterations, the question posed is whether we could we stop earlier? [1]. Luckily, the policy evaluation step of policy iteration can truncated without loosing the convergence gurantees of the method. Moreover, this can be done in several ways [1]. . In particular, when policy evaluation is stopped after just one update of each state, the algorithm is called value iteration. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps [1] . $$V_{k+1}(s) = max_{ alpha} sum_{s^*, r}p(s^*, r | s, alpha) left[r + gamma V_{k}(s^*) right], ~~ forall s in mathbb{S}$$ . Figure 1: Value iteration algorithm. Image from [1]. . The value iteration is obtained simply by turning the Bellman optimality equation into an update rule [1]. It requires the maximum to be taken over all the actions. Furthermore, the algorithm terminates by checking the amount of change of the value function. . Code . import scala.collection.mutable.ArrayBuffer import scala.util.control.Breaks._ import scala.math.max . import scala.collection.mutable.ArrayBuffer import scala.util.control.Breaks._ import scala.math.max . object Grid{ class State(val idx: Int){ val neigbors = new ArrayBuffer[Int]() for(i &lt;- 0 until 4){ neigbors += -1 } def addNeighbors(neighbors: Array[Int]): Unit = { require(neighbors.length == 4) for(n &lt;- 0 until neighbors.length){ addNeighbor(n, neighbors(n)) } } def addNeighbor(idx: Int, nIdx: Int): Unit = { require(idx &lt; 4) neigbors(idx) = nIdx } def getNeighbor(idx: Int): Int = { require(idx &lt; 4) return neigbors(idx) } } } . defined object Grid . class Grid{ val states = new ArrayBuffer[Grid.State]() def nStates : Int = states.length def nActions: Int = 4 def envDynamics(state: Grid.State, action: Int): (Double, Int, Double, Boolean) = { (0.25, states(state.idx).getNeighbor(action), 1.0, false) } def getState(idx: Int): Grid.State = { require(idx &lt; nStates) states(idx) } def create(): Unit = { // add a new state for(s &lt;- 0 until 9){ states += new Grid.State(s) if(s == 0){ states(s).addNeighbors(Array(0, 1, 3, 0)) } else if(s == 1){ states(s).addNeighbors(Array(1, 2, 4, 0)) } else if(s == 2){ states(s).addNeighbors(Array(2, 2, 5, 1)) } else if(s == 3){ states(s).addNeighbors(Array(0, 4, 6, 3)) } else if(s == 4){ states(s).addNeighbors(Array(1, 5, 7, 3)) } else if(s == 5){ states(s).addNeighbors(Array(2, 5, 8, 4)) } else if(s == 6){ states(s).addNeighbors(Array(3, 7, 6, 6)) } else if(s == 7){ states(s).addNeighbors(Array(4, 8, 7, 6)) } else if(s == 8){ states(s).addNeighbors(Array(5, 8, 8, 7)) } } } } . defined class Grid . class ValueIteration(val numIterations: Int, val tolerance: Double, val gamma: Double){ val valueF = new ArrayBuffer[Double]() var residual = 1.0 def train(grid: Grid): Unit = { valueF.clear() for(i &lt;- 0 until grid.nStates){ valueF += 0.0 } breakable { for(itr &lt;- Range(0, numIterations)){ println(&quot;&gt; Learning iteration &quot; + itr) println(&quot;&gt; Learning residual &quot; + residual) step(grid) if(residual &lt; tolerance) break; } } } def step(grid: Grid): Unit = { var delta: Double = 0.0 for(sIdx &lt;- 0 until grid.nStates){ // Do a one-step lookahead to find the best action val lookAheadVals = this.one_step_lookahead(grid, grid.getState(sIdx)) val maxActionValue = lookAheadVals.max delta = max(delta, (maxActionValue - valueF(sIdx).abs)) // # Update the value function. Ref: Sutton book eq. 4.10. valueF(sIdx) = maxActionValue } this.residual = delta } // Helper function to calculate the value for // all action in a given state. // Returns a vector of length grid.nActions containing // the expected value of each action. def one_step_lookahead(grid: Grid, state: Grid.State): ArrayBuffer[Double] = { val values = new ArrayBuffer[Double](grid.nActions) for(i &lt;- 0 until grid.nActions){ values += 0.0 } for(i &lt;- 0 until grid.nActions){ val (prob, next_state, reward, done) = grid.envDynamics(state, i) val oldVal = values(i) values(i) = oldVal + prob * (reward + this.gamma * valueF(next_state)) } values } } . defined class ValueIteration . val grid = new Grid grid.create() . grid: Grid = ammonite.$sess.cmd2$Helper$Grid@794b701a . val valueFunction = new ValueIteration(100, 1.0e-4, 1.0) . valueFunction: ValueIteration = ammonite.$sess.cmd29$Helper$ValueIteration@44c65bd3 . valueFunction.train(grid) . &gt; Learning iteration 0 &gt; Learning residual 1.0 &gt; Learning iteration 1 &gt; Learning residual 0.3330078125 &gt; Learning iteration 2 &gt; Learning residual 0.078125 &gt; Learning iteration 3 &gt; Learning residual 0.0048828125 &gt; Learning iteration 4 &gt; Learning residual 3.0517578125E-4 . References . Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. |",
            "url": "https://pockerman.github.io/qubit_opus/scala/reinforcement-learning/algorithms/dynamic-programming/2021/04/23/scala-value-iteration.html",
            "relUrl": "/scala/reinforcement-learning/algorithms/dynamic-programming/2021/04/23/scala-value-iteration.html",
            "date": " • Apr 23, 2021"
        }
        
    
  
    
        ,"post41": {
            "title": "Scala Classes 3",
            "content": "Overview . In this third part regarding Scala classes we review nested classes. . Scala Classes 3 . In Scala, you can nest just about anything inside anything [1]. For example we can define functions inside other functions, and classes inside other classes. We discuss the latter feature here. . import scala.collection.mutable.ArrayBuffer . import scala.collection.mutable.ArrayBuffer . class Grid{ class Element(val idx: Int){ val children = new ArrayBuffer[Element] } private val elements = new ArrayBuffer[Element] def addElement() : Unit = { val element = new Element(this.elements.size) elements += element } def addElement(element: Element){ elements += element } def nElements(): Int = this.elements.size } . defined class Grid . val grid = new Grid . grid: Grid = ammonite.$sess.cmd10$Helper$Grid@662422e8 . grid.addElement() . println(&quot; Number of elements &quot; + grid.nElements) . Number of elements 1 . In Scala, each instance has its own class Element , just like each instance has its own field members [1]. You can see this below . val grid_2 = new Grid . grid_2: Grid = ammonite.$sess.cmd10$Helper$Grid@6116d1b9 . grid.addElement(new grid_2.Element(0)) . cmd15.sc:1: type mismatch; found : cmd15.this.cmd14.grid_2.Element required: cmd15.this.cmd11.grid.Element val res15 = grid.addElement(new grid_2.Element(0)) ^Compilation Failed . Compilation Failed . Assuming that grid holds quad elemennts and grid_2 holds triangular elements this behaviour makes sense. However, we may want to remove this behaviour. We can do this in two ways [1] . Use a companion object | Use type projection | . object Grid{ class Element(val idx: Int){ val children = new ArrayBuffer[Element] } } . defined object Grid . class Grid{ private val elements = new ArrayBuffer[Grid.Element] def addElement() : Unit = { val element = new Grid.Element(this.elements.size) elements += element } def addElement(element: Grid.Element){ elements += element } def nElements(): Int = this.elements.size } . defined class Grid . With type projection, we write our class as shown below . class Grid{ class Element(val idx: Int){ val children = new ArrayBuffer[Element] } private val elements = new ArrayBuffer[Element] def addElement() : Unit = { val element = new Element(this.elements.size) elements += element } def addElement(element: Element){ elements += element } def nElements(): Int = this.elements.size } . defined class Grid . Type projection means, for our case, Element from any Grid . val grid = new Grid val grid2 = new Grid . grid: Grid = ammonite.$sess.cmd19$Helper$Grid@3bc5f2f5 grid2: Grid = ammonite.$sess.cmd19$Helper$Grid@5f5eafec . grid.addElement(new grid_2.Element(0)) . cmd21.sc:1: type mismatch; found : cmd21.this.cmd14.grid_2.Element required: cmd21.this.cmd20.grid.Element val res21 = grid.addElement(new grid_2.Element(0)) ^Compilation Failed . Compilation Failed . Finally, in a nested class, we can access the this reference of the enclosing class as EnclosingClass.this [1]. This is similar to Java. Furthermore, we can establish an alias for that reference as shown below . class Grid{ outer =&gt; class Element(val idx: Int){ val children = new ArrayBuffer[Element] } private val elements = new ArrayBuffer[Element] def addElement() : Unit = { val element = new Element(this.elements.size) elements += element } def addElement(element: Element){ elements += element } def nElements(): Int = this.elements.size } . defined class Grid . The class Grid{ outer =&gt; syntax makes the variable outer refer to Grid.this. Note that we can choose any name for this variable. The name self is common, but perhaps confusing when used with nested classes [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/programming/2021/04/21/scala-classes-3.html",
            "relUrl": "/scala/classes/programming/2021/04/21/scala-classes-3.html",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post42": {
            "title": "Scala Classes 2",
            "content": "Overview . This post looks into primary and auxiliary constructors for Scala classes. . Scala Classes 2 . A Scala class can have as many constructors as we want. A Scala class has a so called primary constructor. Additionally, a class can have any number of auxiliary constructors. . Primary constructor . A Scala class has a so called primary constructor. The primary constructor is not defined with a this method. This is interwoven with the class definition [1]. . class Person(val name: String, val age: Int){ } . defined class Person . Parameters of the primary constructor turn into fields that are initialized with the construction parameters. . val p = new Person(&quot;Alex&quot;, 10) . p: Person = ammonite.$sess.cmd10$Helper$Person@62b0e31f . println(p.name + &quot; has age &quot; + p.age) . Alex has age 10 . The primary constructor executes all statements in the class definition. This is shown below . class AnotherClass(val val1: Double, val val2: Int){ show() def show()={ println(val1 + &quot;, &quot; + val2) } } . defined class AnotherClass . val cls = new AnotherClass(20.0, 10) . 20.0, 10 . cls: AnotherClass = ammonite.$sess.cmd13$Helper$AnotherClass@1ea96977 . The show() function call is a part of the primary constructor. It will be called every time a new object is created [1]. Moreover, if there are no parameters after the class name, then the class has a primary constructor with no parameters. That constructor simply executes all statements in the body of the class [1]. . The primary constructor of the AnotherClass declares and initializes the following fields . val val1: Double val val2: Int . Since the primary constructor parameters are declared with the private keyword, the getter function is public. Moreover only a getter is generated as both variables are declared with val. . Construction parameters can also be regular method parameters, without val or var . How these parameters are processed depends on their usage inside the class[1]. . If a parameter without val or var is used inside at least one method, it becomes a field. . class AnotherClass_2(val1: Double, val2: Int){ def description = val1 + &quot; , &quot; + val2 } . defined class AnotherClass_2 . The primary constructor above, declares and initializes immutable fields val1 and val2 that are object-private i.e. instances of the same class do not have access to these fields of another instance from the same class. . Otherwise, the parameter is not saved as a field. Meaning it is just a regular parameter that can be accessed in the code of the primary constructor [1]. . Finally, sometimes we may want to declare a primary constructor as private. We can do so as shown below . class AnotherClass_3 private(val1: Double, val2: Int){ def description = val1 + &quot; , &quot; + val2 } . defined class AnotherClass_3 . A class user must then use an auxiliary constructor to construct a AnotherClass_3 object [1]. . Auxiliary constructors . Auxiliary constructs are called this [1]. Each such constructor should start with a call to a previously defined auxiliary constructor or the primary constructor. . class MyCls{ private var name = &quot;&quot; private var age = 0 def this(name: String){ // first call primary constructor this() this.name = name } def this(name: String, age: Int){ this(name) //set also the age this.age = age } } . defined class MyCls . The first auxiliary constructor, i.e. this(name: String), calls the empty primary construtor this(). For a class we do not define a primary constructor has a primary constructor with no arguments [1]. . val cls1 = new MyCls val cls2 = new MyCls(&quot;Alex&quot;) val cls3 = new MyCls(&quot;Alex&quot;, 10) . cls1: MyCls = ammonite.$sess.cmd17$Helper$MyCls@78f51c1b cls2: MyCls = ammonite.$sess.cmd17$Helper$MyCls@4cf2a04c cls3: MyCls = ammonite.$sess.cmd17$Helper$MyCls@5ece3c6c . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/programming/2021/04/20/scala-classes-2.html",
            "relUrl": "/scala/classes/programming/2021/04/20/scala-classes-2.html",
            "date": " • Apr 20, 2021"
        }
        
    
  
    
        ,"post43": {
            "title": "Scala Classes 1",
            "content": "Overview . The next stop in the Scala journey is classes. Classes are the cornerstone of object oriented programming. In the simplest form, a Scala class looks similar to classes in Java and C++. . Scala Classes 1 . In the simplest case, a class will model an entity of the modeled domain. It will expose an API that application code can use. Private fields correspond to the state of the object modelled after a class. The following snippet shows how to create a class that has a private field named value, a getter function and a modifier function. . class MyClass { private var value = 0.0 def increment() = value += 1.0 def getValue = value } . defined class MyClass . Specific instances can be used by using the new operator. For example, the following creates an instance of MyClass. . val inst1 = new MyClass . inst1: MyClass = ammonite.$sess.cmd5$Helper$MyClass@5743442d . println(inst1.getValue) . 0.0 . inst1.increment() . println(inst1.getValue) . 1.0 . Getters and setters are frequently used to change properties of a class. Alghtough, such methods allow for every client of the class to modify the state of the class, they are the preferred way of doing so. This is because they allow us to control how the change of the class state is done. Scala can generate getters and setters for us for every private field of our class [1]. . class MyClass2{ var value = 0.0 } . defined class MyClass2 . val inst2 = new MyClass2 . inst2: MyClass2 = ammonite.$sess.cmd10$Helper$MyClass2@52cac8e8 . print(inst2.value) . 0.0 . inst2.value = 10.0 . print(inst2.value) . 10.0 . . Remark . In Scala, the getters and setters are not named as getXxx and setXxx. Instead, they take the name of the private variable [1]. Note also that if we don&#39;t want the getters and setters to be generated then we should declare this field with private[this]. . . Although Scala defines getters and setters for us, we can redefine the getter and setter methods generated for us [1]. This is shown below . class MyClass3{ var myValue = 0.0 def value = myValue def value_=(newValue: Double) : Unit = { // we can control how the state is // changed here if(newValue &gt; 10.0) myValue = 5 else myValue = newValue } } . defined class MyClass3 . val inst3 = new MyClass3 . inst3: MyClass3 = ammonite.$sess.cmd0$Helper$MyClass3@1384d29d . println(inst3.value) . 0.0 . inst3.value = 3.0 . println(inst3.value) . 3.0 . inst3.value = 50 println(inst3.value) . 5.0 . Here are a few points regarding setters and getters to remember [1] . If the field is private then both functions are private | If the field is declared using val, then only a getter is generated | If no getter or setter is needed for the field, declare this as private[this] | We cannot have a write-only field i.e a field with a setter and no getter | . Finally note that, in contrast to Java, a class source file in Scala can contain multiple classes. Moreover, we do not need to declare a Scala class as public [1]. . private[this] . Every instance of a class has access to the private members of instances of the same class. For example, the following is legal . class ShowMeMsg{ private var msg = &quot;Nothing&quot; def setMsg(newMesg: String) : Unit = msg=newMesg def isMessage(other: ShowMeMsg) = { if(other.msg == this.msg){ println(&quot;It is message&quot;) } } } . defined class ShowMeMsg . var msg1 = new ShowMeMsg msg1.setMsg(&quot;ONE&quot;) . msg1: ShowMeMsg = ammonite.$sess.cmd37$Helper$ShowMeMsg@4e22d967 . var msg2 = new ShowMeMsg msg2.setMsg(&quot;ONE&quot;) . msg2: ShowMeMsg = ammonite.$sess.cmd37$Helper$ShowMeMsg@7a1f4e7 . msg1.isMessage(msg2) . It is message . Thus, msg1 has access to the field of msg2. This means that if the field is of var type mgs1 could potentially change it. Certainly, this is not a desired scenario most of the times. We can prevent this sort of situation by using the private[this] qualifier. This allows an even more severe access restriction [1]. This is shown below. . class ShowMeMsg_2{ private[this] var msg = &quot;Nothing&quot; def setMsg(newMesg: String) : Unit = msg=newMesg def isMessage(other: ShowMeMsg) = { if(other.msg == this.msg){ println(&quot;It is message&quot;) } } } . cmd41.sc:7: variable msg in class ShowMeMsg cannot be accessed as a member of cmd41.this.cmd37.ShowMeMsg from class ShowMeMsg_2 in class Helper if(other.msg == this.msg){ ^Compilation Failed . Compilation Failed . Summary . This was a brief introduction to Scala classes. To a large extent classes in Scala are similar to classes in Java or C++. . Specifically, we touched on how to create simple classes in Scala, getters and setters and the private[this] qualifier. The getters and setters will be private if the field they correspond to is declared as such. The private[this] qualifier is even more restricitve; not even instances of the same class can access the field. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/classes/programming/2021/04/18/scala-classes-1.html",
            "relUrl": "/scala/classes/programming/2021/04/18/scala-classes-1.html",
            "date": " • Apr 18, 2021"
        }
        
    
  
    
        ,"post44": {
            "title": "Linear Methods",
            "content": "Linear methods . In this article, we will continue our function approximation journey by introducing linear models for representing the state value function $V_{ pi}(S_t)$. Recall that in the case of large state spaces it is advantageous to use some sort of parametic funtion approximation rather than a tabular representation of the state value function. Furthermore, we need a model in order to to perform, for examle, the SGD update step given below . $$ mathbf{w}_{t+1} = mathbf{w}_t + eta left[V_{ pi}(S_t)- hat{V}_{ pi}(S_t, mathbf{w}_t) right ] nabla hat{V}(S_t, mathbf{w}_t)$$ . One of the simplest representations we can have, and the one we take in this post, is a linear model with resepct to the weights $ mathbf{w}$. In this case, $ hat{V}(s, mathbf{w})$ becomes . $$ hat{V}(s, mathbf{w}) = sum_{i=1}^{d} w_i x_i(s) = mathbf{w}^T mathbf{x}(s)$$ . The expression above implies that there is a vector $ mathbf{x}(s)$ for every state having the same number of components as the weights vector. The vector $ mathbf{x}(s)$ represents the features of the state $s$. For example for an autonomous vehicle, $ mathbf{x}(s)$ may correspond to the vector with components such as vehicle velocity, vehicle acceleration, vehicle position, vehicle orientatio, gas level e.t.c. Technically, each component $x_i$ represents a function such that $x_i: mathbb{S} rightarrow mathbb{R}$ [1]. For linear methods, features are basis functions because they form a linear basis for the set of approximate functions [1]. . Linear models have a straightforward gradient calculation. Indeed in this case . $$ nabla hat{V}(S_t, mathbf{w}_t) = mathbf{x}(s)$$ . Therefore, the SGD update step becomes . $$ mathbf{w}_{t+1}= mathbf{w}_t + eta left[V_{ pi}(S_t)- hat{V}_{ pi}(S_t, mathbf{w}_t) right ] mathbf{x}(S_t)$$ . Linear models are, in general, very well understood throughout science. For our case, when using a linear model case there is only one optimum. Therefore, any method that is guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum [1]. . As a final note, observe that we need $V_{ pi}(S_t)$ in order to perform SGD weights update. This many not always be available. We may, however, have in hand an approximation of it, let&#39;s calle it $U_t$ i.e. $V_{ pi}(S_t) approx U_t$. In this scenario, we are forced to use the latter. However, if $U_t$ is an unbiasd estimate, then $ mathbf{w}_t$ is guaranteed to converge to a local optimum under the conditions specified above for decreasing $ eta$. . References . Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. |",
            "url": "https://pockerman.github.io/qubit_opus/linear-model/reinforcement-learning/2021/04/16/rl-linear-methods.html",
            "relUrl": "/linear-model/reinforcement-learning/2021/04/16/rl-linear-methods.html",
            "date": " • Apr 16, 2021"
        }
        
    
  
    
        ,"post45": {
            "title": "Scala Maps",
            "content": "Overview . Maps are collections of key-value pais. Just like arrays, in Scala we can distinguish between mutable and immutable maps. Furthermore, the default map type is a hash map. However, tree maps are also provided [1]. . As mentioned above, in Scala, a map is a collection of key-value pairs. A pair is a grouping of two values that do not have necessarily the same type [1]. We have two ways cosntructing a pair . Using the -&gt; operator. | Using (key, value) constructs | . Immutable map . We can construct an immutable Map as shown below . val map1 = Map(&quot;France&quot; -&gt; &quot;Paris&quot;, &quot;England&quot; -&gt; &quot;London&quot;, &quot;Greece&quot; -&gt; &quot;Athens&quot;) . map1: Map[String, String] = Map( &#34;France&#34; -&gt; &#34;Paris&#34;, &#34;England&#34; -&gt; &#34;London&#34;, &#34;Greece&#34; -&gt; &#34;Athens&#34; ) . The elements in Map cannot be changed . map1(&quot;Greece&quot;) = &quot;New York&quot; . cmd1.sc:1: value update is not a member of scala.collection.immutable.Map[String,String] did you mean updated? val res1 = map1(&#34;Greece&#34;) = &#34;New York&#34; ^Compilation Failed . Compilation Failed . Nevertheless, the following is a way to update an immmutable map . val map2 = map1 + (&quot;Greece&quot; -&gt; &quot;New York&quot;) . map2: Map[String, String] = Map( &#34;France&#34; -&gt; &#34;Paris&#34;, &#34;England&#34; -&gt; &#34;London&#34;, &#34;Greece&#34; -&gt; &#34;New York&#34; ) . Similarly, we can remove or add a new element . val map3 = map2 + (&quot;Italy&quot; -&gt; &quot;Rome&quot;) . map3: Map[String, String] = Map( &#34;France&#34; -&gt; &#34;Paris&#34;, &#34;England&#34; -&gt; &#34;London&#34;, &#34;Greece&#34; -&gt; &#34;New York&#34;, &#34;Italy&#34; -&gt; &#34;Rome&#34; ) . val map4 = map3 - &quot;Greece&quot; . map4: Map[String, String] = Map( &#34;France&#34; -&gt; &#34;Paris&#34;, &#34;England&#34; -&gt; &#34;London&#34;, &#34;Italy&#34; -&gt; &#34;Rome&#34; ) . Mutable map . In order to get a mutable map we need to explicitly say so . import scala.collection.mutable.Map . import scala.collection.mutable.Map . val grades = Map(&quot;Alex&quot; -&gt; 10, &quot;Alice&quot; -&gt; 15, &quot;George&quot; -&gt;5) . grades: Map[String, Int] = HashMap(&#34;Alex&#34; -&gt; 10, &#34;George&#34; -&gt; 5, &#34;Alice&#34; -&gt; 15) . grades(&quot;Alex&quot;) = 12 . grades . res7: Map[String, Int] = HashMap(&#34;Alex&#34; -&gt; 12, &#34;George&#34; -&gt; 5, &#34;Alice&#34; -&gt; 15) . Above we have initialized the map at construction time. However, we might not always be able to do so. In this case, we need to specify explicitly what type of map we want [1] . import scala.collection.mutable.HashMap . import scala.collection.mutable.HashMap . val gradesEmpty = new HashMap[String, Int] . gradesEmpty: HashMap[String, Int] = HashMap() . If the map is mutable, this means tha we can add, change or remove elements. We can add a new element in two ways . Use operator (key). If the key exists it will update the value corresponding to the key. Otherwise, it will create a new key-value pair | Use operator += followed by a tuple of pairs | . gradesEmpty += (&quot;Suzana&quot; -&gt; 15, &quot;John&quot; -&gt; 3) . res10: HashMap[String, Int] = HashMap(&#34;Suzana&#34; -&gt; 15, &#34;John&#34; -&gt; 3) . We can remove a key-value pait using the -= operator . gradesEmpty -= &quot;Suzana&quot; . res11: HashMap[String, Int] = HashMap(&#34;John&#34; -&gt; 3) . Querying a map . How can we find whether a key is contained in a map? . grades.contains(&quot;Alex&quot;) . res12: Boolean = true . grades.contains(&quot;&quot;) . res13: Boolean = false . One nice feature is that we can query a map with a key and specify a default value in case that the key does not exist . grades.getOrElse(&quot;Alex&quot;, &quot;Invalid Name&quot;) . res14: Any = 12 . grades.getOrElse(&quot;SomeOne&quot;, &quot;Invalid Name&quot;) . res15: Any = &#34;Invalid Name&#34; . As shown above, we can access the value of a particular key using the () operator. This, however, will an exception if the key does not exit. Finally, we can use grades.get( someKey ). This returns an Option object that is either Some( value for key ) or None [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/maps/programming/2021/04/15/scala-maps.html",
            "relUrl": "/scala/maps/programming/2021/04/15/scala-maps.html",
            "date": " • Apr 15, 2021"
        }
        
    
  
    
        ,"post46": {
            "title": "Stochastic Gradient Descent",
            "content": "Stochastic Gradient Descent . Stochastic gradient descent (SGD) methods are among the most widely used of all function approximation methods. Moreover, they are particularly well suited to online reinforcement learning. The article An overview of gradient descent optimization algorithms gives a nice review of gradient descent methods. . Gradient descent methods assume that the function approximation ( in our case this is the approximate value function $ hat{V}(s, mathbf{w})$) is a differentiable function of $ mathbf{w}$. Furthermore, we assume that this is true for $s in mathbb{S}$ . Gradient-descent methods are iterative algorithms. Thus, we denote with $ mathbf{w}_t$ the weight vector at the $t-th$ iteration. Furtheremore, at each iteration, we observe $S_t rightarrow V_{ pi}(S_t)$ i.e. the example consists of a state $S_t$ and its true value under the policy. Note that we can choose the state randomly. These states might be successive states from an interaction with the environment [1]. . Hence, we have in hand $S_t$ and the corresponding state value i.e. $V_{ pi} (S_t)$. However, the problem we now face is that the number of weights is far less than the number of states i.e. our function approximator has a rather limited resolution. In particular, there is generally no $ mathbf{w}$ that gets all the states, or even all the examples, exactly correct. A second problem is that the function approximator must generalize to all the other states that have not appeared yet [1]. . We assume that states appear in examples with the same distribution, $ mu$, over which we are trying to minimize the $MSVE$ given by: . $$MSVE( mathbf{w}) = sum_{s in S} mu(s) left[V_{ pi}(s) - hat{V}_{ pi}(s, mathbf{w}) right]^2$$ . Stochastic gradient-descent (SGD) methods do this by adjusting the weight vector after each example is visited by a small amount in the direction that would most reduce the error on that example. Namely . $$ mathbf{w}_{t+1} = mathbf{w}_t - frac{1}{2} eta nabla left[V_{ pi}(S_t)- hat{V}_{ pi}(S_t, mathbf{w}_t) right ]^2 = mathbf{w}_t + eta left[V_{ pi}(S_t)- hat{V}_{ pi}(S_t, mathbf{w}_t) right ] nabla hat{V}(S_t, mathbf{w}_t)$$ . where $ eta$ is a positive step-size parameter also known as learning rate and . $$ nabla hat{V}(S_t, mathbf{w}) = left( frac{ partial hat{V}(S, mathbf{w})}{ partial w_1}, ..., frac{ partial hat{V}(S, mathbf{w})}{ partial w_m} right)$$ . SGD methods are gradient descent methods because. The latter methods update $ mathbf{w}_t$ by a small amount towards the direction that most reduces the error. The error metric, $MSVE$, that we use here, uses as an error indicator the following quantity . $$e^2 = left[V_{ pi}(s) - hat{V}_{ pi}(s, mathbf{w}) right]^2$$ . This is reduced most rapidly in the direction of $- nabla hat{V}(S_t, mathbf{w})$. . There are various gradient descent methods, see for example [2]. Gradient descent methods are called stochastic when the update is done on only a single example, which might have been selected stochastically. Over many examples, making small steps, the overall effect is to minimize an average performance measure such as the MSVE. [1] . SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily. This is shown in the image below. . Figure 1. SGD fluctuation. Image from [2]. . This overshooting behavior, in general, complicates convergence to the exact minimum. However, by slowly decreasing the learning rate $ eta$ it has been shown that SGD shows the same convergence behaviour as batch gradient descent and almost certainly converges to a local or the global minimum both for convex and non-convex optimization problems. One other problem one may face with the SGD algorithm is that some bias may be introduced as the algorithm updates the weights on a per weights basis. This can be mitigated by suffling the data after each iteration. . Given that SGD does frequent updates that may exhibit high variance why don&#39;t we move in the minimizing direction in one step and therefore completely eliminate the error on the visited example? Altghough many times this can be done, it may not be the right thing to do so. The reason why this is the case, is that the weights are far less than the states. Hence, we should not seek to find a value function that has zero error for all states. Instead, the approximation should balance the errors in the different states [1]. If we completely corrected each example in one step, then we would not find such a balance [1]. Note also that the convergence results for SGD methods assume that the learning rate, $ eta$, decreases over time. Moreover, if it decreases in such a way as to satisfy the following stochastic approximation conditions . $$ sum_{n = 1}^{ infty} eta_n( alpha) = infty ~~ text{and} ~~ sum_{n = 1}^{ infty} eta_{n}^2( alpha) &lt; infty $$ . then the SGD method is guaranteed to converge to a local optimum [1]. . As a final note, observe that we need $V_{ pi}(S_t)$ in order to perform SGD weights update. This many not always be available. We may, however, have in hand an approximation of it, let&#39;s calle it $U_t$ i.e. $V_{ pi}(S_t) approx U_t$. In this scenario, we are forced to use the latter. However, if $U_t$ is an unbiasd estimate, then $ mathbf{w}_t$ is guaranteed to converge to a local optimum under the conditions specified above for decreasing $ eta$ [1]. . References . Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. | An overview of gradient descent optimization algorithms |",
            "url": "https://pockerman.github.io/qubit_opus/stochastic-gradient-descent/gradient-descent/reinforcement-learning/algorithms/2021/04/12/stochastic-gradient-descent.html",
            "relUrl": "/stochastic-gradient-descent/gradient-descent/reinforcement-learning/algorithms/2021/04/12/stochastic-gradient-descent.html",
            "date": " • Apr 12, 2021"
        }
        
    
  
    
        ,"post47": {
            "title": "Controllability",
            "content": "Overview . When we want to control a dynamical system, the natural question that arises is to what extent can we achive this? The term controllability is used to describe whether a systme is controllable altogether. We will see that whether a system is controllable or not is determined completely by the controllability matrix [1] . Controllability . Recall that we deal with linear systems of the form . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . We can understand whether the linear system above is controllable or not by examing the controllability matrix $ mathbf{ cal{C}}$. In particular, the column space of that matrix. This matrix is defined as follows . $$ mathbf{ cal{C}} = begin{bmatrix} mathbf{B} &amp;&amp; mathbf{AB} &amp;&amp; mathbf{A}^2 mathbf{B} &amp;&amp; dots &amp;&amp; mathbf{A}^{n-1} mathbf{B} end{bmatrix}$$ . Where $n$ is the number of state variables. If the controllability matrix has $n$ linearly independent columns, then the system under consideration is controllable [1]. Note that this does not mean that the columns of $ mathbf{ cal{C}}$ should be linearly independent. All that we require, is that we can find $n$ linearly independent columns (see example 2 below). Let&#39;s see two examples taken from [1]. . Example 1 . Let&#39;s consider the following system . $$ frac{d}{dt} begin{bmatrix}x_1 x_2 end{bmatrix} = begin{bmatrix}1 &amp;&amp; 0 0 &amp;&amp; 2 end{bmatrix} begin{bmatrix}x_1 x_2 end{bmatrix} + begin{bmatrix}0 1 end{bmatrix} u$$ . Immediatelly, we can see that the system is not controllable. This is because the two state variables are decoupled and the control input affects only $x_2$. The controllability matrix is . $$ mathbf{ cal{C}} = begin{bmatrix}0 &amp;&amp; 0 1 &amp;&amp; 2 end{bmatrix} $$ . and we can see that the columns of that matrix are not independent. . Example 2 . If we include a control signal for the both state variables, we can turn this system into a controllable one. The new system now is . $$ frac{d}{dt} begin{bmatrix}x_1 x_2 end{bmatrix} = begin{bmatrix}1 &amp;&amp; 0 0 &amp;&amp; 2 end{bmatrix} begin{bmatrix}x_1 x_2 end{bmatrix} + begin{bmatrix}1 &amp;&amp; 0 0 &amp;&amp; 1 end{bmatrix} begin{bmatrix} u_1 u_2 end{bmatrix} u$$ . By including a control signal for bith state variables, we can control them independently. The controllability matrix now becomes [1] . $$ mathbf{ cal{C}} = begin{bmatrix}1 &amp;&amp; 0 &amp;&amp; 1 &amp;&amp; 0 0 &amp;&amp; 1 &amp;&amp; 0 &amp;&amp; 2 end{bmatrix} $$ . and we can verify that the columns of this matrix do span $ mathbb{R}^2$. Note that the columns of the matrix above are not linearly independent. Indeed the third column is a copy of the first and the fourth is a product of the second. However, the first two columns do span $ mathbb{R}^2$. . Three equivalent conditions . The span of the columns of matrix $ mathbf{ cal{C}}$ form a Krylov subspace [1]. The space determines which state vectors can be controlled. Hence, controllability implies two things [1] . Eigenvalue placement | Any state vector $ boldsymbol{ xi} in mathbb{R}^n$ is reachable with some actuation signal | . The following three conditions are equivalent [1] . Controllability | Arbitrary eigenvalue placement | Reachability of $ mathbb{R}^n$ | . We already saw what controllability means in terms of the controllability matrix. Arbitrary eigenvalue placement means that we can design the eigenvalues of the system through the choice of the feedback signal $ mathbf{u}$. In particular, for $ mathbf{u} = - mathbf{K} mathbf{x}$, the system becomes . $$ frac{d mathbf{x}}{dt} = ( mathbf{A} - mathbf{B} mathbf{K}) mathbf{x}$$ . The matrix $ mathbf{K}$ is called the gain matrix. There are various methods to design this matrix. Finally, reachability in practical terms means that we can steer the system to any arbitrary state with some actuation signal, or, conversely, there exists an actuation signal so that the system can be pushed to an arbitrary state $ boldsymbol{ xi} in mathbb{R}^n$ [1]. . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/dynamical-systems/linear-systems/control/controllability/2021/04/11/controllability.html",
            "relUrl": "/dynamical-systems/linear-systems/control/controllability/2021/04/11/controllability.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post48": {
            "title": "Longitudinal Car Model",
            "content": "Longitudinal car model . In this section, we will go over the concept of the vehicle longitudinal dynamics. The two major elements of the longitudinal vehicle model discussed in this section are . Vehicle dynamics | Powertrain dynamics | . The vehicle dynamics are influenced by longitudinal tire forces, aerodynamic drag forces, rolling resistance forces and gravitational forces. The longitudinal powertrain system of the vehicle consists of the internal combustion engine, the torque converter, the transmission and the wheels. This video explains nicely the concepts. . The longitudinal vehicle dynamic model is simply based on the dynamics of the vehicle that generate forward motion. The following figure shows a typical vehicle longitudinal motion force diagram on an inclined road. . Figure 1. Schematics of vehicle logitudinal model on an inclined road. Image from [1]. . We have the followig forces acting on the vehicle . The front tire forces $F_{xf}$ | The rear tire forces $F_{xr}$ | The aerodynamic drag force $F_{aero}$ | The rolling resistance forces $R_{xf}$ and $R_{xr}$ | The force due to gravity $F_g$ | . According to Newton’s laws of motion, and in particular the second law, the longitudinal tire forces of the front and rear tyres, $F_{xf}$ and $F_{xr}$, should balance the resistance forces $F_{aero}$, the gravitational force $F_g$ , and the rolling resistance of the front and rear tires, $R_{xf}$ and $R_{xr}$. Any imbalance between these forces creates an acceleration of the vehicle in the longitudinal direction denoted by $ ddot{x}$. Thus, the basic logintudinal motion model is given by . $$m ddot{x} = F_{xf} + F_{xr} - F_{aero} - F_g - R_{xf} - R_{xr}$$ . where $m$ is the mass of the vehicle. The forces $F_{xf}$ and $F_{xr}$ come from the vehicle power train. We can express them collectively as $F_x$. Furthermore, we group together the rolling resistance forces under the symbol $R_x$. Thus, the reduced model is . $$m ddot{x} = F_x - F_{aero} - F_g - R_x $$ . We will need a way to express the involved quantities in order to be able to solve for $ ddot{x}$. Let&#39;s start with the gravitational force $F_g$. . Gravitational froce . We can express $F_g$ as [2] . $$F_g = mg sin ( alpha)$$ . where $ alpha$ is the local road slope. For small slope angles, we can write . $$sin ( alpha) approx alpha$$ . Aerodynamic drag . A vehicles longitudinal motion is resisted by aerodynamic drag rolling resistance and the force due to gravity. The aerodynamic drag force $F_{aero}$ is typically modeled as dependent on air density $ rho$, frontal area of the vehicle $A$, the vehicles coefficient of friction $C_D$, and the current speed of the vehicle. The functional relationship of all these quantities is given in the equation beow . $$F_{aero} = frac{1}{2}C_D rho A v^2$$ . Rolling resistance . Tires are elastic materials that are subject to deformation in the patch which is in contact with the road surface. Let&#39;s neglect the the deformation of the road. The tire is subject to a normal load. Due to this load, the tire material will be deflected normally at the contact patch and then regaining its shape whilst leaving the patch neighborhood. However, internal damping of the material does not allow the energy lost during deforming the tire to be completely recovered when the material returns to its original shape [1]. It appears therefore, that some loss of energy occurs. This loss is represented by a force on the tires called the rolling resistance that acts in the opposite direction of the motion of the vehicle. . Hence, the rolling resistance depends on the normal tire load, tire pressure and vehicle speed. A model is given below [1], . $$R_x = N(c_{r, 0} + c_{r,1}| dot{x}| + c_{r,2}| dot{x}|^2)$$ . see also [2] for further modelling. If we assume nominal operating conditions and drop the second-order terms for simplicity, we can arrive at a linear rolling resistance model, where $c_{r,1}$ is the linear rolling resistance coefficient. . $$R_x approx c_{r,1}| dot{x}|$$ . Tire forces . We now discuss the longitudinal tire forces expressed under the term $F_x$. Longitudinal tire forces depend on the following factors [2] . Slip ratio | Normal load on the tires | Friction coefficient on the tire road interface | . Let&#39;s see these components . Slip ratio . For an effective wheel radius $R_{effective}$ and a wheel velocity $ omega_w$ the velocity is described by . $$V_{wheel} = R_{effective} omega_{wheel}$$ . However, the actual longitudinal velocity at the axle of the wheel, $V_x$ may be different than that. This is called longitudinal slip [2]. In other words, the longitudinal slip is defined as [2] . $$ sigma = V_{wheel} - V_x$$ . Moreover, we define the longitudinal slip ratio during braking and acceleration as [2] . $$ sigma_{xf} = begin{cases} frac{R_{effecive} omega_{wf} - V_x}{V_{x}}, ~~ text{during breaking} frac{R_{effecive} omega_{wf} - V_x}{R_{effecive} omega_{wf}}, ~~ text{during acceleration} end{cases}$$ . We have a similar expression for the rear wheels. Given the slip coefficients, we can express the longitudinal tire forces as . $$F_{xf} = C_{ sigma f} sigma_{xf}, ~~ F_{xr} = C_{ sigma r} sigma_{xr}$$ . where $C_{ sigma f}$ and $C_{ sigma r}$ are called the longitudinal tire stiffness parameters of the front and rear tires respectively [2]. . Powertrain forces . The longitudinal tire forces, denoted collectivelly above with $F_x$, acting on the driving wheels are the main forces that drive the vehicle forward [2]. These forces depend on the difference between the rotational wheel velocity $R_{effective} omega_{w}$ and the vehicle longitudinal velocity $ dot{x}$. In particular, we saw that we can model the longitudinal tire forces as . $$F_{xf} = C_{ sigma f} sigma_{xf}, ~~ F_{xr} = C_{ sigma r} sigma_{xr}$$ . where $C_{ sigma f}$ and $C_{ sigma r}$ are called the longitudinal tire stiffness parameters of the front and rear tires respectively [2]. However, $ omega_w$ is highly influence by the powertrain dynamics of the vehicle. The powertrain has the following major components [2] . Engine | Transmission or gearbox | Torque converter or clutch | Differential | Wheels | . Figure 2. Powertrain schematics. Image from [1]. . Let&#39;s see each of the components separately . Torque converter . The torque cnverter connects the engine to the transmission. When the engine is turning slowly, e.g. when the car waits at a stoplight, the amount of torque passed through the torque converter is very small. Thus, maintaining the the car stopped requires only a light pressure on the brake pedal. Hence, we don&#39;t have to stall the engine in order to maintain the vehicle stopped. In contrast, when the vehicle accelerates the torque converter gives the car more torque [2]. . The torque converter has the following major components [2] . pump | turbine | transmission fluid | . The pump turns at the same speed as the engine whilst the turbine is connected to the transmission and causes the transmission to spin at the same speed as the turbine [2] This is what basically moves the vehicle. The coupling between the turbine and the pump is through the transmission fluid. Torque is transmitted from the pump to the turbine of the torque converter [2]. . Various models have been introduced to model the pump torque $T_{pump}$ and the turbine torque $T_{turbine}$ see [2 page 103]. . Transmission dynamics . Let&#39;s denote with $GR$ the gear ratio of the transmission. In general, $GR &lt; 1$ and increases as the gear shifts upwards. The input to the transmission module is the torbine torque $T_{turbine}$ [2]. The torque transmitted to the wheels is $T_{wheels}$. Then, at steady state, this torque is given by . $$ T_{wheels} = frac{1}{GR} T_{turbine}$$ . Furthermore, we have the following relaton between the transmission and the wheel speed [2] . $$ omega_{transmission} = frac{1}{GR} omega_{wheels}$$ . Note that these equations cannot be used during gear change. See [2 page 105] for a model based on first order equations. . Engine dynamics . A simplified engine dynamic model is . $$J_{engine} dot{ omega}_e = T_{engine} - T_{pump}$$ . In general, the engine torque $T_{engine}$ depends on the dynamics in the intake and exhaust manifold of the engine and on the accelerator input from the driver [2]. $T_{pump}$ is the torque from the pump is the load of the engine from the torque converter [2]. . Wheel dynamics . The driving wheels rotational dynamics, e.g. for the rear wheels in a rear wheel driven vehicle, are dictated by [2] . $$J_{wheel} dot{ omega}_{wheel, r} = T_{wheel} - R_{effective}F_{xr}$$ . For the non-driven wheels the torque term is zero. . Refernces . Lesson 4: Longitudinal Vehicle Modeling | Rajamani R. Longitudinal Vehicle Dynamics. In: Vehicle Dynamics and Control., Mechanical Engineering Series. Springer 2012. |",
            "url": "https://pockerman.github.io/qubit_opus/longitudinal-dynamics/autonomous-vehicles/mathematical-modelling/vehicle-dynamics/2021/04/10/longitudinal-vehicle-model.html",
            "relUrl": "/longitudinal-dynamics/autonomous-vehicles/mathematical-modelling/vehicle-dynamics/2021/04/10/longitudinal-vehicle-model.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post49": {
            "title": "Linear Time-Invariant Systems",
            "content": "Linear time-invariant systems . Linear systems form a cornerstone of mathematical modelling of dynamical systems. Indeed a system or some aspects of it can be modelled using a linear model. Furthermore, non-linear systems can be linearized around a certain mode of operation. In this section, we give a brief overview of linear time-invariant systems. The major advantage of linear systems is that in terms of analysis a far simpler. Moreover, understanding the dynamics and thus stability of the system is easier. . Let&#39;s consider the following system . $$ frac{d mathbf{x}}{dt} = mathbf{f}( mathbf{x}, mathbf{u}), ~~ mathbf{y} = mathbf{g}( mathbf{x}, mathbf{u})$$ . As before, $ mathbf{x}$ represents the state of the modelled system whilst $ mathbf{u}$ represents some control input to the system. Note that the right-hand side terms do not depend explicitly on the time variable $t$. . If $ mathbf{f}$ or $ mathbf{g}$ or both are non-linear, then the system is non-linear. In this case, we can linearize the system i.e. its dynamics, using a Taylor series expansion near a fixed point $( bar{ mathbf{x}}, bar{ mathbf{u}})$. Recall that at a fixed point is a point where . $$ mathbf{f}( bar{ mathbf{x}}, bar{ mathbf{u}}) = mathbf{0}$$ . The linear, or linearized, dynamics can be written in the following matrix form (assuming no errors) . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . Unforced dynamics . When $ mathbf{u} = mathbf{0}$ and when there are no measurement errors i.e. $ mathbf{y} = mathbf{x}$. The system reduces to . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x}$$ . The solution to this ODE is [1] . $$ mathbf{x}(t) = e^{ mathbf{A}t} mathbf{x}(0)$$ . Thus $ mathbf{x}(t)$ depends or is determined entirely by the matrix $ mathbf{A}$. The stability of the unforced system therefore, can be understood via the eigenvalues and eigenvectors of $ mathbf{A}$. In particular we have the following cases . All the eigenvalues $ lambda$ satisfy $Re( lambda) &lt; 0$. Then the system is stable and all solutions decay to $ mathbf{u} = mathbf{0}$ as $t rightarrow infty$ | There exists at least one eigenvalue $ lambda$ with $Re( lambda) &gt; 0$ then the system is unsatble and will diverge from the fixed point along the corresponding unstable eigenvector direction. | . Forced dynamics . Now let&#39;s assume that $ mathbf{u} neq mathbf{0}$ and that $ mathbf{x}(0) = mathbf{0}$. In this case the solution up to time $t$ is given by [1] . $$ mathbf{x}(t) = int_{0}^t e^{ mathbf{A}(t - tau)} mathbf{B} mathbf{u}( tau)d tau $$ . This integral is nothing more than a convolution. Thus, we can write . $$ mathbf{x}(t) = e^{ mathbf{A}t} mathbf{B} * mathbf{u}(t)$$ . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/dynamical-systems/linear-systems/ode/time-invariant/2021/04/10/linear-time-invariant-systems.html",
            "relUrl": "/dynamical-systems/linear-systems/ode/time-invariant/2021/04/10/linear-time-invariant-systems.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post50": {
            "title": "Lorenz System Simulation",
            "content": "Lorenz System Simulation . The Lorenz system is a system of ordinary differential equations first studied by Edward Lorenz. The system of ODEs is given below . $$ dot{x} = sigma(y -x), ~~ dot{y} = x ( rho - z) -y, ~~ dot{z} = xy - beta z$$ . The model has three parameters i.e. $ sigma, rho$ and $ beta$. . It is notable for having chaotic solutions for certain parameter values and initial conditions. In particular, the Lorenz attractor is a set of chaotic solutions of the Lorenz system. In popular media the &#39;butterfly effect&#39; stems from the real-world implications of the Lorenz attractor, i.e. that in any physical system, in the absence of perfect knowledge of the initial conditions (even the minuscule disturbance of the air due to a butterfly flapping its wings), our ability to predict its future course will always fail. This underscores that physical systems can be completely deterministic and yet still be inherently unpredictable even in the absence of quantum effects. The shape of the Lorenz attractor itself, when plotted graphically, may also be seen to resemble a butterfly. . import numpy as np from mpl_toolkits import mplot3d import matplotlib.pyplot as plt . class ODE45(object): def __init__(self, dt, n_steps, rhs, y0) -&gt; None: self._dt = dt self._n_steps = n_steps self._rhs = rhs self._time = 0.0 self._yold = y0 def step(self): k1 = self._k1(t=self._time) k2 = self._k2(t=self._time, k1=k1) k3 = self._k3(t=self._time, k2=k2) k4 = self._k4(t=self._time, k3=k3) self._yold += k1/6. + k2/3. + k3/3. + k4/6. def integrate(self) -&gt; None: self._time = 0.0 solutions = [[self._yold[0]], [self._yold[1]], [self._yold[2]]] times = [self._time] for itr in range(self._n_steps): self.step() self._time += self._dt times.append(self._time) solutions[0].append(self._yold[0]) solutions[1].append(self._yold[1]) solutions[2].append(self._yold[2]) return times, solutions def _k1(self, t: float) -&gt; np.array: return self._dt * self._rhs(t, self._yold) def _k2(self, t: float, k1: np.array) -&gt; np.array: return self._dt * self._rhs(t + 0.5 * self._dt, self._yold + 0.5 * k1) def _k3(self, t: float, k2: np.array) -&gt; np.array: return self._dt * self._rhs(t + 0.5 * self._dt, self._yold + 0.5 * k2) def _k4(self, t: float, k3: np.array) -&gt; np.array: return self._dt * self._rhs(t + self._dt, self._yold + k3) . class LorenzRhs(object): def __init__(self, beta: np.array): self._beta = beta def __call__(self, t: float, x:np.array) -&gt; np.array: result = np.array([beta[0]*(x[1] - x[0]), x[0]*(beta[1] - x[2]) - x[1], x[0]*x[1] - beta[2]*x[2]]) return result . beta = np.array([10., 28., 8./3.]) x0 = np.array([0., 1.0, 20.]) dt = 0.001 n_steps = 50000 . rhs = LorenzRhs(beta=beta) rk45 = ODE45(dt=dt, n_steps=n_steps, rhs=rhs, y0=x0) . times, solutions = rk45.integrate() . fig = plt.figure(figsize=(15,15)) ax = plt.axes(projection=&#39;3d&#39;) ax.plot3D(solutions[0], solutions[1], solutions[2], &#39;gray&#39;) . [&lt;mpl_toolkits.mplot3d.art3d.Line3D at 0x7fd911347a90&gt;] . The following video discusses how to simulate the Lorenz system with Matlab. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EnsB1wP3LFM&#39;, width=800, height=300) .",
            "url": "https://pockerman.github.io/qubit_opus/lorenz-system/python/simulation/numerics/2021/04/09/simulate-lorenz-system.html",
            "relUrl": "/lorenz-system/python/simulation/numerics/2021/04/09/simulate-lorenz-system.html",
            "date": " • Apr 9, 2021"
        }
        
    
  
    
        ,"post51": {
            "title": "Scala Programming. Functions",
            "content": "Overview . In this Scala programming notebook, we will review functions in Scala. . Functions in Scala . The Scala programming language is a JVM based language worthwhile exploring. In this post I give a brief review of functions in Scala as I continue my exploration of the language. . Functions, in general, are at the core of every programming language when it comes to code organization and implementin the DRY principle. Scala supports two types of functions namely functions and methods. The difference is that a method operates on an object a function does not. . We define a function as follows . def myFunc(x: Integer) = if(x &gt;=0) x else -x . When defining a function we must . specify the types of all parameters | if the function is not recursive i.e. does not call itself, we do not have to specify the return type | if the body of the function requires more than one expression,then we should use a block i.e. {}. The last expression of the block becomes the value that the function returns. | . As an aside, it is possible to omit the return type of the function. Indeed, the Scala compiler can determine the return type from the type of the expression to the right of the = symbol. However, this may not be always the case. The following example demonstrates that . def myAbs(x: Double) = if(x &gt;=0) x else -x . val x = -10 myAbs(x) . x: Int = -10 res1_1: Double = 10.0 . However, with a recursive function, we must specify the return type . def fuc(x: Int): Int = if(x &lt;= 0) 1 else x*fuc(x-1) . Default &amp; Named Arguments . Just like C++, Scala also supports default arguments i.e. the default arguments for functions that are used when we do not specify explicit values . def showMe(x: Int=5) = println(&quot;You want to show &quot; + x) . showMe() &gt; You want to show 5 showMe(6) &gt; You want to show 6 . One of the features that I really like in Python is the named argument(s). This feature is very handy for understanding arguments at call sites i.e. I don&#39;t need to do the trip to the (unavailable) documentation but more important is really helpful in mitigating errors. Scala also supports the idea of named arguments . def speak(arg1: String, arg2: String, arg3: String=&quot; the end&quot;) = println(arg1 + arg2 + arg3) . speak(arg2=&quot; is &quot;, arg1=&quot;This &quot;) &gt; This is the end . As you can see, the named arguments need not be in the same order as the parameters. Furthermore, we can mix unnamed and named arguments, provided the unnamed ones come first. This is similar to Python. . Variable Arguments . Often it is useful to have a function that can take a variable number of arguments think of printf. Usually we call these as varargs functions Scala supports this idea too . def sum(args: Int*): Int = { var result = 0 for(arg &lt;- args) result += arg result } . val s = sum(1, 4, 9, 16, 25) &gt; s:Int = 55 . The actual type received by the function is of type Seq. However, we can not do the following . val s = sum(1 to 5) &gt; cmd10.sc:1: type mismatch; found : scala.collection.immutable.Range.Inclusive required: Int val s = sum(1 to 5) ^Compilation Failed Compilation Failed . That&#39;s because if the sum function is called with one argument, that must be a single integer. Here is how we can fix this . val s = sum(1 to 5:_*) &gt; s:Int = 15 . References . Cay Horstmann Scala for the impatient, Addison-Wesley. |",
            "url": "https://pockerman.github.io/qubit_opus/scala/functions/programming/2021/04/08/scala-functions.html",
            "relUrl": "/scala/functions/programming/2021/04/08/scala-functions.html",
            "date": " • Apr 8, 2021"
        }
        
    
  
    
        ,"post52": {
            "title": "Logistic Map Simulation",
            "content": "Logistic Map Simulation With Python . The logistic map is a discrete time system of the form . $$x_{k+1} = beta x_k (1-x_k)$$ . The logistic map is a polynomial mapping (equivalently, recurrence relation) of degree 2, often cited as an archetypal example of how complex, chaotic behaviour can arise from very simple non-linear dynamical equations. The map was popularized in a 1976 paper by the biologist Robert May,[1] in part as a discrete-time demographic model analogous to the logistic equation written down by Pierre François Verhulst . import numpy as np import matplotlib.pyplot as plt . betas = np.linspace(0.0, 4.0, 400) . def get_steady_state(xinit, nitrs, beta): xold = xinit for itr in range(nitrs): xnew = (xold - xold**2)*beta xold = xnew return xold . def iterate(xinit, betas, use_steady_state, steady_state_itrs, itrs): xvals = [] beta_vals = [] xinit = xinit for beta in betas: #print(&quot;Working with beta={0}&quot;.format(beta)) if use_steady_state: xold = get_steady_state(xinit=xinit, nitrs=steady_state_itrs, beta=beta) else: xold = xinit xss = xold for i in range(itrs): xnew = (xold - xold**2)*beta xold = xnew beta_vals.append(beta) xvals.append(xnew) # if this is the case # the solution is boring :) if np.abs(xnew - xss) &lt; 0.001: break return beta_vals, xvals . beta_vals, xvals = iterate(xinit=0.5, betas=betas, use_steady_state=True, steady_state_itrs=2000, itrs=1000) . plt.plot(beta_vals, xvals) plt.xlabel(&quot;beta&quot;) plt.ylabel(&quot;x&quot;) plt.show() . beta_vals, xvals = iterate(xinit=0.5, betas=betas, use_steady_state=False, steady_state_itrs=2000, itrs=1000) . plt.plot(beta_vals, xvals) plt.xlabel(&quot;beta&quot;) plt.ylabel(&quot;x&quot;) plt.show() . from IPython.display import YouTubeVideo YouTubeVideo(&#39;_BvAkyuWhOI&#39;, width=800, height=300) .",
            "url": "https://pockerman.github.io/qubit_opus/logistic-map/python/simulation/numerics/2021/04/08/logistic-map-simulation.html",
            "relUrl": "/logistic-map/python/simulation/numerics/2021/04/08/logistic-map-simulation.html",
            "date": " • Apr 8, 2021"
        }
        
    
  
    
        ,"post53": {
            "title": "Deutsch's algorithm",
            "content": "Deutsch&#39;s algorithm . Quantum computers pose as the future of computing. Although, at the time of writing, the computing harwdare based on quantum mechanics principles can accommodate only a small number of qubits, algorithms have been developed that demonstrate the superiority of quantum computers for certain class problems. . One such algorithm, and perhaps the simplest one is Deutsch&#39;s algorithm. The algorithm solves the following problem [1] . Given a boolean function $f: {0,1 } rightarrow {0,1 }$ determine if $f$ is constant. . The algorithm, can solve the problem with fewer calls to the function $f$ than is possible on a classical machine [1]. A function is called constant if $f(0) = f(1)$. On the other hand, if $f$ is one-to-one, is called balanced [1]. . Using a classical computer we need to do two evaluations of the function; one for each of the two inputs [1, 2]. On the other hand, Deutsch&#39;s algorithm requires only a single call to a black box to solve the problem. The key to the algorithm is the ability to place the second qubit of the input to the black box in a superposition [2]. Let&#39;s see how to do this. . Deutsch&#39;s algorithm works by putting both qubits representing the two inputs into a superposition [1]. The way to do this is using the Hadamard gate. The following image shows this schematically. . Figure 1. Deutsch&#39;s algorithm circuit. Image from [1]. . Let&#39;s study how the state system $| psi rangle$ evolves. Initially the system is at . $$| psi rangle = |01 rangle$$ . Appication of the Hadamard gate moves the two qubits respectively to . $$|0 rangle = frac{|0 rangle + |1 rangle}{ sqrt{2}}$$ . $$ |1 rangle = frac{|0 rangle - |1 rangle}{ sqrt{2}}$$ . Thus, $| psi rangle$ will be at . $$| psi rangle = left[ frac{|0 rangle + |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . Let&#39;s rename the top qubit as $|x rangle$. We want to evaluate $f(x)$. Note that when the bottom qubit is put into a superposition and then multiply by $U_f$, the system will be at state [1] . $$| psi rangle = (-1)^{f(x)}|x rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . Given however that $|x rangle$ is also in superposition, we will have that the system will be at state [1] . $$| psi rangle = left[ frac{(-1)^{f(0)}|0 rangle + (-1)^{f(1)}|1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . The actual state, as shown in the equation above, depends on the values of $f$. We can summarize this as follows [1]. . $$| psi rangle = begin{cases} ( pm1) left[ frac{|0 rangle + |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] ( pm1) left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] end{cases}$$ The final step is to apply the Hadamard gate on the top qubit. Recall that the Hadamard matrix is its own inverse. Thus applying it to the top qubit we get [1] . $$| psi rangle = begin{cases} ( pm1) |0 rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right], ~~ text{if} ~~ f ~~ text{is constant} ( pm1) |1 rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right], ~~ text{if} ~~ f ~~ text{is balanced} end{cases}$$ Now, we simply measure the top qubit. If it is in state $|0 rangle$, then we know that f is a constant function [1]. This was all accomplished with only one function evaluation. . One of the nice points demonstared by the algorithm is that a change of basis can allow solving a problem that otherwise requires more questions to the oracle. In Deutsch algorithm, we start in the canonical basis $|01 rangle$. The first application of the Hadamard matrices is used to change the basis to go into a balanced superposition of basic states. While in this noncanonical basis, we evaluate $f$ with the bottom qubit. The last Hadamard matrix is used as a change of basis matrix to revert back to the canonical basis [1]. . import numpy as np import random . H = np.array([[1.0/np.sqrt(2.0), 1.0/np.sqrt(2.0)], [1.0/np.sqrt(2.0), - 1.0/np.sqrt(2.0)]]) . def oracle(x, y, constant): if constant: f0 = 0 #random.choice([0,1]) f1 = 0 #random.choice([0,1]) else: f0 = 0 #random.choice([0,1]) f1 = 1 #random.choice([0,1]) return np.array([(-1)**f0*x[0], (-1)**f1*x[1]]) . zero = np.array([1., 0.]) one = np.array([0.0, 1.0]) . zero_H = np.dot(H, zero) one_H = np.dot(H, one) . print(zero_H) print(one_H) . [0.70710678 0.70710678] [ 0.70710678 -0.70710678] . out_oracle = oracle(x=zero_H, y=one_H, constant=True) . x = np.dot(H, out_oracle) . print(x) . [1. 0.] . out_oracle = oracle(x=zero_H, y=one_H, constant=False) . x = np.dot(H, out_oracle) . print(x) . [0. 1.] . References . Noson S. Yanofsky and Mirco A. Mannucci, Quantum Computing for Computer Scientists, Cambridge University Press | Eleanor Rieffel, Wolfgang Polak, Quantum Computing: A Gentle Introduction, The MIT Press. | Deutsch&#39;s algorithm |",
            "url": "https://pockerman.github.io/qubit_opus/deutsch/quantum-computing/algorithms/numerics/2021/03/20/deutsch-algo.html",
            "relUrl": "/deutsch/quantum-computing/algorithms/numerics/2021/03/20/deutsch-algo.html",
            "date": " • Mar 20, 2021"
        }
        
    
  
    
        ,"post54": {
            "title": "Singular Value Decomposition",
            "content": "Singular Value Decomposition . One of the most important matrix factorization techniques is the singular value decomposition most often abbreviated as SVD. The reason why is so popular lies on the fact that it is the foundation for many other computational techniques. For example, just to name a few: . Computing pseudo-inverses | Obtaining low-rank matrix approximations | Dynamic mode decomposition | Proper orthogonal ecomposition | Principal components analysis | . For a complex matrix $A in mathbb{C}^{n times m}$, its SVD is . $$A = U Sigma V^{*}$$ . where $V^{*}$ is the complex conjugate transpose. Both $U$ and $V$ are unitary matrices that is the following holds . $$UU^{*} = U^{*}U = I$$ . In general, if a matrix $W$ is a real matrix i.e. its entries are real numbers, then $W^{*} = W^T$. Thus, if $A in mathbb{R}^{n times m}$ the matrices $U$ and $V$ are real orthogonal matrices i.e. . $$UU^{T} = U^{T}U = I$$ . The matrix $ Sigma$ is a diagonal matrix with real and nonnegative entries on the diagonal. The entries $ Sigma_{ii}$ are called the singular values of $A$. The number of the non-zero singular values corresponds to the rank of the matrix $A$. . Given the popularity of the SVD method, it is not surpsising that most linear algebra libraries provide a way to perform it. The following script shows how to compute the SVD in Python using numpy . import numpy as np X = np.random.rand(10 , 10) U, S, V = np.linalg.svd(X, full_matrices=True) # or doing economy SVD U, S, V = np.linalg.svd(X, full_matrices=False) . You can find the documentation at numpy.linalg.svd. Similarly, using the Blaze C++ library . template&lt; typename MT1, bool SO, typename VT, bool TF, typename MT2, typename MT3 &gt; void svd( const DenseMatrix&lt;MT1,SO&gt;&amp; A, DenseMatrix&lt;MT2,SO&gt;&amp; U, DenseVector&lt;VT,TF&gt;&amp; s, DenseMatrix&lt;MT3,SO&gt;&amp; V ); . Overall, the SVD algorithm is a very important matrix decomposition technique used throughout numerical modeling control theory and system identification. We will see applications of the method in future posts. .",
            "url": "https://pockerman.github.io/qubit_opus/linear-algebra/singular-value-decomposition/algorithms/numerics/2021/03/13/singular-value-decomposition.html",
            "relUrl": "/linear-algebra/singular-value-decomposition/algorithms/numerics/2021/03/13/singular-value-decomposition.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post55": {
            "title": "Scala Programming.  Conditionals and loops",
            "content": "Overview . For loops . a to b: iteration range is $[a,b]$ | a until b: iteration range is $[a, b-1]$ | . This is shown below. . for( i &lt;- 0 to 5){ println(i) } . 0 1 2 3 4 5 . for( i &lt;- 0 until 5){ println(i) } . 0 1 2 3 4 . References .",
            "url": "https://pockerman.github.io/qubit_opus/scala/conditionals/loops/programming/2021/01/10/scala-prog-conditionals-loops.html",
            "relUrl": "/scala/conditionals/loops/programming/2021/01/10/scala-prog-conditionals-loops.html",
            "date": " • Jan 10, 2021"
        }
        
    
  
    
        ,"post56": {
            "title": "Scala Arrays",
            "content": "Scala Arrays . There are two types of arrays in Scala just like in most programming languanges. Fixed length and variable length arrays. . Fixed-length arrays . If we know the size of the needed array and that size does not change, we can use the Array class. . val nums = new Array[Int](10) . nums: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) . nums(0) = 10 . nums . res2: Array[Int] = Array(10, 0, 0, 0, 0, 0, 0, 0, 0, 0) . A Scala Array is implemented as a Java array. For example the nums array above, inside the JVM is represented as int[] in the JVM. . Variable-length arrays . Variable-length arrays in Scala are utilized via the ArrayBuffer class . val b = ArrayBuffer[Int]() . cmd3.sc:1: not found: value ArrayBuffer val b = ArrayBuffer[Int]() ^Compilation Failed . Compilation Failed . In contrast to the Array class, we need to explicitly import ArrayBuffer . import scala.collection.mutable.ArrayBuffer . import scala.collection.mutable.ArrayBuffer . val b = ArrayBuffer[Int]() . b: ArrayBuffer[Int] = ArrayBuffer() . We can now add elements to the buffer . b += 1 . res5: ArrayBuffer[Int] = ArrayBuffer(1) . or add more than one elements in one go . b += (5, 6, 7, 8, 9) . res6: ArrayBuffer[Int] = ArrayBuffer(1, 5, 6, 7, 8, 9) . b ++= Array(0, 0, 0) . res7: ArrayBuffer[Int] = ArrayBuffer(1, 5, 6, 7, 8, 9, 0, 0, 0) . There are various operations supported by the ArrayBuffer class; check the Scala documentation. One thing to note however is the following. Adding or removing elements at the end of an ArrayBuffer is an amortized constant time operation [1]. We can insert and remove elements at an arbitrary location, but those operations are not as efficient since all elements after that location must be shifted [1]. . Traversing arrays . Scala is much more uniform compared to C++ when it comes to traversing arrays. . for(i &lt;- 0 until b.length) println(b(i)) . 1 5 6 7 8 9 0 0 0 . Note that we can also use a guard inside the for expression . for(i &lt;- 0 until b.length if b(i) &gt; 0) println(b(i)) . 1 5 6 7 8 9 . Algorithms . Scala arrays have built-in some commin algorithms e.g. sum and sort, min and max . println(&quot;Max element of b &quot; + b.max) println(&quot;Min element of b &quot; + b.min) println(&quot;Sum of element of b &quot; + b.sum) . Max element of b 9 Min element of b 0 Sum of element of b 36 . The sorted method sorts an Array or ArrayBuffer and returns the sorted array without modifying the original [1]. . val newB = b.sorted println(b) . ArrayBuffer(1, 5, 6, 7, 8, 9, 0, 0, 0) . newB: ArrayBuffer[Int] = ArrayBuffer(0, 0, 0, 1, 5, 6, 7, 8, 9) . Note that you can sort an Array, but not an array buffer, in place [1]. Also note that for the min, max , and quickSort algorithms, the element type must have a comparison operation. This is the case for types with the Ordered trait [1]. . References . Cay Horstmann, Scala for the Impatient 1st Edition |",
            "url": "https://pockerman.github.io/qubit_opus/scala/arrays/programming/2021/01/08/scala-arrays.html",
            "relUrl": "/scala/arrays/programming/2021/01/08/scala-arrays.html",
            "date": " • Jan 8, 2021"
        }
        
    
  
    
        ,"post57": {
            "title": "Scala Programming. Values, variables & types",
            "content": "Overview . In this post, we will briefly discuss values, variables and types in Scala. . Values &amp; variables . A value indicated by the keyword val denotes an identifier that has a constant value; we cannot change its contents [2]. Obviously, a val must be initialized at the point of declaration. . val v1 = 0.5 . v1: Double = 0.5 . v1 = 1.0 . cmd1.sc:1: reassignment to val val res1 = v1 = 1.0 ^Compilation Failed . Compilation Failed . val v2; . (console):1: &#39;=&#39; expected but &#39;;&#39; found. val v2; ^ . (console):1: &#39;=&#39; expected but &#39;;&#39; found. val v2; ^ . In contrast, a variable, indicated with the keyword var, means that it can have its contents changed [2]. . var one = 1.0 . one: Double = 2.0 . // if we want we can say that one equals 2 one = 2.0 . In Scala we don&#39;t need to specify the type of either a val or var. This is inferred from the type of the expression that is used to initialize it. However, here is how we can specify the type if needed . val hello: String = &quot;Hello&quot; val weight: Double = 14.5 . hello: String = &#34;Hello&#34; weight: Double = 14.5 . Lazy values . Lazy evaluation is a programming technique where the evaluation of an expression is done when it is needed for the first time and not before. Lazy evaluation is useful when we want to delay costly initialization statements. Moreover, lazy evaluation can deal with circular dependencies and for developing lazy data structures. . Scala allows to use the keyword lazy when declaring val values. In this case the value initialization is deferred until it is accessed for the first time. . val v1 = 3.5 . v1: Double = 3.5 . lazy val v2 = 3.6 . v2: Double = 3.6 . println(&quot;What is the value of v2?&quot; + v2) . What is the value of v2?3.6 . . Remark . Lazy evaluation is not cost-free. In fact, every time a lazy value is accessed, a method is called that checks, in a threadsafe manner, whether the value has already been initialized [2]. . . Types . In Scala there is no distinction between primitives and class types [2]; all types in Scala are classes. This means we can do things likes calling methods on numbers . 1.toString . res7: String = &#34;1&#34; . or things like . 1.to(5) . res8: Range.Inclusive = Range(1, 2, 3, 4, 5) . In Scala, we do not need wrapper types [2]. The Scala compiler does the conversion for us between primitive types and wrappers. For example, if you make an array of Int , you get an int[] array in the virtual machine [2]. . The compiler also checks that we do not combine expressions of different type. For example: . 1 to 4.0 . cmd9.sc:1: type mismatch; found : Double(4.0) required: Int val res9 = 1 to 4.0 ^Compilation Failed . Compilation Failed . References . Martin Odersky, Lex Spoon, Bill Venners, Programming in Scala, 3rd Edition, artima. | Cay Horstmann, Scala for the Impatient 1st Edition, |",
            "url": "https://pockerman.github.io/qubit_opus/scala/values-variables/programming/2021/01/05/scala-values-variables-types.html",
            "relUrl": "/scala/values-variables/programming/2021/01/05/scala-values-variables-types.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post58": {
            "title": "Occupancy grids",
            "content": "Overview . In this post we look into occupancy grid maps. . Occupancy grids . Simply put an occupancy grid is a discretized version of the environment surrounding the ego vehicle [3]. The discretization can be either two or three dimensional. The following images show versions of 2D and 3D grids. . Figure 1. 2D occupancy grid map. . Figure 2. 3D occupancy grid map. . Each cell in an occupancy grid indicates if the space represented by the cell is empty or occupied by an obstacle. Given this, the more dense the grid is the finer the representation of the environment will be. However, the more dense the grid is the more computationally expensive is to create it. Each cell in the grid is binary i.e. it has a value, $m_i$, of either one or zero [3]. . Typically, in order to create an occupancy grid, we make the following assumptions [3] . Static environment i.e. no dynamic objects | Grid cells are independent | The vehicle state is known | . Given the gride cell value $mi_i$, we can construct a belief map meaning a map at time $t$ such that for each cell . $$bel_{t}(m_i) = P(m_i | (x,y))$$ . where $(x,y)$ denotes the vehicle state and the sensor measurements for a given cell. We can establish a threshold at which a given belief can be classified as occupied. Furthermore, we can combine measurements from different time steps to obtain a more accurate belief. In particular, using Baye&#39;s theorem we can come up with the following equation [3] . $$bel_{t}(m_i) = eta P(y_t | m_i)bel_{t-1}(m^i)$$ . where $ eta$ is a scaling or normalization constant to ensure that the $bel$ function represents a probability. . Disadvantages of occupancy grids . In occupancy grid mapping every grid cell is one of two states; occupied or empty [2]. But in some situations it makes sense for a cell to be partially filled. This may occur when only part of the grid cell is filled and the rest is empty, or when the objects that occupies the grid cell have special characteristics [2]. For example it may be that we want a grid cell occupied with vegetation to be somehow less occupied than a grid cell filled with solid rock. . Semi-transparent obstacles and Mitigation Classical occupancy grids have trouble dealing with semi-transparent obstacles such as glass and vegetation. These obstacles may return hits to the laser rangefinder about half of the time, but eventually the occupancy grid will converge to either occupied or not, both of which are incorrect. [2] . A possible solution to this problem would be to consider a more continous measure that measures the density or probability of a beam to have reflected and not passed. Perhaps the simplest approach towards this is to treat the random variable as a biased coin and for each state keep a count of the number of hits and pass throughs [2]. Thus, the only difference is that each state tracks two numbers. Then, the probability can be calculated empirically as the ratio of the hits to the sum of hits and passes. For the inverse sensor model, we either spread the high probability zone or use fractional number of hits and misses for each state [2]. . References . Bruno Siciliano, Lorenzo Sciavicco, Luigi Villani, Giuseppe Oriolo, Robotics Modelling, Planning and Control, Springer | Drew Bagnell Statistical Techniques in Robotics, Lecture notes. | Motion planning for self-driving cars |",
            "url": "https://pockerman.github.io/qubit_opus/robotics/planning/mapping/navigation/occupancy-grid/2020/09/15/occupancy-grids.html",
            "relUrl": "/robotics/planning/mapping/navigation/occupancy-grid/2020/09/15/occupancy-grids.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post59": {
            "title": "Gradient Descent",
            "content": "Overview . Both polynomial and exponential curve fitting admit best-fit least squares solutions [2]. However, these are specialized cases. Thus a more general framework is needed. In this post, we discuss the gradient descent algorithm also known as steepest descent. . Gradient Descent . Consider the following function [1] . $$f( theta_1, theta_2) = frac{1}{2}( theta_{1}^2 - theta_2)^2 + frac{1}{2}( theta_1 -1)^2$$ . The gradient of the function $ nabla f$ is given by . $$ nabla f = (2( theta_{1}^2 - theta_2) theta_1 +( theta_1 - 1), -( theta_{1}^2 - theta_2))$$ . The gradient gives the direction of steepest descent towards the minimum point of $f$ [2]. The minimum point is located in the direction $- nabla f$. . We are interested in finding $ theta_1, theta_2$ that minimize $f$. Gradient descent is an iterative algorithm that uses the gradient of the function in order to update the parameters. The update rule is [1, 2] . $$ boldsymbol{ theta}_k = boldsymbol{ theta}_{k-1} - eta nabla f|_{ boldsymbol{ theta}_{k-1}} $$ . $ eta$ is the so called learning rate and tunes how fast we move to the direction of the gradient. A small $ eta$ slows down convergence whilst a large value may not allow convergence of the algorithm. This is shown in the two figures below: . Figure 1. Gradient descent with eta 0.1. . Figure 2. Gradient descent with eta 0.6. . The code below is a simple implementation of the gradient descent algorithm. . import numpy as np import matplotlib.pyplot as plt . def f(theta1, theta2): return 0.5*(theta1**2 - theta2)**2 + 0.5*(theta1 -1.0)**2 . def f_grad(theta1, theta2): return (2.0*theta1*(theta1**2 - theta2) + (theta1 - 1.0), -(theta1**2 - theta2)) . def gd(eta, itrs, tol): coeffs_series = [] coeffs = [0.0, 0.0] coeffs_series.append([coeffs[0], coeffs[1]]) val_old = f(theta1=coeffs[0], theta2=coeffs[1]) for itr in range(itrs): grad = f_grad(theta1=coeffs[0], theta2=coeffs[1]) coeffs[0] -= eta*grad[0] coeffs[1] -= eta*grad[1] coeffs_series.append([coeffs[0], coeffs[1]]) val = f(theta1=coeffs[0], theta2=coeffs[1]) abs_error = np.abs(val - val_old) if itr % 10 == 0: print(&quot;&gt;Iteration {0} absolute error {1} exit tolerance {2}&quot;.format(itr, abs_error, tol)) if abs_error &lt; tol: print(&quot;&gt;GD converged with residual {0}&quot;.format(np.abs(val - val_old))) return coeffs_series val_old = val return coeffs_series . coeffs_series = gd(eta=0.1, itrs=100, tol=1.0e-4) . &gt;Iteration 0 absolute error 0.09494999999999998 exit tolerance 0.0001 &gt;Iteration 10 absolute error 0.007786932467621618 exit tolerance 0.0001 &gt;Iteration 20 absolute error 0.0033734842877468432 exit tolerance 0.0001 &gt;Iteration 30 absolute error 0.0017811992002927796 exit tolerance 0.0001 &gt;Iteration 40 absolute error 0.001019816407902937 exit tolerance 0.0001 &gt;Iteration 50 absolute error 0.0006153302829237615 exit tolerance 0.0001 &gt;Iteration 60 absolute error 0.00038497166087553616 exit tolerance 0.0001 &gt;Iteration 70 absolute error 0.0002472394968245067 exit tolerance 0.0001 &gt;Iteration 80 absolute error 0.0001619172164950512 exit tolerance 0.0001 &gt;Iteration 90 absolute error 0.00010763619875779401 exit tolerance 0.0001 &gt;GD converged with residual 9.934284762933097e-05 . coeffs_x = [] coeffs_y = [] for item in coeffs_series: coeffs_x.append(item[0]) coeffs_y.append(item[1]) . theta1 = np.linspace(0.0, 2.0, 100) theta2 = np.linspace(-0.5, 3.0, 100) . X, Y = np.meshgrid(theta1, theta2) . Z = f(X, Y) . plt.contour(X, Y, Z, 60, colors=&#39;black&#39;); plt.plot(coeffs_x, coeffs_y, &#39;r-o&#39;) plt.show() . Summary . References . Kevin P. Murphy, Machine learning a probabilistic perspective, The MIT Press | Steven L. Brunton and J. Nathan Kutz, Data-driven science and engineering. Machine learning, dynamical systems and control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/gradient-descent/unconstrained-optimization/machine-learning/algorithms/numerics/2020/06/22/gradient-descent.html",
            "relUrl": "/gradient-descent/unconstrained-optimization/machine-learning/algorithms/numerics/2020/06/22/gradient-descent.html",
            "date": " • Jun 22, 2020"
        }
        
    
  
    
        ,"post60": {
            "title": "The Viterbi algorithm",
            "content": "The Viterbi algorithm . The backward and forward algorithms can be used to compute $P(O| lambda)$. In this notebook we are interested in computing the most likely path given a sequence $O$ and a hidden Markov model $ lambda$. The Viterbi algorithm gives us a way to do so. The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states, also called the Viterbi path, that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM) [2]. The algorithm uses a maximum operation instead of the sum. The operation of Viterbi&#39;s algorithm can be visualized by means of a trellis diagram [2]. It is essentially the shortest path through this trellis. . Given a state sequence $Q=q_1q_2, cdots,q_T$ and an observation sequence $O=O_1O_2, cdots,O_T$ we dfine the variable $ delta_t(i)$ as the probability of the highest probability path at time $t$ that accounts for the first $t$ observations and ends in $S_i$ [1]: . $$ delta_t(i) = max_{Q}p(q_1q_2, cdots,q_t=S_i,O_1O_2, cdots,O_t| lambda)$$ . Then we can recursively calculate $ delta_{t+1}(i)$ and the optimal path can be read by backtracking from $T$ , choosing the most probable at each instant. The algorithm is as follows [1]: . Initialize | . $$ delta_1(i) = pi_i b_i(O_1)$$ . This is initialization is the same as in the forward algorithm. To retrieve the state sequence we also need to keep track of the argument which maximized for each $t$ and $j$. We therefore use the array $ psi$, and in the initialization step the first $ psi$ variable of every state will be equal to 0 because no specific argument coming from the initial probability maximized the value of the first state. . $$ psi_1(i) = 0$$ . Recurse | . $$ delta_t(j) = max_{i=1}^{N} delta_{t-1}(i)a_{ij}b_j(O_t)$$ . $$ psi_t(j) = argmax_{i=1}^{N} delta_{t-1}(i)a_{ij}$$ . Termination | . $$p^{*} = max_i delta_T(i)$$ . $$q^{*}_T = arg max_i delta_T(i)$$ . Path backtracking | . $$q_t{*} = psi_{t+1}(q^{*}_{t+1}), t= T-1, T-2, cdots, 1$$ . $ psi_t (j)$ keeps track of the state that maximizes $ delta_t(j)$ at time $t-1$, that is, the best previous state. The Viterbi algorithm has the same complexity with the forward phase, where instead of the sum, we take the maximum at each step [1]. . Let&#39;s see an example applying the Viterbi algorithm. The example is taken from [2]. Some coding hints from Implement Viterbi Algorithm in Hidden Markov Model using Python and R have also been used. . import numpy as np . obs_to_idx = {&#39;normal&#39;:0, &#39;cold&#39;: 1, &#39;dizzy&#39;:2} # state to index map state_to_idx = {&#39;Healthy&#39;:0, &#39;Fever&#39;:1} . pi = np.array([0.6, 0.4]) # transition probabilities A = np.array([[0.7, 0.3], [0.4, 0.6]]) # emission probabilties B = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]) . o = [&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;] . delta = np.zeros(shape=(len(o), A.shape[0])) previous = np.zeros((len(o)-1, A.shape[0])) for st in state_to_idx: state_idx = state_to_idx[st] delta[0][state_idx] = pi[state_idx] * B[state_idx][obs_to_idx[o[0]]] print(delta) . [[0.3 0.04] [0. 0. ] [0. 0. ]] . for t in range(1, len(o)): obs_idx = obs_to_idx[o[t]] for i in state_to_idx: i_st_idx = state_to_idx[i] probs=[] for j in state_to_idx: j_st_idx = state_to_idx[j] probs.append(delta[t - 1][j_st_idx]*A[j_st_idx][i_st_idx]*B[i_st_idx][obs_idx]) # This is our most probable state given previous state at time t (1) previous[t-1, i_st_idx] = np.argmax(probs) delta[t, i_st_idx] = np.max(probs) # Path Array S = np.zeros(len(o)) # Find the most probable last hidden state last_state = np.argmax(delta[len(o) - 1, :]) S[0] = last_state backtrack_index = 1 for i in range(len(o) - 2, -1, -1): S[backtrack_index] = previous[i, int(last_state)] last_state = previous[i, int(last_state)] backtrack_index += 1 # Flip the path array since we were backtracking S = np.flip(S, axis=0) # Convert numeric values to actual hidden states path = [] for s in S: if s == 0: path.append(&quot;Healthy&quot;) else: path.append(&quot;Fever&quot;) . print(&quot;Path is &quot;, path) . Path is [&#39;Healthy&#39;, &#39;Healthy&#39;, &#39;Fever&#39;] . Another way to compute the $ delta$ matrix is the following more Pythonic way, taken from Implement Viterbi Algorithm in Hidden Markov Model using Python and R. Note the use of the log function. . delta = np.zeros(shape=(len(o), A.shape[0])) previous = np.zeros((len(o)-1, A.shape[0])) for st in state_to_idx: state_idx = state_to_idx[st] delta[0, :] = np.log(pi * B[:, obs_to_idx[o[0]]]) print(delta) . [[-1.2039728 -3.21887582] [ 0. 0. ] [ 0. 0. ]] . for t in range(1, len(o)): obs_idx = obs_to_idx[o[t]] for i in state_to_idx: i_st_idx = state_to_idx[i] # Same as Forward Probability probability = delta[t - 1] + np.log(A[:, i_st_idx]) + np.log(B[i_st_idx, obs_idx]) # This is our most probable state given previous state at time t (1) previous[t - 1, i_st_idx] = np.argmax(probability) # This is the probability of the most probable state (2) delta[t, i_st_idx] = np.max(probability) # Path Array S = np.zeros(len(o)) # Find the most probable last hidden state last_state = np.argmax(delta[len(o) - 1, :]) S[0] = last_state backtrack_index = 1 for i in range(len(o) - 2, -1, -1): S[backtrack_index] = previous[i, int(last_state)] last_state = previous[i, int(last_state)] backtrack_index += 1 # Flip the path array since we were backtracking S = np.flip(S, axis=0) # Convert numeric values to actual hidden states path = [] for s in S: if s == 0: path.append(&quot;Healthy&quot;) else: path.append(&quot;Fever&quot;) . print(&quot;Path is &quot;, path) . Path is [&#39;Healthy&#39;, &#39;Healthy&#39;, &#39;Fever&#39;] . The following video provides a motivation behind the use of the Viterbi algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;MPeedE6Odj0&#39;, width=800, height=300) . The following video provides nice description of the Viterbi algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;s9dU3sFeE40&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Viterbi algorithm, Wikipedia. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20viterbi-algorithm/dynamic-programming/algorithms/numerics/2020/05/24/viterbi-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20viterbi-algorithm/dynamic-programming/algorithms/numerics/2020/05/24/viterbi-algorithm.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post61": {
            "title": "Backward Algorithm",
            "content": "The backward algorithm is the complement of the forward algorithm. Let&#39;s introduce the backward variable $ beta_t(i)$. This is the probability of being in $S_i$ at time $t$ abd observing the partial sequence $O_{t+1}, cdots,O_T$ [1]. This can be written as . $$ beta_t(i) = P(O_{t+1}, cdots,O_T | q_t=S_i, lambda)$$ . The backward algorithm computes this recursively . Initialize | . $$ beta_T(i) = 1$$ . Recurse | . $$ beta_t(i) = P(O_{t+1}, cdots,O_T | q_t=S_i, lambda)$$ . which can be written as, see [1], . $$ beta_{t}(i) = sum_{j}^{N}b_j(O_{t+1})a_{i,j} beta_{t+1}(j)$$ . Let&#39;s implement this as we did for the forward algorithm. . Assume a system with two states $S= {S_0, S_1 }$. Futher, assume that the observation sequence consists of elements from the following set $V= {a, b,c }$. Also let&#39;s assume the following HMM: . $$ boldsymbol{ pi}= begin{bmatrix}0.6 &amp; 0.4 end{bmatrix}$$ . $$ mathbf{A}= begin{bmatrix}0.7 &amp; 0.3 0.4 &amp; 0.6 end{bmatrix}$$ . $$ mathbf{B}= begin{bmatrix} 0.5 &amp; 0.4 &amp; 0.1 0.1 &amp; 0.3 &amp; 0.6 end{bmatrix}$$ . Assume the following sequence $V= {a, b, c }$. We introduce the $ beta$ matrix: . $$ beta = begin{bmatrix}0 &amp; 0 0 &amp; 0 0 &amp; 0 end{bmatrix}$$ . First we initialize . $$ beta = begin{bmatrix}0 &amp; 0 0 &amp; 0 1 &amp; 1 end{bmatrix}$$ . Then use the recursion formula . $$ beta_{t}(i) = sum_{j}^{N}b_j(O_{t+1})a_{i,j} beta_{t+1}(j)$$ . import numpy as np . obs_to_idx = {&#39;a&#39;:0, &#39;b&#39;: 1, &#39;c&#39;:2} # state to index map state_to_idx = {&#39;S0&#39;:0, &#39;S1&#39;:1} . pi = np.array([0.6, 0.4]) # transition probabilities A = np.array([[0.7, 0.3], [0.4, 0.6]]) # emission probabilties B = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]) . o = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] . beta = np.zeros(shape=(len(o),A.shape[0])) . beta[len(o) - 1] = np.ones(A.shape[0]) . # start at the position one before the end # proceed until t is -1. Move back one step at the time for t in range(len(o)-2, -1, -1): for j in range(A.shape[0]): # for x = np.array([1.,2.]) and # y = np.array([1.,2.])then # x*y = np.array([1., 4.]) # that is element-wise product is performed beta[t, j] = (beta[t + 1] * B[:, obs_to_idx[o[t + 1]]]).dot(A[j, :]) . print(beta) . [[0.106 0.112] [0.25 0.4 ] [1. 1. ]] . Overall the forward and backward algorithms can be used to compute $P(O| lambda)$. Indeed using the forward algorithm we have: . $$P(O| lambda) = sum_{i}^{N} alpha_{T}(i)$$ . whilst using the backward algorithm . $$P(O| lambda) = sum_{i}^{N} pi_i b_i(O_1) beta_{1}(i)$$ . The following video nicely explains the motivation behind the backward algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EbxLWGw2zJ4&#39;, width=800, height=300) . The following video nicely explains both the forward and backward algorithms. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;gYma8Gw38Os&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Forward–backward algorithm. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20backward-algorithm/dynamic-programming/algorithms/numerics/2020/05/23/backward-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20backward-algorithm/dynamic-programming/algorithms/numerics/2020/05/23/backward-algorithm.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post62": {
            "title": "Machine Learning Notes. Forward algorithm",
            "content": "Overview . In this post, we will go over the forward algorithm. This is a dynamic programming base algorithm that will allow as to calculate the probability $P(O| lambda)$, where $ lambda$ represents a hidden Markov model. . Forward algorithm . Given a hidden Markov model $ lambda$, meaning . A set of $N$ states $ mathbb{S}= {S_1, dots S_N }$ | A set of $M$ distinct observation symbols $ mathbf{V} = {v_1 dots v_M }$ | A transition probability matrix $ mathbf{A}$ | An observations probabilities matrix $ mathbf{B}$ | An initial state of probabilities $ boldsymbol{ pi}$ | . we want to be able to compute the marginal probability $P(O| lambda)$. This is the so called evaluation problem [1]; given the observation sequence $O$ calculate the probability that the sequence can occur under $ lambda$ . In theory, this can be calculated by using . $$P(O| lambda) = sum_{Q}P(O, Q| lambda)$$ . where . $$P(O, Q| lambda) = P(q_1) Pi_{t=2}^{T}P(q_t|q_{t-1}) Pi_{t=1}^{T}P(O_t|q_{t})$$ . Despite this, you should note that there are $N^T$ possible $Q$s assuming that all probabilities are nonzero [1]. Thus, the marginalization step above is rather, or can be, computationally expensive. We need another way to calculate $P(O| lambda)$. . In order to compute the probability $P(O| lambda)$ we need to know the joint probability $P(O, Q| lambda)$. The forward algorithm allows us to compute the latter without using marginalization. It does so by using recursion. We will divide the observation sequence into two parts; the first part will be $[1,t]$, the second is $[t+1, T]$ [1]. We further define the forward variable $ alpha_t(i)$. This will denote the probability of observing the partial sequence $ {O_1, cdots,O_t }$ unitl time $t$ and being in $S_i$ at time $t$ given $ lambda$: . $$ alpha_t(i) = P(O_1, cdots,O_t, q_t = S_i | lambda)$$ . This can be calculated recursively: . Initialize | . $$ alpha_t(i) = pi_ib_i(O_1)$$ . Recurse | . $$ alpha_{t+1}(j) = P(O_1, cdots,O_{t+1}, q_{t+1} = S_j | lambda)$$ . which can be written as . $$ alpha_{t+1}(j) = [ sum_{i}^{N} alpha_t(i)a_{i,j}]b_j(O_{t+1})$$ . Now $ alpha_t(i)$ explains the first $t$ observations and ends in state $S_i$. We multiply with the transition probability $a_{ij}$ in order to move to state $S_j$. since the are $N$ possible previous states we have to sum over all of them. Finally, we weight the result with $b_j(O_{t+1})$ which is the probability of observing $O_{t+1}$ at state $S_j$ at time $t+1$. . Once we know the forward variables, it is easy to calculate $P(O| lambda)$: . $$P(O| lambda) = sum_{i}^{N} alpha_{T}(i)$$ . $ alpha_{T}(i)$ is the probability of generating the full observation sequence and ending up in state $S_i$. We need to sum up over all the possible final states. The $ alpha_{t}(i)$ can be represented as a matrix of size $T times N$. Where $T$ is the size of the observation sequence and $N$ the number of states. Let&#39;s see an example. . Let&#39;s see a simple example. Assume a system with three states $S= {S_1, S_2, S_3 }$. Futher, assume that the observation sequence consists of elements from the following set $V= {a, b,c }$. Also let&#39;s assume the following HMM: . $$ boldsymbol{ pi}= begin{bmatrix}0.7 &amp; 0.15 &amp; 0.15 end{bmatrix}$$ . $$ mathbf{A}= begin{bmatrix}0.5 &amp; 0.25 &amp; 0.25 0.1 &amp; 0.8 &amp; 0.1 0.3 &amp; 0.15 &amp; 0.6 end{bmatrix}$$ . $$ mathbf{B}= begin{bmatrix} 0.16 &amp; 0.26 &amp; 0.58 0.25 &amp; 0.28 &amp; 0.47 0.2 &amp; 0.1 &amp; 0.7 end{bmatrix}$$ . We want to calculate the probability $P(O| lambda)$ where $O= {a, b, a, c, b, a }$. We will use the forward algorithm for this. We will first do the computation using pencil and paper and then write a small Python script for us. We create the matrix $ alpha$: . $$ alpha = begin{bmatrix}0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 end{bmatrix}$$ . The first step is the initialization of the matrix. We take the first observation in the sequence $O$ which &#39;a&#39;. This has to be mapped to an index. Assume that we have available such a mapping: . $$ {a:0, b:1, c:2 }$$ . Further assume that we use zero-based counting. We have . $$ alpha_{0,0} = pi_0 mathbf{B}_{0, 0} = 0.7 0.16 = 0.112$$ . $$ alpha_{0,1} = pi_1 mathbf{B}_{1, 0} = 0.15 0.25 = 0.0375 $$ . $$ alpha_{0,2} = pi_2 mathbf{B}_{2, 0} = 0.15 0.2 = 0.03$$ . we now proceed to the calculation of the probabilities of the next symbols. The symbol &#39;b&#39;, which is the next symbol in $O$ has index 1. Its probabilities are . $$ alpha_{1,0} = mathbf{B}_{0,1}( alpha_{0,0} mathbf{A}_{0,0} + alpha_{0,1} mathbf{A}_{1,0} + alpha_{0,2} mathbf{A}_{2,0}) $$ . Similarly for $ alpha_{1,1}$ and $ alpha_{1,2}$ . $$ alpha_{1,1} = mathbf{B}_{1,1}( alpha_{0,0} mathbf{A}_{0,1} + alpha_{0,1} mathbf{A}_{1,1} + alpha_{0,2} mathbf{A}_{2,1}) $$ . $$ alpha_{1,2} = mathbf{B}_{2,1}( alpha_{0,0} mathbf{A}_{0,2} + alpha_{0,1} mathbf{A}_{1,2} + alpha_{0,2} mathbf{A}_{2,2}) $$ . After filling the matrix $ alpha$ we can calculate the probability $P(O| lambda)$, This is given by the following sum: . $$P(O| lambda) = alpha_{5,0} + alpha_{5,1} + alpha_{5,2}$$ . Below is a simple Python script that performs these tedious calculations for us. . import numpy as np . obs_to_idx = {&#39;a&#39;:0, &#39;b&#39;: 1, &#39;c&#39;:2} # state to index map state_to_idx = {&#39;S1&#39;:0, &#39;S2&#39;:1, &#39;S3&#39;: 2} . pi = np.array([0.7, 0.15, 0.15]) # transition probabilities A = np.array([[0.5, 0.25, 0.25], [0.1, 0.8, 0.1], [0.3, 0.15, 0.6]]) # emission probabilties B = np.array([[0.16, 0.26, 0.58], [0.25, 0.28, 0.47], [0.2, 0.1, 0.7]]) . o = [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] . a = np.zeros(shape=(len(o),A.shape[0])) . # initialize alpha for i in range(len(state_to_idx)): # only first row the rest is zero a[0][i] = pi[i]*B[i][obs_to_idx[o[0]]] . print(&quot;Initial probabilities&quot;) print(a) . Initial probabilities [[0.112 0.0375 0.03 ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ]] . for t in range(1, len(o)): for j in range(A.shape[0]): a[t][j] = 0 # fix j = state_idx and sum over the states for i in range(A.shape[0]): a[t][j] += a[t -1][i] * A[i][j] a[t][j] *= B[j][obs_to_idx[o[i]]] print(&quot;alpha matrix: &quot;) print(a) . alpha matrix: [[1.12000000e-01 3.75000000e-02 3.00000000e-02] [1.10000000e-02 1.56250000e-02 9.95000000e-03] [1.60760000e-03 4.18562500e-03 2.05650000e-03] [2.94290000e-04 1.01471875e-03 4.10872500e-04] [5.95005800e-05 2.36744594e-04 8.43135750e-05] [1.25950115e-05 5.42294641e-05 1.78275499e-05]] . prob = 0; for i in range(A.shape[0]): prob += a[len(o)-1][i] print(&quot;Probability for sequence {0} is {1} &quot;.format(o, prob)) . Probability for sequence [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] is 8.465202543750001e-05 . Log probabilities . The forward variable, as well as the backward variable used in the backward algorithm are calculated as products of probabilities. When we have long sequences this may result in underflow [1]. In order to avoid this, we normalize $ alpha_t(i)$ by multiplying it with . $$c_t = frac{1}{ sum_{j} alpha_t(j)}$$ . After this normalization, $P(O| lambda)$ is given by . $$P(O| lambda) = frac{1}{ Pi_t c_t}$$ . or . $$logP(O| lambda) = - sum_t log c_t$$ . The motivation behind the forward algorithm is nicely presented in the following video . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EbxLWGw2zJ4&#39;, width=800, height=300) . The following video nicely explains both the forward and backward algorithms. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;gYma8Gw38Os&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Forward–backward algorithm. | Lawrence R. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20forward-algorithm/dynamic-programming/algorithms/numerics/2020/05/22/forward-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20forward-algorithm/dynamic-programming/algorithms/numerics/2020/05/22/forward-algorithm.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post63": {
            "title": "Hidden Markov models",
            "content": "Introduction to hidden Markov models . We know that given a sample of independent observations we can get the likelihood of the sample by forming the product of the likelihoods of the individual instances. However, there are situations in which the assumption of observation independence simply breaks down. An example of such a situation is when we consider words from the English dictionary. In this case, within a word, successive letters are dependent; in English h is very likely to follow t but not x. In such a scenario, it is better to assume that the sequence is generated by a parametric random process. Our goal is to establish the parameters of this process. Hidden Markov models are a way to model such a process. Let&#39;s see how. . A hidden Markov model or HMM, is a statistical model in which the system being modeled is assumed to be a Markov process. Let&#39;s call that process with $X$. The system can be on a series of states from a give state set. However, we don&#39;t know at each time instant the specific state the system is in. In other words, the system state is unobservable. HMM assumes that there is another process, say $Y$, whose behavior depends on $X$. The goal is to learn about $X$ by observing $Y$. . Let&#39;s assume that at any time the system or random process we model can be in any of $N$ distinct states. Let&#39;s denote this set with $ mathbb{S}$ . $$ mathbb{S} = {S_1, S_2, cdots,S_N }$$ . Furthermore, let&#39;s denote the state that the system is at time $t$ by $q_t$. . The system can move from one state to another. The probability of being at state $S_j$ at time $t$ depends on the values of the previous states. We express this mathematically using the following conditional probability: . $$P(q_t=S_j | q_{t-1}=S_i, q_{t-2}=S_k, cdots)$$ . A hidden Markov model assume that system states form a Markov chain. To be more specific we will restrict ourselves to the first-order Markov model which is quite frequent in practice. In this case, the probability above simply becomes: . $$P(q_t=S_j | q_{t-1}=S_i, q_{t-2}=S_k, cdots)=P(q_t=S_j | q_{t-1}=S_i)$$ . In words, what the first order Markov property tells us is that the state of the system depends solely on the previous state; a rather memoryless situation. . Let&#39;s move further by introducing the so-called transition probabilities $ alpha_{i,j}$: . $$ alpha_{i,j}=P(q_t=S_j | q_{t-1}=S_i)$$ . Since the $ alpha_{i,j}$&#39;s are probabilities they should satisfy the followin constraints . $$ alpha_{i,j} geq 0, ~~~ sum_{j=1}^{N} alpha_{i,j} = 1$$ . These are nothing more than the usual axioms of the definition of probability. Note that for the latter condition we keep the $i$ index fixed. . We will assume that the transition probabilities are independent of time. What this means is that going from $S_i$ to $S_j$ has the same probability regardless of when it happens (i.e. in the observation sequence see below). . We usually arrange the transition probabilities into an $N times N$ matrix, denoted here with $ mathbf{A}$ that its rows sum to one. . We now have a way, or a model, that allows us to move from one state to another. However, we cannot do much with it as the states are unknown, hidden, unobserved or any other expression that suits your needs. The point is that we cannot access them. In order to have progress, hidden Markov models assume a second process that produces observation sequences. We can use this process to infer the state of the system. . Let&#39;s denote by $ lambda$ ( we will be more specific about what $ lambda$ denotes further below) the HMM instance we are using. Let $O_T$ be an observation sequence of length $T$. We assume that $O_T$ has elements from a given discrete set $ mathbb{V}$: . $$ mathbb{V}= {v_1, v_2, cdots, v_M }$$ . The set $ mathbb{V}$ has in total $M$ elements. Also let&#39;s introduce the mechanism that characterizes the generation of a sequence given a state $S_j$. This is done via the so-called emission probability matrix $b_j(m)$: . $$b_j(m) = P(O_t = v_m|q_t = S_j)$$ . this is the probability that we observe element $v_m$ at time $t$ when the system is at state $S_j$. For example, let&#39;s assume that that we have two states and three symbols and we are given the following emission probabilities matrix . $$ mathbf{B}= begin{bmatrix} 0.16 &amp; 0.26 &amp; 0.58 0.25 &amp; 0.28 &amp; 0.47 end{bmatrix}$$ . what this tells us is that at state $S_1$ symbol $v_1$ has probability 0.16 to be observed, symbol $v_2$ will be observed 26 % and symbol $v_3$ will be observed 58 %. . Although, we cannot observe the state sequence $Q$, this can be inferred from the observation sequence $O$. Note however that in general there are many different sequences $Q$ that can generate the same observation sequence. This is however done with different probabilities. This is similar when we have an iid sample from, say, a normal distribution; there are an infinite number of $ mu, sigma$ pairs possible which can generate the sameple. Thus, we are more interested in a maximum likelihood state sequence or a sequence that has the maximum probability of generating the sequence $O$. . What is $ lambda$? . Above we used the notation $ lambda$ in order to indicate a specific HMM instance. Let&#39;s see what this $ lambda$ parameter actually imply. This is also a summary of the basic element of an HMM. Specifically, . An HMM model assumes a set of states in the model $ mathbb{S} = {S_1, S_2, cdots,S_N }$ | . An HMM model assumes a number of distinct observation symbols $ mathbb{V}= {v_1, v_2, cdots, v_M }$ | . An HMM model assumes the existence of transition probabilities $ mathbf{A}$ where $ alpha_{i,j}=P(q_t=S_j | q_{t-1}=S_i)$ | . An HMM model assumes the existence of observation probabilities $ mathbf{B}$ where $b_j(m) = P(O_t = v_m|q_t = S_j)$ | . The last thing we need to talk about, is how to initialize the model. This is done by a vector of initial probabilties $ boldsymbol{ pi}$ where each $ pi_i = P(q_1 = S_i)$ that is each $ pi_i$ is the probability that the first state of the model is $S_i$. . The $ lambda$ parameter is the triplett consisting of the matrices $ mathbf{A}$, $ mathbf{B}$ and the vector $ boldsymbol{ pi}$ . $$ lambda = { mathbf{A}, mathbf{B}, boldsymbol{ pi} }$$ . For a state set with $N$ states, $ mathbf{A}$ is $N times N$. Likewise for a set $V$ with $M$ symbols, $ mathbf{B}$ is $N times M$. Finally the vector $ boldsymbol{ pi}$ has size $N$. . Typically, when dealing with an HMM we are intersted in the following three problems [1] . Given an HMM i.e. $ lambda$ evaluate the probability of a given observation sequence: | $$P(O| lambda)$$ . Given an HMM and an observation sequence $O$ we want to find the state sequence $Q$ with the highest probability of producing $O$ i.e we want to find $Q$ such that | $$P(Q|O, lambda) ~~ text{is maximum}$$ . Given a training set of observation sequences $ mathbf{X}$ we want to learn the HMM that maximizes the probability of generating $ mathbf{X}$ that is we want to find $ lambda$ so that | $$P( mathbf{X}| lambda)~~ text{is maximum}$$ . Checkout the video below for a motivation about Hidden Markov models . from IPython.display import YouTubeVideo YouTubeVideo(&#39;PAngl8DZ8yk&#39;, width=800, height=300) . The following video explains the Markov property . from IPython.display import YouTubeVideo YouTubeVideo(&#39;J_y5hx_ySCg&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning/algorithms/numerics/2020/05/21/hidden-markov-model.html",
            "relUrl": "/hidden-markov-model/machine-learning/algorithms/numerics/2020/05/21/hidden-markov-model.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post64": {
            "title": "Machine Learning Notes. The  Baum-Welch algorithm",
            "content": "Overview . The third problem associated with Hidden Markov models is that of learning the optimal parameters of the model given a set of sequences. In other words, we want to learn the optimal $ mathbf{B}$ and $ mathbf{A}$ that approximate the given set of sequences. An approach to do this is using the Expectation–Maximization algorithm or EM algorithm. In the context of HMM the algorithm bears the name Baum–Welch algorithm which is a special instance of the EM algorithm. The following video explains the method. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;JRsdt05pMoI&#39;, width=800, height=300) . Baum-Welch algorithm . In the so-called training problem we want to calculate $ lambda$ that maximizes the likelihood of the sample of training sequences $ mathbf{x} = {O_k }$ i.e. we want to maximize $P( mathbf{X}| lambda)$ [1]. Let&#39;s see how this can be done. Let the variable $ xi_t(i,j)$ denote the probability of being at $S_i$ at time $t$ and in $S_j$ at time $t+1$ given $O$ and $ lambda$ i.e. . $$ xi_t(i,j) = P(q_t = S_i, q_{t+1} = S_j | O, lambda)$$ . Note that here $O in mathbf{X}$ denotes just one observation sequence. This can be computed as [1] . $$ xi_t(i,j) = frac{ alpha_t(i)a_{ij}b_j(O_{t+1}) beta_{t+1}(j)}{ sum_{k=1}^{N} sum_{l=1}^N alpha_t(k)a_{kl}b_l(O_{t+1}) beta_{t+1}(l)}$$ . . Remark . Recall that the probability $P(X,Y|Z)$ can be calculated according to . $$P(X,Y|Z) = P(X|Y,Z)P(Y|Z)$$ . from this formula we can write: . $$P(X|Y,Z) = frac{P(X,Y|Z)}{P(Y|Z)}$$ . Thus, . $$P(q_t = S_i , q_{t+1} = S_j | O, lambda) = frac{P(q_t = S_i, q_{t+1} = S_j , O | lambda)}{P(O| lambda)}$$ . The numerator of the equation avove can be expressed using the forward and backward probabilities. . . $ alpha_t(i)$ and $ beta_{t+1}(l)$ are simply the forward and backward variables we saw in the forward and backawrds algorithms respectively. The probability of being in state $S_i$ at time $t$, i.e. $P(q_t=S_i|O, lambda)$, is given by marginalizing over the probabilities for all possible next states [1]: . $$ gamma_t(i) = sum_{j=1}^N xi_t(i,j)$$ . The Baum-Welch algorithm is an EM method. At each iteration we have two steps Expectation, or E-step, followed by a Maximization, or M-step. At E-step, one computes $ xi_t(i,j)$ and $ gamma_t(i)$ values given the current $ lambda$. At the M-step we recalculate $ lambda$ given the estimated $ xi_t(i,j)$ and $ gamma_t(i)$. The E and M steps are alternated until some specified threshold is reached at which point convergence of the algorithm is proclaimed. . Let&#39;s introduce the following two variables [1] . $$z_{i}^{t} = begin{cases} 1, text{if}~ q_t = S_i 0, text{otherwise} end{cases}$$ . and . $$Z_{i,j}^{t} = begin{cases} 1, text{if}~ q_t = S_i ~ text{and}~ q_{t+1}=S_j 0, text{otherwise} end{cases}$$ . We estimate these variables in the E-step as, see [1], . $$E left[z_{i}^{t} right] = gamma_t(i)$$ . $$E left[Z_{ij}^{t} right] = xi_t(i,j)$$ . At the M-step, given the estimation of $ xi$ and $ gamma$, we calculate the parameters of $ lambda$ as follows, see [1]: . $$ hat{a}_{ij} = frac{ sum_{t=1}^{T-1} xi_t(i,j)}{ sum_{t=1}^{T-1} gamma_t(i)}$$ . . Remark . Recall that the transition probability from state $S_i$ to state $S_j$, i.e. $a_{ij}$, is the number of transitions from $S_i$ to $S_j$ divided by the totlal number of transitions from $S_i$ over all sequences: . $$ a_{ij} = frac{ sum_k sum_{t=1}^{T-1} I(q_{t}^k = S_i, q_{t+1}^k = S_j)}{ sum_k sum_{t=1}^{T-1} I(q_{t}^k = S_i )}$$ . . Similarly, we calculate the probability of observing $O_m$ in $S_j$. This is the expected number of times $O_m$ is observed when the system is in $S_j$ over the total number of times the system is in $S_j$ [1]: . $$ hat{b}_j(m) = frac{ sum_{t=1}^T gamma_t(j)I(O_t=O_m)}{ sum_{t=1}^T gamma_t(j)}$$ . In the case where we have more than one observation sequences, say we have $K$ of these in total at our disposal, then the parameters are evaluated as averages over all observations in all sequences [1]: . $$ hat{a}_{ij} = frac{ sum_{k=1}^K sum_{t=1}^{T-1} xi_{t}^k(i,j)}{ sum_{k=1}^K sum_{t=1}^{T-1} gamma_{t}^k(i)}$$ . $$ hat{b}_j(m) = frac{ sum_{k=1}^K sum_{t=1}^T gamma_{t}^k(j)I(O_{t}^{k}=O_m)}{ sum_{k=1}^K sum_{t=1}^T gamma_{t}^{k}(j)}$$ . $$ hat{ pi}_i = frac{ sum_{k=1}^K gamma_{1}^k(i)}{K}$$ . Example . We will continue with the example we used in the Viterbi path notebook. . import numpy as np . obs_to_idx = {&#39;normal&#39;:0, &#39;cold&#39;: 1, &#39;dizzy&#39;:2} # state to index map state_to_idx = {&#39;Healthy&#39;:0, &#39;Fever&#39;:1} . pi = np.array([0.6, 0.4]) # transition probabilities A = np.array([[0.7, 0.3], [0.4, 0.6]]) # emission probabilties B = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]) . # the sequence X = [[&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;], [&#39;normal&#39;, &#39;cold&#39;, &#39;cold&#39;], [&#39;cold&#39;, &#39;cold&#39;, &#39;dizzy&#39;], [&#39;normal&#39;, &#39;normal&#39;, &#39;normal&#39;]] . def idx_to_obs(idx): global obs_to_idx for item in obs_to_idx: if obs_to_idx[item] == idx: return item return None . def forward(o, A, B, pi, state_to_idx, obs_to_idx): a = np.zeros(shape=(len(o), A.shape[0])) for i in range(len(state_to_idx)): a[0][i] = pi[i] * B[i][obs_to_idx[o[0]]] for t in range(1, len(o)): for j in range(A.shape[0]): a[t][j] = 0 # fix j = state_idx and sum over the states for i in range(A.shape[0]): a[t][j] += a[t - 1][i] * A[i][j] a[t][j] *= B[j][obs_to_idx[o[t]]] return a . def backward(o, A, B, obs_to_idx): beta = np.zeros(shape=(len(o), A.shape[0])) # initialize beta beta[len(o) - 1] = np.ones(A.shape[0]) for t in range(len(o) - 2, -1, -1): for j in range(A.shape[0]): beta[t, j] = (beta[t + 1] * B[:, obs_to_idx[o[t + 1]]]).dot(A[j, :]) return beta . def compute_xi_denom(alpha, A, B, beta, t, idx, idx2): denom = 0.0 for i in range(A.shape[0]): for j in range(A.shape[0]): denom += alpha[t, i]*A[i, j]*B[j, idx]*beta[idx2, j] return denom . def sequnce_xi_gamma_calculation(o, alpha, beta, A, B, obs_to_idx): N = A.shape[0] T = len(o) xi = [] #[np.zeros((N, N))]*(T-1) gamma = np.zeros((T-1, N)) # calculate xi and gamma for the sequence for t in range(T-1): local_xi = np.zeros((N, N)) for i in range(N): for j in range(N): numerator = alpha[t, i] * A[i, j] * B[j, obs_to_idx[o[t + 1]]] * beta[t + 1, j] denom = compute_xi_denom(alpha=alpha, A=A, B=B, beta=beta, t=t, idx=obs_to_idx[o[t + 1]], idx2=t+1) local_xi[i, j] = numerator / denom for k in range(N): gamma[t][i] += local_xi[i][k] xi.append(local_xi) return xi, gamma . def get_extra_gammas(alpha, beta, A, B, obs_to_idx, X): N = A.shape[0] extra_gammas = [] for k in range(len(X)): gamma_k = [0.0 for i in range(N)] seq = X[k] T = len(seq) t = T-2 # we need to compute the xi # for the last observation xi_k = np.zeros(shape=(N, N)) for i in range(N): for j in range(N): numerator = alpha[t, i] * A[i, j] * B[j, obs_to_idx[seq[t + 1]]] * beta[t + 1, j] denom = compute_xi_denom(alpha=alpha, A=A, B=B, beta=beta, t=t, idx=obs_to_idx[seq[t + 1]], idx2=t+1) xi_k[i, j] = numerator / denom for kg in range(N): gamma_k[i] += xi_k[i][kg] extra_gammas.append(gamma_k) return extra_gammas . def baum_welch(X, A, B, pi, n_iter, state_to_idx, obs_to_idx): print(&quot;Number of sequences: &quot;, len(X)) N = A.shape[0] M = B.shape[1] print(&quot;shape A: &quot;, A.shape) print(&quot;shape B: &quot;, B.shape) #import pdb #pdb.set_trace() for itr in range(n_iter): print(&quot;================&quot;) print(&quot;Iteration: &quot;, itr) # collect the xis and gammas for # evety sequence given in this iteration xis = [] gammas = [] for seq in range(len(X)): o = X[seq] T = len(o) alpha = forward(o, A, B, pi=pi, state_to_idx=state_to_idx, obs_to_idx=obs_to_idx) beta = backward(o, A, B, obs_to_idx=obs_to_idx) assert alpha.shape[0] == len(o), &quot;Invalid number of rows for alpha mat: {0} should be {1}&quot;.format(alpha.shape[0], len(o)) assert alpha.shape[1] == N, &quot;Invalid number of columns for alpha mat: {0} should be {1}&quot;.format( alpha.shape[1], N) assert beta.shape[0] == len(o), &quot;Invalid number of rows for beta mat: {0} should be {1}&quot;.format(beta.shape[0], len(o)) assert beta.shape[1] == N, &quot;Invalid number of columns for beta mat: {0} should be {1}&quot;.format( beta.shape[1], N) # calculate xi and gamma for this observation sequence xi, gamma = sequnce_xi_gamma_calculation(o=o, alpha=alpha, beta=beta, A=A, B=B, obs_to_idx=obs_to_idx) # xi is an array of length T-1 assert len(xi) == T - 1, &quot;Invalid size of xi array: {0} should be {1}&quot;.format(len(xi), T - 1) # for each t we computed xi which is an NxN matrix assert xi[0].shape == (N, N), &quot;Invalid xi matrix shape: {0} should be {1}&quot;.format(xi[0].shape, (N,N)) # gamma is a matrix T-1 x N assert gamma.shape == (T - 1, N), &quot;Invalid gamma matrix shape: {0} should be {1}&quot;.format(gamma.shape, (T - 1, N)) xis.append(xi) gammas.append(gamma) assert len(gammas) == len(X), &quot;Invalid number of gammas {0} should be {1}&quot;.format(len(gammas), len(X)) extra_gammas = get_extra_gammas(alpha=alpha, beta=beta, A=A, B=B, obs_to_idx=obs_to_idx, X=X) assert len(extra_gammas) == len(X), &quot;Invalid number of extra gammas {0} should be {1}&quot;.format(len(extra_gammas), len(X)) # update the transition probabilities # the calculations for A have up to T -1 for i in range(N): denom = 0.0 for k in range(len(X)): obs = X[k] T = len(obs) gamma_k = gammas[k] assert gamma_k.shape == (T - 1, N), &quot;Invalid gamma matrix &quot; &quot;shape: {0} should be {1}&quot;.format(gamma_k.shape, (T - 1, N)) for obs_item in range(T - 1): denom += gamma_k[obs_item][i] for j in range(A.shape[1]): nom = 0.0 for k in range(len(X)): obs = X[k] xi_k = xis[k] T = len(obs) assert len(xi_k) == T - 1, &quot;Invalid number of &quot; &quot;xi_k&#39;s: {0} should be {1}&quot;.format(len(xi_k), T - 1) for t in range(T - 1): nom += xi_k[t][i, j] A[i, j] = nom/denom # update the gammas so that we have the # final observations for k in range(len(gammas)): gamma_k = gammas[k] gamma_k = np.vstack([gamma_k, extra_gammas[k]]) gammas[k] = gamma_k # in order to calculate B we also need the final # bits for gammas for i in range(N): denom = 0.0 for k in range(len(X)): seq = X[k] gamma_k = gammas[k] assert len(seq) == gamma_k.shape[0], &quot;Invalid number of Gamma rows {0} should be {1}&quot;.format(gamma_k.shape[0], len(seq)) for t in range(len(seq)): denom += gamma_k[t, i] for j in range(M): nom = 0.0 for k in range(len(X)): seq = X[k] for t in range(len(seq)): obs = seq[t] if obs == idx_to_obs(j): nom += gammas[k][t, i] B[i, j] = nom/denom # update the pi vector for i in range(N): pi_val = 0.0 for k in range(len(X)): gamma_k = gammas[k] pi_val += gamma_k[0][i] pi[i] = pi_val/(len(X)) return A, B, pi . baum_welch(X=X, A=A, B=B, pi=pi, n_iter=10, state_to_idx=state_to_idx, obs_to_idx=obs_to_idx) . Number of sequences: 4 shape A: (2, 2) shape B: (2, 3) ================ Iteration: 0 ================ Iteration: 1 ================ Iteration: 2 ================ Iteration: 3 ================ Iteration: 4 ================ Iteration: 5 ================ Iteration: 6 ================ Iteration: 7 ================ Iteration: 8 ================ Iteration: 9 . (array([[0.60187444, 0.39812556], [0.71039728, 0.28960272]]), array([[0.50518563, 0.30097656, 0.19383781], [0.06660769, 0.87417724, 0.05921507]]), array([0.96748237, 0.03251763])) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Wikipedia-Baum–Welch algorithm. |",
            "url": "https://pockerman.github.io/qubit_opus/machine-learning/hidden-markov-model/algorithms/2020/05/13/ml-notes-hmm-baum-welch-algorithm.html",
            "relUrl": "/machine-learning/hidden-markov-model/algorithms/2020/05/13/ml-notes-hmm-baum-welch-algorithm.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post65": {
            "title": "Machine Learning Notes. Confusion Matrix",
            "content": "Overview . More than often, we want to evaluate the performance of a classifier. This can be summarised by using a table knowm as contingency table or confusion matrix . Confusion matrix . A confusion matrix is a table where each row refers to actual classes as recorder in the test set, and each column to classes as predicted by the classifier. When we have the confusion matrix computed, we can extract various perormance metrics. We will illustrate this with an example concerning two-class classification. Most, however not all, of the metrics can easilly be extended to more than two classes classification. The following table summarizes various metrics . Accuracy . Accuracy is perhaps the simplest of the metrics we can compute [2]. It is defined s the proportion of correctly classified instances: . $$ text{Accuracy} = frac{ text{Number of correctly classified examples}}{ text{Total number of examples}}$$ . Similarly, we can define the error rate as the proportion of incorrectly classified insrances. Clearly, this will be given by: . $$ text{Error rate} = 1 - text{accuracy}$$ . Conceptually, we can view accuracy as an estimate of the probability that an arbitrary instance $ mathbf{x} in mathbf{X}$ is classified correctly i.e. as the probability [2] . $$P( hat{c}( mathbf{x}) = c( mathbf{x})| hat{f})$$ . References . Confusion matrix | Peter Flach, Machine Learning The Art and Science of Algorithms that Make Sense of Data, Cambridge Press |",
            "url": "https://pockerman.github.io/qubit_opus/machine-learning/confusion-matrix/classifier-assessment/2020/03/25/ml-confusion-matrix.html",
            "relUrl": "/machine-learning/confusion-matrix/classifier-assessment/2020/03/25/ml-confusion-matrix.html",
            "date": " • Mar 25, 2020"
        }
        
    
  
    
        ,"post66": {
            "title": "Matrix approximation with SVD",
            "content": "Overview . Very often in numerical modeling, the data we need to work with is rather large and therefore not really handy. Moreover, the behavior under investigation can be explained by small number of features. Thus, it is desireable to be able to approximate the matrices. One way to do so is using the singular value decomposition method. . Matrix approximation with SVD . In fact SVD provides an optimal low-rank approaximation to a matrix $A$ (this is the Eckart-Young theorem). We can obtain a hierarchy of low rank matrices by just keeping the leading $k$ singular values and the corresponding eigenvectors. . Image copression is a simple example illustrating matrix approximation using SVD. We can view a grayscale image as matrix $A in mathbb{R}^{n times m}$ where $n, m$ are the number of pixels in the vertical and horizonal directions. . The Python code below computes the full SVD of the matrix representing the loaded image. We then compute approximations of the image using a range of retained singular values. We can see that as the number of retained singular values increases the quality of the image increases. . import numpy as np from matplotlib import image import matplotlib.pyplot as plt from numpy.linalg import matrix_rank . def rgb2gray(rgb): return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]) . A = image.imread(&#39;my_icons/volvo_img.png&#39;) A = np.mean(A, -1)#rgb2gray(rgb=data) # summarize shape of the pixel array print(A.shape) # display the array of pixels as an image plt.imshow(A) plt.show() . (366, 424) . print(&quot;Matrix rank &quot;, matrix_rank(A)) . Matrix rank 307 . U, S, V = np.linalg.svd(A, full_matrices=False) S = np.diag(S) . print(&quot;Shape U &quot;, U.shape) print(&quot;Shape S &quot;, S.shape) print(&quot;Shape V &quot;, V.shape) . Shape U (366, 366) Shape S (366, 366) Shape V (366, 424) . for r in [5, 20, 100]: print(&quot;Working with r&quot;, r) # construct approximate image img_approx = U[:,:r] @ S[0:r,:r] @ V[:r,:] plt.imshow(img_approx) plt.show() . Working with r 5 . Working with r 20 . Working with r 100 . plt.semilogy(np.diag(S)) plt.show() . plt.plot(np.cumsum(np.diag(S))/np.sum(np.diag(S))) plt.show() . References .",
            "url": "https://pockerman.github.io/qubit_opus/linear-algebra/singular-value-decomposition/matrix-approximation/algorithms/numerics/2020/03/14/matrix-approximation.html",
            "relUrl": "/linear-algebra/singular-value-decomposition/matrix-approximation/algorithms/numerics/2020/03/14/matrix-approximation.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post67": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pockerman.github.io/qubit_opus/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi and welcome to my blog. This is Alex. I am a self-taught, at least to a large extent, software engineer. I am really intersted in developing scientific applications (that do not involve drop down menus), mathematical modeling, machine learning and AI. I am also heavily interested in robotics, here is my non-working and still under-development-whenever-I-have-the-time, attempt. Admitedly, I still have many thing to learn. In this blog, I write for myself, things that I work, or worked, on and find difficult to grasp, or things that I read and want to make my understanding more tangible by…explaining them to myself. In any case, if you find useful any information in here, feel free to use it. . You can reach me, optional :), via Linkedin or via GitHub. .",
          "url": "https://pockerman.github.io/qubit_opus/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "A brief list of past and current projects .",
          "url": "https://pockerman.github.io/qubit_opus/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  

  

  

  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pockerman.github.io/qubit_opus/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}