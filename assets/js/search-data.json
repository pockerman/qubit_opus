{
  
    
        "post0": {
            "title": "Controllability",
            "content": "When we want to control a dynamical system, the natural question that arises is to what extent can we achive this? The term controllability is used to describe whether a system is controllable altogether. We will see that whether a system is controllable or not is determined completely by the controllability matrix [1]. Let&#39;s consider the following linear system . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . We can understand whether the system above is controllable or not by examing the controllability matrix $ mathcal{ mathbf{C}}$. In particular, the column space of that matrix. This matrix is defined as follows [1] . $$ mathcal{ mathbf{C}} = begin{bmatrix} mathbf{B} &amp;&amp; mathbf{AB} &amp;&amp; mathbf{A}^2 mathbf{B} &amp;&amp; dots &amp;&amp; mathbf{A}^{n-1} mathbf{B} end{bmatrix}$$ . Where $n$ is the number of state variables. If the controllability matrix has $n$ linearly independent columns, then the system under consideration is controllable [1]. Note that this does not mean that the columns of $ mathcal{ mathbf{C}}$ should be linearly independent. All that we require, is that we can find $n$ linearly independent columns (see example 2 below). Let&#39;s see two examples taken from [1]. . Example 1 . Let&#39;s consider the following system . $$ frac{d}{dt} begin{bmatrix}x_1 x_2 end{bmatrix} = begin{bmatrix}1 &amp;&amp; 0 0 &amp;&amp; 2 end{bmatrix} begin{bmatrix}x_1 x_2 end{bmatrix} + begin{bmatrix}0 1 end{bmatrix} u$$ . Immediatelly, we can see that the system is not controllable. This is because the two state variables are decoupled and the control input affects only $x_2$. The controllability matrix is . $$ mathcal{ mathbf{C}} = begin{bmatrix}0 &amp;&amp; 0 1 &amp;&amp; 2 end{bmatrix} $$ . and we can see that the columns of that matrix are not independent. . Example 2 . If we include a control signal for the both state variables, we can turn this system into a controllable one. The new system now is . $$ frac{d}{dt} begin{bmatrix}x_1 x_2 end{bmatrix} = begin{bmatrix}1 &amp;&amp; 0 0 &amp;&amp; 2 end{bmatrix} begin{bmatrix}x_1 x_2 end{bmatrix} + begin{bmatrix}1 &amp;&amp; 0 0 &amp;&amp; 1 end{bmatrix} begin{bmatrix} u_1 u_2 end{bmatrix} u$$ . By including a control signal for bith state variables, we can control them independently. The controllability matrix now becomes [1] . $$ mathcal{ mathbf{C}} = begin{bmatrix}1 &amp;&amp; 0 &amp;&amp; 1 &amp;&amp; 0 0 &amp;&amp; 1 &amp;&amp; 0 &amp;&amp; 2 end{bmatrix} $$ . and we can verify that the columns of this matrix do span $ mathbb{R}^2$. Note that the columns of the matrix above are not linearly independent. Indeed the third column is a copy of the first and the fourth is a product of the second. However, the first two columns do span $ mathbb{R}^2$. . Three equivalent conditions . The span of the columns of matrix $ mathcal{ mathbf{C}}$ forms a Krylov subspace [1]. The space determines which state vectors can be controlled. Hence, controllability implies two things [1] . Eigenvalue placement | Any state vector $ boldsymbol{ xi} in mathbb{R}^n$ is reachable with some actuation signal | . The following three conditions are equivalent [1] . Controllability | Arbitrary eigenvalue placement | Reachability of $ mathbb{R}^n$ | . We already saw what controllability means in terms of the controllability matrix. Arbitrary eigenvalue placement means that we can design the eigenvalues of the system through the choice of the feedback signal $ mathbf{u}$. In particular, for $ mathbf{u} = - mathbf{K} mathbf{x}$, the system becomes . $$ frac{d mathbf{x}}{dt} = ( mathbf{A} - mathbf{B} mathbf{K}) mathbf{x}$$ . The matrix $ mathbf{K}$ is called the gain matrix. There are various methods to design this matrix. Finally, reachability in practical terms means that we can steer the system to any arbitrary state with some actuation signal, or, conversely, there exists an actuation signal so that the system can be pushed to an arbitrary state $ boldsymbol{ xi} in mathbb{R}^n$ [1]. . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/control/controllability/linear-dynamical-systems/2021/04/11/controllability.html",
            "relUrl": "/control/controllability/linear-dynamical-systems/2021/04/11/controllability.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Longitudinal Car Model",
            "content": "Longitudinal car model . In this section, we will go over the concept of the vehicle longitudinal dynamics. The two major elements of the longitudinal vehicle model discussed in this section are . Vehicle dynamics | Powertrain dynamics | . The vehicle dynamics are influenced by longitudinal tire forces, aerodynamic drag forces, rolling resistance forces and gravitational forces. The longitudinal powertrain system of the vehicle consists of the internal combustion engine, the torque converter, the transmission and the wheels. This video explains nicely the concepts. . The longitudinal vehicle dynamic model is simply based on the dynamics of the vehicle that generate forward motion. The following figure shows a typical vehicle longitudinal motion force diagram on an inclined road. . Figure 1. Schematics of vehicle logitudinal model on an inclined road. Image from [1]. . We have the followig forces acting on the vehicle . The front tire forces $F_{xf}$ | The rear tire forces $F_{xr}$ | The aerodynamic drag force $F_{aero}$ | The rolling resistance forces $R_{xf}$ and $R_{xr}$ | The force due to gravity $F_g$ | . According to Newton’s laws of motion, and in particular the second law, the longitudinal tire forces of the front and rear tyres, $F_{xf}$ and $F_{xr}$, should balance the resistance forces $F_{aero}$, the gravitational force $F_g$ , and the rolling resistance of the front and rear tires, $R_{xf}$ and $R_{xr}$. Any imbalance between these forces creates an acceleration of the vehicle in the longitudinal direction denoted by $ ddot{x}$. Thus, the basic logintudinal motion model is given by . $$m ddot{x} = F_{xf} + F_{xr} - F_{aero} - F_g - R_{xf} - R_{xr}$$ . where $m$ is the mass of the vehicle. The forces $F_{xf}$ and $F_{xr}$ come from the vehicle power train. We can express them collectively as $F_x$. Furthermore, we group together the rolling resistance forces under the symbol $R_x$. Thus, the reduced model is . $$m ddot{x} = F_x - F_{aero} - F_g - R_x $$ . We will need a way to express the involved quantities in order to be able to solve for $ ddot{x}$. Let&#39;s start with the gravitational force $F_g$. . Gravitational froce . We can express $F_g$ as [2] . $$F_g = mg sin ( alpha)$$ . where $ alpha$ is the local road slope. For small slope angles, we can write . $$sin ( alpha) approx alpha$$ . Aerodynamic drag . A vehicles longitudinal motion is resisted by aerodynamic drag rolling resistance and the force due to gravity. The aerodynamic drag force $F_{aero}$ is typically modeled as dependent on air density $ rho$, frontal area of the vehicle $A$, the vehicles coefficient of friction $C_D$, and the current speed of the vehicle. The functional relationship of all these quantities is given in the equation beow . $$F_{aero} = frac{1}{2}C_D rho A v^2$$ . Rolling resistance . For rolling resistance, we have a similar model which can depend on the normal force, tire pressure and characteristics, and vehicle speed. . $$R_x = N(c_{r, 0} + c_{r,1}| dot{x}| + c_{r,2}| dot{x}|^2 approx c_{r,1}| dot{x}|$$ . If we again assume nominal operating conditions and drop the second-order terms for simplicity, we can arrive at a linear rolling resistance model, where $c_{r,1}$ is the linear rolling resistance coefficient. In both cases, these are basic approximate models that are a good starting point for controller design. In practice, the fidelity of the model used depends on the accuracy required of the controller or the simulation environment. . Tire forces . We now discuss the longitudinal tire forces expressed under the term $F_x$. Longitudinal tire forces depend on the following factors [2] . Slip ratio | Normal load on the tires | Friction coefficient on the tire road interface | . Let&#39;s see these components . Slip ratio . For an effective wheel radius $R_{effective}$ and a wheel velocity $ omega_w$ the velocity is described by . $$V = R_{effective} omega_w$$ . However, the actual longitudinal velocity at the axle of the wheel, $V_x$ may be different than that. This is called longitudinal slip [2]. In other words, the longitudinal slip is defined as [2] . $$ sigma = V - V_x$$ . We can further distinguish between slip ratios during braking and acceleration according to [2] . An explanation of why longitudinal tire force depends on the slip ratio is provided in section 4.1.3. A more complete understanding of the influence of all three variables – slip ratio, normal force and tire-road friction coefficient – on tire force can be obtained by reading Chapter 13 of this book. . Powertrain forces . The longitudinal tire forces acting on the driving wheels are the main forces that drive the vehicle forward [2]. These forces depend on the difference between the rotational wheel velocity $R_{effective} omega_{w}$ and the vehicle longitudinal velocity $ dot{x}$. In particular, we saw that we can model the longitudinal tire forces as . $$F_{xf} = C_{ sigma f} sigma_{xf}, ~~ F_{xr} = C_{ sigma r} sigma_{xr}$$ . where $C_{ sigma f}$ and $C_{ sigma r}$ are called the longitudinal tire stiffness parameters of the front and rear tires respectively [2]. We saw then that these coefficients can be written as [2] . $$ sigma_{xf} = begin{cases} frac{R_{effecive} omega_{wf} - dot{x}}{ dot{x}}, ~~ text{during breaking} frac{R_{effecive} omega_{wf} - dot{x}}{R_{effecive} omega_{wf}}, ~~ text{during acceleration} end{cases}$$ . and a similar expression for $ sigma_{rf}$. However, $ omega_w$ is highly influence by the powertrain dynamics of the vehicle. The powertrain has the following major components [2] . Engine | Transmission or gearbox | Torque converter or clutch | Differential | . Figure 2. Powertrain schematics. Image from [1]. . Let&#39;s see each of the components separately . Torque converter . The torque converter is a type of fluid coupling that connects the engine to the transmission. If the engine is turning slowly, such as when the car is idling at a stoplight, the amount of torque passed through the torque converter is very small, so keeping the car still requires only a light pressure on the brake pedal. In addition to allowing the car come to a complete stop without stalling the engine, the torque converter gives the car more torque when it accelerates out of a stop. Modern torque converters can multiply the torque of the engine by two to three times. This effect only happens when the engine is turning much faster than the transmission. At higher speeds, the transmission catches up to the engine, eventually moving at almost the same speed. Ideally, though, the transmission should move at exactly the same speed as the engine, because the difference in speed wastes power. To counter this effect, many cars have a torque converter with a lockup clutch. When the two halves of the torque converter get up to speed, this clutch locks them together, eliminating the slippage and improving efficiency. The torque converter is typically unlocked as soon as the driver removes his/her foot from the accelerator pedal and steps on the brakes. This allows the engine to keep running even if the driver brakes to slow the wheels down. . Transmission dynamics . Let&#39;s denote with $GR$ the gear ratio of the transmission. In general, $GR &lt; 1$ and increases as the gear shifts upwards. The input to the transmission module is the torbine torque $T_{turbine}$ [2]. The torque transmitted to the wheels is $T_{wheels}$. Then, at steady state, this torque is given by . $$ T_{wheels} = frac{1}{GR} T_{turbine}$$ . Furthermore, we have the following relaton between the transmission and the wheel speed [2] . $$ omega_{transmission} = frac{1}{GR} omega_{wheels}$$ . Note that these equations cannot be used during gear change. See [2 page 105] for a model based on first order equations. . Engine dynamics . A simplified engine dynamic model is . $$J_w dot{ omega}_e = T_{engine} - T_{pump}$$ . In general, the engine torque $T_{engine}$ depends on the dynamics in the intake and exhaust manifold of the engine and on the accelerator input from the driver [2]. $T_{pump}$ is the torque from the pump is the load of the engine from the torque converter [2]. . Wheel dynamics . Refernces . Lesson 4: Longitudinal Vehicle Modeling | Rajamani R. Longitudinal Vehicle Dynamics. In: Vehicle Dynamics and Control., Mechanical Engineering Series. Springer 2012. |",
            "url": "https://pockerman.github.io/qubit_opus/longitudinal-dynamics/autonomous-vehicles/mathematical-modelling/vehicle-dynamics/2021/04/10/longitudinal-vehicle-model.html",
            "relUrl": "/longitudinal-dynamics/autonomous-vehicles/mathematical-modelling/vehicle-dynamics/2021/04/10/longitudinal-vehicle-model.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Linear Time-Invariant Systems",
            "content": "Linear time-invariant systems . Linear systems form a cornerstone of mathematical modelling of dynamical systems. Indeed a system or some aspects of it can be modelled using a linear model. Furthermore, non-linear systems can be linearized around a certain mode of operation. In this section, we give a brief overview of linear time-invariant systems. The major advantage of linear systems is that in terms of analysis a far simpler. Moreover, understanding the dynamics and thus stability of the system is easier. . Let&#39;s consider the following system . $$ frac{d mathbf{x}}{dt} = mathbf{f}( mathbf{x}, mathbf{u}), ~~ mathbf{y} = mathbf{g}( mathbf{x}, mathbf{u})$$ . As before, $ mathbf{x}$ represents the state of the modelled system whilst $ mathbf{u}$ represents some control input to the system. Note that the right-hand side terms do not depend explicitly on the time variable $t$. . If $ mathbf{f}$ or $ mathbf{g}$ or both are non-linear, then the system is non-linear. In this case, we can linearize the system i.e. its dynamics, using a Taylor series expansion near a fixed point $( bar{ mathbf{x}}, bar{ mathbf{u}})$. Recall that at a fixed point is a point where . $$ mathbf{f}( bar{ mathbf{x}}, bar{ mathbf{u}}) = mathbf{0}$$ . The linear, or linearized, dynamics can be written in the following matrix form (assuming no errors) . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x} + mathbf{B} mathbf{u}, ~~ mathbf{y} = mathbf{C} mathbf{x} + mathbf{D} mathbf{u}$$ . Unforced dynamics . When $ mathbf{u} = mathbf{0}$ and when there are no measurement errors i.e. $ mathbf{y} = mathbf{x}$. The system reduces to . $$ frac{d mathbf{x}}{dt} = mathbf{A} mathbf{x}$$ . The solution to this ODE is [1] . $$ mathbf{x}(t) = e^{ mathbf{A}t} mathbf{x}(0)$$ . Thus $ mathbf{x}(t)$ depends or is determined entirely by the matrix $ mathbf{A}$. The stability of the unforced system therefore, can be understood via the eigenvalues and eigenvectors of $ mathbf{A}$. In particular we have the following cases . All the eigenvalues $ lambda$ satisfy $Re( lambda) &lt; 0$. Then the system is stable and all solutions decay to $ mathbf{u} = mathbf{0}$ as $t rightarrow infty$ | There exists at least one eigenvalue $ lambda$ with $Re( lambda) &gt; 0$ then the system is unsatble and will diverge from the fixed point along the corresponding unstable eigenvector direction. | . Forced dynamics . Now let&#39;s assume that $ mathbf{u} neq mathbf{0}$ and that $ mathbf{x}(0) = mathbf{0}$. In this case the solution up to time $t$ is given by [1] . $$ mathbf{x}(t) = int_{0}^t e^{ mathbf{A}(t - tau)} mathbf{B} mathbf{u}( tau)d tau $$ . This integral is nothing more than a convolution. Thus, we can write . $$ mathbf{x}(t) = e^{ mathbf{A}t} mathbf{B} * mathbf{u}(t)$$ . References . Steven L. Brunton, J. Nathan Kutz, Data-Driven Science and Engineering. Machine Learning, Dynamical System and Control, Cambridge University Press. |",
            "url": "https://pockerman.github.io/qubit_opus/dynamical-systems/linear-systems/ode/time-invariant/2021/04/10/linear-time-invariant-systems.html",
            "relUrl": "/dynamical-systems/linear-systems/ode/time-invariant/2021/04/10/linear-time-invariant-systems.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Η Τραγώδια Τής Κύπρου",
            "content": "from IPython.display import YouTubeVideo YouTubeVideo(&#39;UAdKpsdCQtc&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;00iwLvhAomQ&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;dDvmbjiorEM&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;BIs2Y_ndq7w&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;9pdh_XnoFpM&#39;, width=800, height=300) . from IPython.display import YouTubeVideo YouTubeVideo(&#39;f-pKhRLp4ko&#39;, width=800, height=300) .",
            "url": "https://pockerman.github.io/qubit_opus/%CE%B9%CF%83%CF%84%CE%BF%CF%81%CE%AF%CE%B1/%CE%BA%CF%8D%CF%80%CF%81%CE%BF%CF%82/%CE%B5%CE%B9%CF%83%CE%B2%CE%BF%CE%BB%CE%AE/2021/04/09/tragodia-kipros.html",
            "relUrl": "/%CE%B9%CF%83%CF%84%CE%BF%CF%81%CE%AF%CE%B1/%CE%BA%CF%8D%CF%80%CF%81%CE%BF%CF%82/%CE%B5%CE%B9%CF%83%CE%B2%CE%BF%CE%BB%CE%AE/2021/04/09/tragodia-kipros.html",
            "date": " • Apr 9, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Lorenz System Simulation",
            "content": "The Lorenz system is a system of ordinary differential equations first studied by Edward Lorenz. The system of ODEs is given below . $$ dot{x} = sigma(y -x), ~~ dot{y} = x ( rho - z) -y, ~~ dot{z} = xy - beta z$$ . The model has three parameters i.e. $ sigma, rho$ and $ beta$. . It is notable for having chaotic solutions for certain parameter values and initial conditions. In particular, the Lorenz attractor is a set of chaotic solutions of the Lorenz system. In popular media the &#39;butterfly effect&#39; stems from the real-world implications of the Lorenz attractor, i.e. that in any physical system, in the absence of perfect knowledge of the initial conditions (even the minuscule disturbance of the air due to a butterfly flapping its wings), our ability to predict its future course will always fail. This underscores that physical systems can be completely deterministic and yet still be inherently unpredictable even in the absence of quantum effects. The shape of the Lorenz attractor itself, when plotted graphically, may also be seen to resemble a butterfly. . import numpy as np from mpl_toolkits import mplot3d import matplotlib.pyplot as plt . class ODE45(object): def __init__(self, dt, n_steps, rhs, y0) -&gt; None: self._dt = dt self._n_steps = n_steps self._rhs = rhs self._time = 0.0 self._yold = y0 def step(self): k1 = self._k1(t=self._time) k2 = self._k2(t=self._time, k1=k1) k3 = self._k3(t=self._time, k2=k2) k4 = self._k4(t=self._time, k3=k3) self._yold += k1/6. + k2/3. + k3/3. + k4/6. def integrate(self) -&gt; None: self._time = 0.0 solutions = [[self._yold[0]], [self._yold[1]], [self._yold[2]]] times = [self._time] for itr in range(self._n_steps): self.step() self._time += self._dt times.append(self._time) solutions[0].append(self._yold[0]) solutions[1].append(self._yold[1]) solutions[2].append(self._yold[2]) return times, solutions def _k1(self, t: float) -&gt; np.array: return self._dt * self._rhs(t, self._yold) def _k2(self, t: float, k1: np.array) -&gt; np.array: return self._dt * self._rhs(t + 0.5 * self._dt, self._yold + 0.5 * k1) def _k3(self, t: float, k2: np.array) -&gt; np.array: return self._dt * self._rhs(t + 0.5 * self._dt, self._yold + 0.5 * k2) def _k4(self, t: float, k3: np.array) -&gt; np.array: return self._dt * self._rhs(t + self._dt, self._yold + k3) . class LorenzRhs(object): def __init__(self, beta: np.array): self._beta = beta def __call__(self, t: float, x:np.array) -&gt; np.array: result = np.array([beta[0]*(x[1] - x[0]), x[0]*(beta[1] - x[2]) - x[1], x[0]*x[1] - beta[2]*x[2]]) return result . beta = np.array([10., 28., 8./3.]) x0 = np.array([0., 1.0, 20.]) dt = 0.001 n_steps = 50000 . rhs = LorenzRhs(beta=beta) rk45 = ODE45(dt=dt, n_steps=n_steps, rhs=rhs, y0=x0) . times, solutions = rk45.integrate() . fig = plt.figure(figsize=(15,15)) ax = plt.axes(projection=&#39;3d&#39;) ax.plot3D(solutions[0], solutions[1], solutions[2], &#39;gray&#39;) . [&lt;mpl_toolkits.mplot3d.art3d.Line3D at 0x7fd911347a90&gt;] . The following video discusses how to simulate the Lorenz system with Matlab. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EnsB1wP3LFM&#39;, width=800, height=300) .",
            "url": "https://pockerman.github.io/qubit_opus/lorenz-system/python/simulation/numerics/2021/04/09/simulate-lorenz-system.html",
            "relUrl": "/lorenz-system/python/simulation/numerics/2021/04/09/simulate-lorenz-system.html",
            "date": " • Apr 9, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Scala Functions",
            "content": "The Scala programming language is a JVM based language worthwhile exploring. In this post I give a brief review of functions in Scala as I continue my exploration of the language. . Functions, in general, are at the core of every programming language when it comes to code organization and implementin the DRY principle. Scala supports two types of functions namely functions and methods. The difference is that a method operates on an object a function does not. . We define a function as follows . def myFunc(x: Integer) = if(x &gt;=0) x else -x . When defining a function we must . specify the types of all parameters | if the function is not recursive i.e. does not call itself, we do not have to specify the return type | if the body of the function requires more than one expression,then we should use a block i.e. {}. The last expression of the block becomes the value that the function returns. | . As an aside, it is possible to omit the return type of the function. Indeed, the Scala compiler can determine the return type from the type of the expression to the right of the = symbol. However, this may not be always the case. The following example demonstrates that . def myAbs(x: Double) = if(x &gt;=0) x else -x . val x = -10 myAbs(x) . x: Int = -10 res1_1: Double = 10.0 . However, with a recursive function, we must specify the return type . def fuc(x: Int): Int = if(x &lt;= 0) 1 else x*fuc(x-1) . Default &amp; Named Arguments . Just like C++, Scala also supports default arguments i.e. the default arguments for functions that are used when we do not specify explicit values . def showMe(x: Int=5) = println(&quot;You want to show &quot; + x) . showMe() &gt; You want to show 5 showMe(6) &gt; You want to show 6 . One of the features that I really like in Python is the named argument(s). This feature is very handy for understanding arguments at call sites i.e. I don&#39;t need to do the trip to the (unavailable) documentation but more important is really helpful in mitigating errors. Scala also supports the idea of named arguments . def speak(arg1: String, arg2: String, arg3: String=&quot; the end&quot;) = println(arg1 + arg2 + arg3) . speak(arg2=&quot; is &quot;, arg1=&quot;This &quot;) &gt; This is the end . As you can see, the named arguments need not be in the same order as the parameters. Furthermore, we can mix unnamed and named arguments, provided the unnamed ones come first. This is similar to Python. . Variable Arguments . Often it is useful to have a function that can take a variable number of arguments think of printf. Usually we call these as varargs functions Scala supports this idea too . def sum(args: Int*): Int = { var result = 0 for(arg &lt;- args) result += arg result } . val s = sum(1, 4, 9, 16, 25) &gt; s:Int = 55 . The actual type received by the function is of type Seq. However, we can not do the following . val s = sum(1 to 5) &gt; cmd10.sc:1: type mismatch; found : scala.collection.immutable.Range.Inclusive required: Int val s = sum(1 to 5) ^Compilation Failed Compilation Failed . That&#39;s because if the sum function is called with one argument, that must be a single integer. Here is how we can fix this . val s = sum(1 to 5:_*) &gt; s:Int = 15 . References . Cay Horstmann Scala for the impatient, Addison-Wesley. |",
            "url": "https://pockerman.github.io/qubit_opus/scala/functions/programming/2021/04/08/scala-functions.html",
            "relUrl": "/scala/functions/programming/2021/04/08/scala-functions.html",
            "date": " • Apr 8, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Logistic Map Simulation",
            "content": "The logistic map is a discrete time system of the form . $$x_{k+1} = beta x_k (1-x_k)$$ . The logistic map is a polynomial mapping (equivalently, recurrence relation) of degree 2, often cited as an archetypal example of how complex, chaotic behaviour can arise from very simple non-linear dynamical equations. The map was popularized in a 1976 paper by the biologist Robert May,[1] in part as a discrete-time demographic model analogous to the logistic equation written down by Pierre François Verhulst . import numpy as np import matplotlib.pyplot as plt . betas = np.linspace(0.0, 4.0, 400) . def get_steady_state(xinit, nitrs, beta): xold = xinit for itr in range(nitrs): xnew = (xold - xold**2)*beta xold = xnew return xold . def iterate(xinit, betas, use_steady_state, steady_state_itrs, itrs): xvals = [] beta_vals = [] xinit = xinit for beta in betas: #print(&quot;Working with beta={0}&quot;.format(beta)) if use_steady_state: xold = get_steady_state(xinit=xinit, nitrs=steady_state_itrs, beta=beta) else: xold = xinit xss = xold for i in range(itrs): xnew = (xold - xold**2)*beta xold = xnew beta_vals.append(beta) xvals.append(xnew) # if this is the case # the solution is boring :) if np.abs(xnew - xss) &lt; 0.001: break return beta_vals, xvals . beta_vals, xvals = iterate(xinit=0.5, betas=betas, use_steady_state=True, steady_state_itrs=2000, itrs=1000) . plt.plot(beta_vals, xvals) plt.xlabel(&quot;beta&quot;) plt.ylabel(&quot;x&quot;) plt.show() . beta_vals, xvals = iterate(xinit=0.5, betas=betas, use_steady_state=False, steady_state_itrs=2000, itrs=1000) . plt.plot(beta_vals, xvals) plt.xlabel(&quot;beta&quot;) plt.ylabel(&quot;x&quot;) plt.show() . from IPython.display import YouTubeVideo YouTubeVideo(&#39;_BvAkyuWhOI&#39;, width=800, height=300) .",
            "url": "https://pockerman.github.io/qubit_opus/logistic-map/python/simulation/numerics/2021/04/08/logistic-map-simulation.html",
            "relUrl": "/logistic-map/python/simulation/numerics/2021/04/08/logistic-map-simulation.html",
            "date": " • Apr 8, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Deutsch's Algorithm",
            "content": "Quantum computers pose as the future of computing. Although, at the time of writing, the computing harwdare based on quantum mechanics principles can accommodate only a small number of qubits, algorithms have been developed that demonstrate the superiority of quantum computers for certain class problems. . One such algorithm, and perhaps the simplest one is Deutsch&#39;s algorithm. The algorithm solves the following problem [1] . Given a boolean function $f: {0,1 } rightarrow {0,1 }$ determine if $f$ is constant. . The algorithm, can solve the problem with fewer calls to the function $f$ than is possible on a classical machine [1]. A function is called constant if $f(0) = f(1)$. On the other hand, if $f$ is one-to-one, is called balanced [1]. . Using a classical computer we need to do two evaluations of the function; one for each of the two inputs [1, 2]. On the other hand, Deutsch&#39;s algorithm requires only a single call to a black box to solve the problem. The key to the algorithm is the ability to place the second qubit of the input to the black box in a superposition [2]. Let&#39;s see how to do this. . Deutsch&#39;s algorithm works by putting both qubits representing the two inputs into a superposition [1]. The way to do this is using the Hadamard gate. The following image shows this schematically. . Figure 1. Deutsch&#39;s algorithm circuit. Image from [1]. . Let&#39;s study how the state system $| psi rangle$ evolves. Initially the system is at . $$| psi rangle = |01 rangle$$ . Appication of the Hadamard gate moves the two qubits respectively to . $$|0 rangle = frac{|0 rangle + |1 rangle}{ sqrt{2}}$$ . $$ |1 rangle = frac{|0 rangle - |1 rangle}{ sqrt{2}}$$ . Thus, $| psi rangle$ will be at . $$| psi rangle = left[ frac{|0 rangle + |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . Let&#39;s rename the top qubit as $|x rangle$. We want to evaluate $f(x)$. Note that when the bottom qubit is put into a superposition and then multiply by $U_f$, the system will be at state [1] . $$| psi rangle = (-1)^{f(x)}|x rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . Given however that $|x rangle$ is also in superposition, we will have that the system will be at state [1] . $$| psi rangle = left[ frac{(-1)^{f(0)}|0 rangle + (-1)^{f(1)}|1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right]$$ . The actual state, as shown in the equation above, depends on the values of $f$. We can summarize this as follows [1]. . $$| psi rangle = begin{cases} ( pm1) left[ frac{|0 rangle + |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] ( pm1) left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right] end{cases}$$ The final step is to apply the Hadamard gate on the top qubit. Recall that the Hadamard matrix is its own inverse. Thus applying it to the top qubit we get [1] . $$| psi rangle = begin{cases} ( pm1) |0 rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right], ~~ text{if} ~~ f ~~ text{is constant} ( pm1) |1 rangle left[ frac{|0 rangle - |1 rangle}{ sqrt{2}} right], ~~ text{if} ~~ f ~~ text{is balanced} end{cases}$$ Now, we simply measure the top qubit. If it is in state $|0 rangle$, then we know that f is a constant function [1]. This was all accomplished with only one function evaluation. . One of the nice points demonstared by the algorithm is that a change of basis can allow solving a problem that otherwise requires more questions to the oracle. In Deutsch algorithm, we start in the canonical basis $|01 rangle$. The first application of the Hadamard matrices is used to change the basis to go into a balanced superposition of basic states. While in this noncanonical basis, we evaluate $f$ with the bottom qubit. The last Hadamard matrix is used as a change of basis matrix to revert back to the canonical basis [1]. . import numpy as np import random . H = np.array([[1.0/np.sqrt(2.0), 1.0/np.sqrt(2.0)], [1.0/np.sqrt(2.0), - 1.0/np.sqrt(2.0)]]) . def oracle(x, y, constant): if constant: f0 = 0 #random.choice([0,1]) f1 = 0 #random.choice([0,1]) else: f0 = 0 #random.choice([0,1]) f1 = 1 #random.choice([0,1]) return np.array([(-1)**f0*x[0], (-1)**f1*x[1]]) . zero = np.array([1., 0.]) one = np.array([0.0, 1.0]) . zero_H = np.dot(H, zero) one_H = np.dot(H, one) . print(zero_H) print(one_H) . [0.70710678 0.70710678] [ 0.70710678 -0.70710678] . out_oracle = oracle(x=zero_H, y=one_H, constant=True) . x = np.dot(H, out_oracle) . print(x) . [1. 0.] . out_oracle = oracle(x=zero_H, y=one_H, constant=False) . x = np.dot(H, out_oracle) . print(x) . [0. 1.] . References . Noson S. Yanofsky and Mirco A. Mannucci, Quantum Computing for Computer Scientists, Cambridge University Press | Eleanor Rieffel, Wolfgang Polak, Quantum Computing: A Gentle Introduction, The MIT Press. | Deutsch&#39;s algorithm |",
            "url": "https://pockerman.github.io/qubit_opus/deutsch/quantum-computing/algorithms/numerics/2021/03/20/deutsch-algo.html",
            "relUrl": "/deutsch/quantum-computing/algorithms/numerics/2021/03/20/deutsch-algo.html",
            "date": " • Mar 20, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Singular Value Decomposition",
            "content": "One of the most important matrix factorization techniques is the singular value decomposition most often abbreviated as SVD. The reason why is so popular lies on the fact that it is the foundation for many other computational techniques. For example, just to name a few: . Computing pseudo-inverses | Obtaining low-rank matrix approximations | Dynamic mode decomposition | Proper orthogonal ecomposition | Principal components analysis | . For a complex matrix $A in mathbb{C}^{n times m}$, its SVD is . $$A = U Sigma V^{*}$$ . where $V^{*}$ is the complex conjugate transpose. Both $U$ and $V$ are unitary matrices that is the following holds . $$UU^{*} = U^{*}U = I$$ . In general, if a matrix $W$ is a real matrix i.e. its entries are real numbers, then $W^{*} = W^T$. Thus, if $A in mathbb{R}^{n times m}$ the matrices $U$ and $V$ are real orthogonal matrices i.e. . $$UU^{T} = U^{T}U = I$$ . The matrix $ Sigma$ is a diagonal matrix with real and nonnegative entries on the diagonal. The entries $ Sigma_{ii}$ are called the singular values of $A$. The number of the non-zero singular values corresponds to the rank of the matrix $A$. . Given the popularity of the SVD method, it is not surpsising that most linear algebra libraries provide a way to perform it. The following script shows how to compute the SVD in Python using numpy . import numpy as np X = np.random.rand(10 , 10) U, S, V = np.linalg.svd(X, full_matrices=True) # or doing economy SVD U, S, V = np.linalg.svd(X, full_matrices=False) . You can find the documentation at numpy.linalg.svd. Similarly, using the Blaze C++ library . template&lt; typename MT1, bool SO, typename VT, bool TF, typename MT2, typename MT3 &gt; void svd( const DenseMatrix&lt;MT1,SO&gt;&amp; A, DenseMatrix&lt;MT2,SO&gt;&amp; U, DenseVector&lt;VT,TF&gt;&amp; s, DenseMatrix&lt;MT3,SO&gt;&amp; V ); . Overall, the SVD algorithm is a very important matrix decomposition technique used throughout numerical modeling control theory and system identification. We will see applications of the method in future posts. .",
            "url": "https://pockerman.github.io/qubit_opus/linear-algebra/singular-value-decomposition/algorithms/numerics/2021/03/13/singular-value-decomposition.html",
            "relUrl": "/linear-algebra/singular-value-decomposition/algorithms/numerics/2021/03/13/singular-value-decomposition.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Gradient Descent",
            "content": "Perhaps the simplest algorithm for uncostrained optimization is gradient descent also known as steepest descent. Consider the following function [1] . $$f( theta_1, theta_2) = frac{1}{2}( theta_{1}^2 - theta_2)^2 + frac{1}{2}( theta_1 -1)^2$$ . We are interested in finding $ theta_1, theta_2$ that minimize $f$. Gradient descent is an iterative algorithm that uses the gradient of the function in order to update the parameters. The update rule is . $$ boldsymbol{ theta}_k = boldsymbol{ theta}_{k-1} - eta nabla f|_{ boldsymbol{ theta}_{k-1}} $$ . $ eta$ is the so called learning rate and tunes how fast we move to the direction of the gradient. A small $ eta$ slows down convergence whilst a large value may not allow convergence of the algorithm. This is shown in the two figures below: . Figure 1. Gradient descent with eta 0.1. . Figure 2. Gradient descent with eta 0.6. . The code below is a simple implementation of the gradient descent algorithm. . import numpy as np import matplotlib.pyplot as plt . def f(theta1, theta2): return 0.5*(theta1**2 - theta2)**2 + 0.5*(theta1 -1.0)**2 . def f_grad(theta1, theta2): return (2.0*theta1*(theta1**2 - theta2) + (theta1 - 1.0), -(theta1**2 - theta2)) . def gd(eta, itrs, tol): coeffs_series = [] coeffs = [0.0, 0.0] coeffs_series.append([coeffs[0], coeffs[1]]) val_old = f(theta1=coeffs[0], theta2=coeffs[1]) for itr in range(itrs): grad = f_grad(theta1=coeffs[0], theta2=coeffs[1]) coeffs[0] -= eta*grad[0] coeffs[1] -= eta*grad[1] coeffs_series.append([coeffs[0], coeffs[1]]) val = f(theta1=coeffs[0], theta2=coeffs[1]) abs_error = np.abs(val - val_old) print(&quot;&gt;Iteration {0} absolute error {1} exit tolerance {2}&quot;.format(itr, abs_error, tol)) if abs_error &lt; tol: print(&quot;&gt;GD converged with residual {0}&quot;.format(np.abs(val - val_old))) return coeffs_series val_old = val return coeffs_series . coeffs_series = gd(eta=0.1, itrs=100, tol=1.0e-4) . &gt;Iteration 0 absolute error 0.09494999999999998 exit tolerance 0.0001 &gt;Iteration 1 absolute error 0.07622463831103915 exit tolerance 0.0001 &gt;Iteration 2 absolute error 0.05968293530998797 exit tolerance 0.0001 &gt;Iteration 3 absolute error 0.04523783343545831 exit tolerance 0.0001 &gt;Iteration 4 absolute error 0.03333771938318547 exit tolerance 0.0001 &gt;Iteration 5 absolute error 0.024254166137182898 exit tolerance 0.0001 &gt;Iteration 6 absolute error 0.01782239274907732 exit tolerance 0.0001 &gt;Iteration 7 absolute error 0.013533685475623586 exit tolerance 0.0001 &gt;Iteration 8 absolute error 0.010768584908705553 exit tolerance 0.0001 &gt;Iteration 9 absolute error 0.008983651981479004 exit tolerance 0.0001 &gt;Iteration 10 absolute error 0.007786932467621618 exit tolerance 0.0001 &gt;Iteration 11 absolute error 0.006930789314960939 exit tolerance 0.0001 &gt;Iteration 12 absolute error 0.0062723026732663945 exit tolerance 0.0001 &gt;Iteration 13 absolute error 0.005733543774474825 exit tolerance 0.0001 &gt;Iteration 14 absolute error 0.0052730560188138376 exit tolerance 0.0001 &gt;Iteration 15 absolute error 0.004868553639578624 exit tolerance 0.0001 &gt;Iteration 16 absolute error 0.00450745300901266 exit tolerance 0.0001 &gt;Iteration 17 absolute error 0.004182026970546315 exit tolerance 0.0001 &gt;Iteration 18 absolute error 0.003887028764196082 exit tolerance 0.0001 &gt;Iteration 19 absolute error 0.0036185521793817496 exit tolerance 0.0001 &gt;Iteration 20 absolute error 0.0033734842877468432 exit tolerance 0.0001 &gt;Iteration 21 absolute error 0.0031492347340679877 exit tolerance 0.0001 &gt;Iteration 22 absolute error 0.0029435928019137803 exit tolerance 0.0001 &gt;Iteration 23 absolute error 0.0027546441698343416 exit tolerance 0.0001 &gt;Iteration 24 absolute error 0.0025807166947806187 exit tolerance 0.0001 &gt;Iteration 25 absolute error 0.0024203414221908165 exit tolerance 0.0001 &gt;Iteration 26 absolute error 0.0022722224779619174 exit tolerance 0.0001 &gt;Iteration 27 absolute error 0.0021352127684441946 exit tolerance 0.0001 &gt;Iteration 28 absolute error 0.002008293861733755 exit tolerance 0.0001 &gt;Iteration 29 absolute error 0.0018905590854755572 exit tolerance 0.0001 &gt;Iteration 30 absolute error 0.0017811992002927796 exit tolerance 0.0001 &gt;Iteration 31 absolute error 0.0016794901835057927 exit tolerance 0.0001 &gt;Iteration 32 absolute error 0.0015847827649886279 exit tolerance 0.0001 &gt;Iteration 33 absolute error 0.0014964934298845774 exit tolerance 0.0001 &gt;Iteration 34 absolute error 0.0014140966564608337 exit tolerance 0.0001 &gt;Iteration 35 absolute error 0.0013371181986932233 exit tolerance 0.0001 &gt;Iteration 36 absolute error 0.0012651292559366714 exit tolerance 0.0001 &gt;Iteration 37 absolute error 0.0011977413984442034 exit tolerance 0.0001 &gt;Iteration 38 absolute error 0.001134602138991761 exit tolerance 0.0001 &gt;Iteration 39 absolute error 0.0010753910584805904 exit tolerance 0.0001 &gt;Iteration 40 absolute error 0.001019816407902937 exit tolerance 0.0001 &gt;Iteration 41 absolute error 0.0009676121210644636 exit tolerance 0.0001 &gt;Iteration 42 absolute error 0.0009185351824353497 exit tolerance 0.0001 &gt;Iteration 43 absolute error 0.0008723633028205682 exit tolerance 0.0001 &gt;Iteration 44 absolute error 0.0008288928625002738 exit tolerance 0.0001 &gt;Iteration 45 absolute error 0.0007879370873338197 exit tolerance 0.0001 &gt;Iteration 46 absolute error 0.0007493244282380136 exit tolerance 0.0001 &gt;Iteration 47 absolute error 0.0007128971186049562 exit tolerance 0.0001 &gt;Iteration 48 absolute error 0.000678509887739322 exit tolerance 0.0001 &gt;Iteration 49 absolute error 0.0006460288113815469 exit tolerance 0.0001 &gt;Iteration 50 absolute error 0.0006153302829237615 exit tolerance 0.0001 &gt;Iteration 51 absolute error 0.000586300091094321 exit tolerance 0.0001 &gt;Iteration 52 absolute error 0.0005588325917408928 exit tolerance 0.0001 &gt;Iteration 53 absolute error 0.000532829962933393 exit tolerance 0.0001 &gt;Iteration 54 absolute error 0.0005082015339742743 exit tolerance 0.0001 &gt;Iteration 55 absolute error 0.00048486318008077005 exit tolerance 0.0001 &gt;Iteration 56 absolute error 0.00046273677552067724 exit tolerance 0.0001 &gt;Iteration 57 absolute error 0.0004417496988608476 exit tolerance 0.0001 &gt;Iteration 58 absolute error 0.00042183438475075323 exit tolerance 0.0001 &gt;Iteration 59 absolute error 0.00040292791732379415 exit tolerance 0.0001 &gt;Iteration 60 absolute error 0.00038497166087553616 exit tolerance 0.0001 &gt;Iteration 61 absolute error 0.00036791092397959677 exit tolerance 0.0001 &gt;Iteration 62 absolute error 0.00035169465363989703 exit tolerance 0.0001 &gt;Iteration 63 absolute error 0.00033627515646207293 exit tolerance 0.0001 &gt;Iteration 64 absolute error 0.00032160784416228154 exit tolerance 0.0001 &gt;Iteration 65 absolute error 0.0003076510010270577 exit tolerance 0.0001 &gt;Iteration 66 absolute error 0.00029436557119745174 exit tolerance 0.0001 &gt;Iteration 67 absolute error 0.000281714963878678 exit tolerance 0.0001 &gt;Iteration 68 absolute error 0.00026966487477881555 exit tolerance 0.0001 &gt;Iteration 69 absolute error 0.000258183122257602 exit tolerance 0.0001 &gt;Iteration 70 absolute error 0.0002472394968245067 exit tolerance 0.0001 &gt;Iteration 71 absolute error 0.0002368056227645514 exit tolerance 0.0001 &gt;Iteration 72 absolute error 0.0002268548307944717 exit tolerance 0.0001 &gt;Iteration 73 absolute error 0.00021736204076207993 exit tolerance 0.0001 &gt;Iteration 74 absolute error 0.00020830365349946717 exit tolerance 0.0001 &gt;Iteration 75 absolute error 0.00019965745102808012 exit tolerance 0.0001 &gt;Iteration 76 absolute error 0.0001914025043919798 exit tolerance 0.0001 &gt;Iteration 77 absolute error 0.00018351908846457078 exit tolerance 0.0001 &gt;Iteration 78 absolute error 0.0001759886031370006 exit tolerance 0.0001 &gt;Iteration 79 absolute error 0.00016879350035171707 exit tolerance 0.0001 &gt;Iteration 80 absolute error 0.0001619172164950512 exit tolerance 0.0001 &gt;Iteration 81 absolute error 0.00015534410970717907 exit tolerance 0.0001 &gt;Iteration 82 absolute error 0.000149059401708577 exit tolerance 0.0001 &gt;Iteration 83 absolute error 0.0001430491237779689 exit tolerance 0.0001 &gt;Iteration 84 absolute error 0.00013730006654984238 exit tolerance 0.0001 &gt;Iteration 85 absolute error 0.000131799733328664 exit tolerance 0.0001 &gt;Iteration 86 absolute error 0.00012653629664396288 exit tolerance 0.0001 &gt;Iteration 87 absolute error 0.00012149855779407855 exit tolerance 0.0001 &gt;Iteration 88 absolute error 0.00011667590914838256 exit tolerance 0.0001 &gt;Iteration 89 absolute error 0.00011205829899737585 exit tolerance 0.0001 &gt;Iteration 90 absolute error 0.00010763619875779401 exit tolerance 0.0001 &gt;Iteration 91 absolute error 0.00010340057235624662 exit tolerance 0.0001 &gt;Iteration 92 absolute error 9.934284762933097e-05 exit tolerance 0.0001 &gt;GD converged with residual 9.934284762933097e-05 . coeffs_x = [] coeffs_y = [] for item in coeffs_series: coeffs_x.append(item[0]) coeffs_y.append(item[1]) . theta1 = np.linspace(0.0, 2.0, 100) theta2 = np.linspace(-0.5, 3.0, 100) . X, Y = np.meshgrid(theta1, theta2) . Z = f(X, Y) . plt.contour(X, Y, Z, 60, colors=&#39;black&#39;); plt.plot(coeffs_x, coeffs_y, &#39;r-o&#39;) plt.show() . References . Kevin P. Murphy, Machine Learning A Probabilistic Perspective, The MIT Press |",
            "url": "https://pockerman.github.io/qubit_opus/gradient-descent/unconstrained-optimization/machine-learning/algorithms/numerics/2020/06/22/gradient-descent.html",
            "relUrl": "/gradient-descent/unconstrained-optimization/machine-learning/algorithms/numerics/2020/06/22/gradient-descent.html",
            "date": " • Jun 22, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Viterbi Algorithm",
            "content": "The backward and forward algorithms can be used to compute $P(O| lambda)$. In this notebook we are interested in computing the most likely path given a sequence $O$ and a hidden Markov model $ lambda$. The Viterbi algorithm gives us a way to do so. The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states, also called the Viterbi path, that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM) [2]. The algorithm uses a maximum operation instead of the sum. The operation of Viterbi&#39;s algorithm can be visualized by means of a trellis diagram [2]. It is essentially the shortest path through this trellis. . Given a state sequence $Q=q_1q_2, cdots,q_T$ and an observation sequence $O=O_1O_2, cdots,O_T$ we dfine the variable $ delta_t(i)$ as the probability of the highest probability path at time $t$ that accounts for the first $t$ observations and ends in $S_i$ [1]: . $$ delta_t(i) = max_{Q}p(q_1q_2, cdots,q_t=S_i,O_1O_2, cdots,O_t| lambda)$$ . Then we can recursively calculate $ delta_{t+1}(i)$ and the optimal path can be read by backtracking from $T$ , choosing the most probable at each instant. The algorithm is as follows [1]: . Initialize | . $$ delta_1(i) = pi_i b_i(O_1)$$ . This is initialization is the same as in the forward algorithm. To retrieve the state sequence we also need to keep track of the argument which maximized for each $t$ and $j$. We therefore use the array $ psi$, and in the initialization step the first $ psi$ variable of every state will be equal to 0 because no specific argument coming from the initial probability maximized the value of the first state. . $$ psi_1(i) = 0$$ . Recurse | . $$ delta_t(j) = max_{i=1}^{N} delta_{t-1}(i)a_{ij}b_j(O_t)$$ . $$ psi_t(j) = argmax_{i=1}^{N} delta_{t-1}(i)a_{ij}$$ . Termination | . $$p^{*} = max_i delta_T(i)$$ . $$q^{*}_T = arg max_i delta_T(i)$$ . Path backtracking | . $$q_t{*} = psi_{t+1}(q^{*}_{t+1}), t= T-1, T-2, cdots, 1$$ . $ psi_t (j)$ keeps track of the state that maximizes $ delta_t(j)$ at time $t-1$, that is, the best previous state. The Viterbi algorithm has the same complexity with the forward phase, where instead of the sum, we take the maximum at each step [1]. . Let&#39;s see an example applying the Viterbi algorithm. The example is taken from [2]. Some coding hints from Implement Viterbi Algorithm in Hidden Markov Model using Python and R have also been used. . import numpy as np . obs_to_idx = {&#39;normal&#39;:0, &#39;cold&#39;: 1, &#39;dizzy&#39;:2} # state to index map state_to_idx = {&#39;Healthy&#39;:0, &#39;Fever&#39;:1} . pi = np.array([0.6, 0.4]) # transition probabilities A = np.array([[0.7, 0.3], [0.4, 0.6]]) # emission probabilties B = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]) . o = [&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;] . delta = np.zeros(shape=(len(o), A.shape[0])) previous = np.zeros((len(o)-1, A.shape[0])) for st in state_to_idx: state_idx = state_to_idx[st] delta[0][state_idx] = pi[state_idx] * B[state_idx][obs_to_idx[o[0]]] print(delta) . [[0.3 0.04] [0. 0. ] [0. 0. ]] . for t in range(1, len(o)): obs_idx = obs_to_idx[o[t]] for i in state_to_idx: i_st_idx = state_to_idx[i] probs=[] for j in state_to_idx: j_st_idx = state_to_idx[j] probs.append(delta[t - 1][j_st_idx]*A[j_st_idx][i_st_idx]*B[i_st_idx][obs_idx]) # This is our most probable state given previous state at time t (1) previous[t-1, i_st_idx] = np.argmax(probs) delta[t, i_st_idx] = np.max(probs) # Path Array S = np.zeros(len(o)) # Find the most probable last hidden state last_state = np.argmax(delta[len(o) - 1, :]) S[0] = last_state backtrack_index = 1 for i in range(len(o) - 2, -1, -1): S[backtrack_index] = previous[i, int(last_state)] last_state = previous[i, int(last_state)] backtrack_index += 1 # Flip the path array since we were backtracking S = np.flip(S, axis=0) # Convert numeric values to actual hidden states path = [] for s in S: if s == 0: path.append(&quot;Healthy&quot;) else: path.append(&quot;Fever&quot;) . print(&quot;Path is &quot;, path) . Path is [&#39;Healthy&#39;, &#39;Healthy&#39;, &#39;Fever&#39;] . Another way to compute the $ delta$ matrix is the following more Pythonic way, taken from Implement Viterbi Algorithm in Hidden Markov Model using Python and R. Note the use of the log function. . delta = np.zeros(shape=(len(o), A.shape[0])) previous = np.zeros((len(o)-1, A.shape[0])) for st in state_to_idx: state_idx = state_to_idx[st] delta[0, :] = np.log(pi * B[:, obs_to_idx[o[0]]]) print(delta) . [[-1.2039728 -3.21887582] [ 0. 0. ] [ 0. 0. ]] . for t in range(1, len(o)): obs_idx = obs_to_idx[o[t]] for i in state_to_idx: i_st_idx = state_to_idx[i] # Same as Forward Probability probability = delta[t - 1] + np.log(A[:, i_st_idx]) + np.log(B[i_st_idx, obs_idx]) # This is our most probable state given previous state at time t (1) previous[t - 1, i_st_idx] = np.argmax(probability) # This is the probability of the most probable state (2) delta[t, i_st_idx] = np.max(probability) # Path Array S = np.zeros(len(o)) # Find the most probable last hidden state last_state = np.argmax(delta[len(o) - 1, :]) S[0] = last_state backtrack_index = 1 for i in range(len(o) - 2, -1, -1): S[backtrack_index] = previous[i, int(last_state)] last_state = previous[i, int(last_state)] backtrack_index += 1 # Flip the path array since we were backtracking S = np.flip(S, axis=0) # Convert numeric values to actual hidden states path = [] for s in S: if s == 0: path.append(&quot;Healthy&quot;) else: path.append(&quot;Fever&quot;) . print(&quot;Path is &quot;, path) . Path is [&#39;Healthy&#39;, &#39;Healthy&#39;, &#39;Fever&#39;] . The following video provides a motivation behind the use of the Viterbi algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;MPeedE6Odj0&#39;, width=800, height=300) . The following video provides nice description of the Viterbi algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;s9dU3sFeE40&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Viterbi algorithm, Wikipedia. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20viterbi-algorithm/dynamic-programming/algorithms/numerics/2020/05/24/viterbi-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20viterbi-algorithm/dynamic-programming/algorithms/numerics/2020/05/24/viterbi-algorithm.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Backward Algorithm",
            "content": "The backward algorithm is the complement of the forward algorithm. Let&#39;s introduce the backward variable $ beta_t(i)$. This is the probability of being in $S_i$ at time $t$ abd observing the partial sequence $O_{t+1}, cdots,O_T$ [1]. This can be written as . $$ beta_t(i) = P(O_{t+1}, cdots,O_T | q_t=S_i, lambda)$$ . The backward algorithm computes this recursively . Initialize | . $$ beta_T(i) = 1$$ . Recurse | . $$ beta_t(i) = P(O_{t+1}, cdots,O_T | q_t=S_i, lambda)$$ . which can be written as, see [1], . $$ beta_{t}(i) = sum_{j}^{N}b_j(O_{t+1})a_{i,j} beta_{t+1}(j)$$ . Let&#39;s implement this as we did for the forward algorithm. . Assume a system with two states $S= {S_0, S_1 }$. Futher, assume that the observation sequence consists of elements from the following set $V= {a, b,c }$. Also let&#39;s assume the following HMM: . $$ boldsymbol{ pi}= begin{bmatrix}0.6 &amp; 0.4 end{bmatrix}$$ . $$ mathbf{A}= begin{bmatrix}0.7 &amp; 0.3 0.4 &amp; 0.6 end{bmatrix}$$ . $$ mathbf{B}= begin{bmatrix} 0.5 &amp; 0.4 &amp; 0.1 0.1 &amp; 0.3 &amp; 0.6 end{bmatrix}$$ . Assume the following sequence $V= {a, b, c }$. We introduce the $ beta$ matrix: . $$ beta = begin{bmatrix}0 &amp; 0 0 &amp; 0 0 &amp; 0 end{bmatrix}$$ . First we initialize . $$ beta = begin{bmatrix}0 &amp; 0 0 &amp; 0 1 &amp; 1 end{bmatrix}$$ . Then use the recursion formula . $$ beta_{t}(i) = sum_{j}^{N}b_j(O_{t+1})a_{i,j} beta_{t+1}(j)$$ . import numpy as np . obs_to_idx = {&#39;a&#39;:0, &#39;b&#39;: 1, &#39;c&#39;:2} # state to index map state_to_idx = {&#39;S0&#39;:0, &#39;S1&#39;:1} . pi = np.array([0.6, 0.4]) # transition probabilities A = np.array([[0.7, 0.3], [0.4, 0.6]]) # emission probabilties B = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]) . o = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] . beta = np.zeros(shape=(len(o),A.shape[0])) . beta[len(o) - 1] = np.ones(A.shape[0]) . # start at the position one before the end # proceed until t is -1. Move back one step at the time for t in range(len(o)-2, -1, -1): for j in range(A.shape[0]): # for x = np.array([1.,2.]) and # y = np.array([1.,2.])then # x*y = np.array([1., 4.]) # that is element-wise product is performed beta[t, j] = (beta[t + 1] * B[:, obs_to_idx[o[t + 1]]]).dot(A[j, :]) . print(beta) . [[0.106 0.112] [0.25 0.4 ] [1. 1. ]] . Overall the forward and backward algorithms can be used to compute $P(O| lambda)$. Indeed using the forward algorithm we have: . $$P(O| lambda) = sum_{i}^{N} alpha_{T}(i)$$ . whilst using the backward algorithm . $$P(O| lambda) = sum_{i}^{N} pi_i b_i(O_1) beta_{1}(i)$$ . The following video nicely explains the motivation behind the backward algorithm . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EbxLWGw2zJ4&#39;, width=800, height=300) . The following video nicely explains both the forward and backward algorithms. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;gYma8Gw38Os&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Forward–backward algorithm. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20backward-algorithm/dynamic-programming/algorithms/numerics/2020/05/23/backward-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20backward-algorithm/dynamic-programming/algorithms/numerics/2020/05/23/backward-algorithm.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Forward Algorithm",
            "content": "Given a hidden Markov model $ lambda$, meaning . A set of $N$ states $ mathbb{S}= {S_1, dots S_N }$ | A set of $M$ distinct observation symbols $ mathbf{V} = {v_1 dots v_M }$ | A transition probability matrix $ mathbf{A}$ | An observations probabilities matrix $ mathbf{B}$ | An initial state of probabilities $ boldsymbol{ pi}$ | . we want to be able to compute the marginal probability $P(O| lambda)$. This is the so called evaluation problem [1]; given the observation sequence $O$ calculate the probability that the sequence can occur under $ lambda$ . In theory, this can be calculated by using . $$P(O| lambda) = sum_{Q}P(O, Q| lambda)$$ . where . $$P(O, Q| lambda) = P(q_1) Pi_{t=2}^{T}P(q_t|q_{t-1}) Pi_{t=1}^{T}P(O_t|q_{t})$$ . Despite this, you should note that there are $N^T$ possible $Q$s assuming that all probabilities are nonzero [1]. Thus, the marginalization step above is rather, or can be, computationally expensive. We need another way to calculate $P(O| lambda)$. . In order to compute the probability $P(O| lambda)$ we need to know the joint probability $P(O, Q| lambda)$. The forward algorithm allows us to compute the latter without using marginalization. It does so by using recursion. We will divide the observation sequence into two parts; the first part will be $[1,t]$, the second is $[t+1, T]$ [1]. We further define the forward variable $ alpha_t(i)$. This will denote the probability of observing the partial sequence $ {O_1, cdots,O_t }$ unitl time $t$ and being in $S_i$ at time $t$ given $ lambda$: . $$ alpha_t(i) = P(O_1, cdots,O_t, q_t = S_i | lambda)$$ . This can be calculated recursively: . Initialize | . $$ alpha_t(i) = pi_ib_i(O_1)$$ . Recurse | . $$ alpha_{t+1}(j) = P(O_1, cdots,O_{t+1}, q_{t+1} = S_j | lambda)$$ . which can be written as . $$ alpha_{t+1}(j) = [ sum_{i}^{N} alpha_t(i)a_{i,j}]b_j(O_{t+1})$$ . Now $ alpha_t(i)$ explains the first $t$ observations and ends in state $S_i$. We multiply with the transition probability $a_{ij}$ in order to move to state $S_j$. since the are $N$ possible previous states we have to sum over all of them. Finally, we weight the result with $b_j(O_{t+1})$ which is the probability of observing $O_{t+1}$ at state $S_j$ at time $t+1$. . Once we know the forward variables, it is easy to calculate $P(O| lambda)$: . $$P(O| lambda) = sum_{i}^{N} alpha_{T}(i)$$ . $ alpha_{T}(i)$ is the probability of generating the full observation sequence and ending up in state $S_i$. We need to sum up over all the possible final states. The $ alpha_{t}(i)$ can be represented as a matrix of size $T times N$. Where $T$ is the size of the observation sequence and $N$ the number of states. Let&#39;s see an example. . Let&#39;s see a simple example. Assume a system with three states $S= {S_1, S_2, S_3 }$. Futher, assume that the observation sequence consists of elements from the following set $V= {a, b,c }$. Also let&#39;s assume the following HMM: . $$ boldsymbol{ pi}= begin{bmatrix}0.7 &amp; 0.15 &amp; 0.15 end{bmatrix}$$ . $$ mathbf{A}= begin{bmatrix}0.5 &amp; 0.25 &amp; 0.25 0.1 &amp; 0.8 &amp; 0.1 0.3 &amp; 0.15 &amp; 0.6 end{bmatrix}$$ . $$ mathbf{B}= begin{bmatrix} 0.16 &amp; 0.26 &amp; 0.58 0.25 &amp; 0.28 &amp; 0.47 0.2 &amp; 0.1 &amp; 0.7 end{bmatrix}$$ . We want to calculate the probability $P(O| lambda)$ where $O= {a, b, a, c, b, a }$. We will use the forward algorithm for this. We will first do the computation using pencil and paper and then write a small Python script for us. We create the matrix $ alpha$: . $$ alpha = begin{bmatrix}0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 end{bmatrix}$$ . The first step is the initialization of the matrix. We take the first observation in the sequence $O$ which &#39;a&#39;. This has to be mapped to an index. Assume that we have available such a mapping: . $$ {a:0, b:1, c:2 }$$ . Further assume that we use zero-based counting. We have . $$ alpha_{0,0} = pi_0 mathbf{B}_{0, 0} = 0.7 0.16 = 0.112$$ . $$ alpha_{0,1} = pi_1 mathbf{B}_{1, 0} = 0.15 0.25 = 0.0375 $$ . $$ alpha_{0,2} = pi_2 mathbf{B}_{2, 0} = 0.15 0.2 = 0.03$$ . we now proceed to the calculation of the probabilities of the next symbols. The symbol &#39;b&#39;, which is the next symbol in $O$ has index 1. Its probabilities are . $$ alpha_{1,0} = mathbf{B}_{0,1}( alpha_{0,0} mathbf{A}_{0,0} + alpha_{0,1} mathbf{A}_{1,0} + alpha_{0,2} mathbf{A}_{2,0}) $$ . Similarly for $ alpha_{1,1}$ and $ alpha_{1,2}$ . $$ alpha_{1,1} = mathbf{B}_{1,1}( alpha_{0,0} mathbf{A}_{0,1} + alpha_{0,1} mathbf{A}_{1,1} + alpha_{0,2} mathbf{A}_{2,1}) $$ . $$ alpha_{1,2} = mathbf{B}_{2,1}( alpha_{0,0} mathbf{A}_{0,2} + alpha_{0,1} mathbf{A}_{1,2} + alpha_{0,2} mathbf{A}_{2,2}) $$ . After filling the matrix $ alpha$ we can calculate the probability $P(O| lambda)$, This is given by the following sum: . $$P(O| lambda) = alpha_{5,0} + alpha_{5,1} + alpha_{5,2}$$ . Below is a simple Python script that performs these tedious calculations for us. . import numpy as np . obs_to_idx = {&#39;a&#39;:0, &#39;b&#39;: 1, &#39;c&#39;:2} # state to index map state_to_idx = {&#39;S1&#39;:0, &#39;S2&#39;:1, &#39;S3&#39;: 2} . pi = np.array([0.7, 0.15, 0.15]) # transition probabilities A = np.array([[0.5, 0.25, 0.25], [0.1, 0.8, 0.1], [0.3, 0.15, 0.6]]) # emission probabilties B = np.array([[0.16, 0.26, 0.58], [0.25, 0.28, 0.47], [0.2, 0.1, 0.7]]) . o = [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] . a = np.zeros(shape=(len(o),A.shape[0])) . # initialize alpha for i in range(len(state_to_idx)): # only first row the rest is zero a[0][i] = pi[i]*B[i][obs_to_idx[o[0]]] . print(&quot;Initial probabilities&quot;) print(a) . Initial probabilities [[0.112 0.0375 0.03 ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ] [0. 0. 0. ]] . for t in range(1, len(o)): for j in range(A.shape[0]): a[t][j] = 0 # fix j = state_idx and sum over the states for i in range(A.shape[0]): a[t][j] += a[t -1][i] * A[i][j] a[t][j] *= B[j][obs_to_idx[o[i]]] print(&quot;alpha matrix: &quot;) print(a) . alpha matrix: [[1.12000000e-01 3.75000000e-02 3.00000000e-02] [1.10000000e-02 1.56250000e-02 9.95000000e-03] [1.60760000e-03 4.18562500e-03 2.05650000e-03] [2.94290000e-04 1.01471875e-03 4.10872500e-04] [5.95005800e-05 2.36744594e-04 8.43135750e-05] [1.25950115e-05 5.42294641e-05 1.78275499e-05]] . prob = 0; for i in range(A.shape[0]): prob += a[len(o)-1][i] print(&quot;Probability for sequence {0} is {1} &quot;.format(o, prob)) . Probability for sequence [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] is 8.465202543750001e-05 . Log probabilities . The forward variable, as well as the backward variable used in the backward algorithm are calculated as products of probabilities. When we have long sequences this may result in underflow [1]. In order to avoid this, we normalize $ alpha_t(i)$ by multiplying it with . $$c_t = frac{1}{ sum_{j} alpha_t(j)}$$ . After this normalization, $P(O| lambda)$ is given by . $$P(O| lambda) = frac{1}{ Pi_t c_t}$$ . or . $$logP(O| lambda) = - sum_t log c_t$$ . The motivation behind the forward algorithm is nicely presented in the following video . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EbxLWGw2zJ4&#39;, width=800, height=300) . The following video nicely explains both the forward and backward algorithms. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;gYma8Gw38Os&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. | Forward–backward algorithm. | Lawrence R. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning%20forward-algorithm/dynamic-programming/algorithms/numerics/2020/05/22/forward-algorithm.html",
            "relUrl": "/hidden-markov-model/machine-learning%20forward-algorithm/dynamic-programming/algorithms/numerics/2020/05/22/forward-algorithm.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Hidden Markov Models",
            "content": "We know that given a sample of independent observations we can get the likelihood of the sample by forming the product of the likelihoods of the individual instances. However, there are situations in which the assumption of observation independence simply breaks down. An example of such a situation is when we consider words from the English dictionary. In this case, within a word, successive letters are dependent; in English h is very likely to follow t but not x. In such a scenario, it is better to assume that the sequence is generated by a parametric random process. Our goal is to establish the parameters of this process. Hidden Markov models are a way to model such a process. Let&#39;s see how. . A hidden Markov model or HMM, is a statistical model in which the system being modeled is assumed to be a Markov process. Let&#39;s call that process with $X$. The system can be on a series of states from a give state set. However, we don&#39;t know at each time instant the specific state the system is in. In other words, the system state is unobservable. HMM assumes that there is another process, say $Y$, whose behavior depends on $X$. The goal is to learn about $X$ by observing $Y$. . Let&#39;s assume that at any time the system or random process we model can be in any of $N$ distinct states. Let&#39;s denote this set with $ mathbb{S}$ . $$ mathbb{S} = {S_1, S_2, cdots,S_N }$$ . Furthermore, let&#39;s denote the state that the system is at time $t$ by $q_t$. . The system can move from one state to another. The probability of being at state $S_j$ at time $t$ depends on the values of the previous states. We express this mathematically using the following conditional probability: . $$P(q_t=S_j | q_{t-1}=S_i, q_{t-2}=S_k, cdots)$$ . A hidden Markov model assume that system states form a Markov chain. To be more specific we will restrict ourselves to the first-order Markov model which is quite frequent in practice. In this case, the probability above simply becomes: . $$P(q_t=S_j | q_{t-1}=S_i, q_{t-2}=S_k, cdots)=P(q_t=S_j | q_{t-1}=S_i)$$ . In words, what the first order Markov property tells us is that the state of the system depends solely on the previous state; a rather memoryless situation. . Let&#39;s move further by introducing the so-called transition probabilities $ alpha_{i,j}$: . $$ alpha_{i,j}=P(q_t=S_j | q_{t-1}=S_i)$$ . Since the $ alpha_{i,j}$&#39;s are probabilities they should satisfy the followin constraints . $$ alpha_{i,j} geq 0, ~~~ sum_{j=1}^{N} alpha_{i,j} = 1$$ . These are nothing more than the usual axioms of the definition of probability. Note that for the latter condition we keep the $i$ index fixed. . We will assume that the transition probabilities are independent of time. What this means is that going from $S_i$ to $S_j$ has the same probability regardless of when it happens (i.e. in the observation sequence see below). . We usually arrange the transition probabilities into an $N times N$ matrix, denoted here with $ mathbf{A}$ that its rows sum to one. . We now have a way, or a model, that allows us to move from one state to another. However, we cannot do much with it as the states are unknown, hidden, unobserved or any other expression that suits your needs. The point is that we cannot access them. In order to have progress, hidden Markov models assume a second process that produces observation sequences. We can use this process to infer the state of the system. . Let&#39;s denote by $ lambda$ ( we will be more specific about what $ lambda$ denotes further below) the HMM instance we are using. Let $O_T$ be an observation sequence of length $T$. We assume that $O_T$ has elements from a given discrete set $ mathbb{V}$: . $$ mathbb{V}= {v_1, v_2, cdots, v_M }$$ . The set $ mathbb{V}$ has in total $M$ elements. Also let&#39;s introduce the mechanism that characterizes the generation of a sequence given a state $S_j$. This is done via the so-called emission probability matrix $b_j(m)$: . $$b_j(m) = P(O_t = v_m|q_t = S_j)$$ . this is the probability that we observe element $v_m$ at time $t$ when the system is at state $S_j$. For example, let&#39;s assume that that we have two states and three symbols and we are given the following emission probabilities matrix . $$ mathbf{B}= begin{bmatrix} 0.16 &amp; 0.26 &amp; 0.58 0.25 &amp; 0.28 &amp; 0.47 end{bmatrix}$$ . what this tells us is that at state $S_1$ symbol $v_1$ has probability 0.16 to be observed, symbol $v_2$ will be observed 26 % and symbol $v_3$ will be observed 58 %. . Although, we cannot observe the state sequence $Q$, this can be inferred from the observation sequence $O$. Note however that in general there are many different sequences $Q$ that can generate the same observation sequence. This is however done with different probabilities. This is similar when we have an iid sample from, say, a normal distribution; there are an infinite number of $ mu, sigma$ pairs possible which can generate the sameple. Thus, we are more interested in a maximum likelihood state sequence or a sequence that has the maximum probability of generating the sequence $O$. . What is $ lambda$? . Above we used the notation $ lambda$ in order to indicate a specific HMM instance. Let&#39;s see what this $ lambda$ parameter actually imply. This is also a summary of the basic element of an HMM. Specifically, . An HMM model assumes a set of states in the model $ mathbb{S} = {S_1, S_2, cdots,S_N }$ | . An HMM model assumes a number of distinct observation symbols $ mathbb{V}= {v_1, v_2, cdots, v_M }$ | . An HMM model assumes the existence of transition probabilities $ mathbf{A}$ where $ alpha_{i,j}=P(q_t=S_j | q_{t-1}=S_i)$ | . An HMM model assumes the existence of observation probabilities $ mathbf{B}$ where $b_j(m) = P(O_t = v_m|q_t = S_j)$ | . The last thing we need to talk about, is how to initialize the model. This is done by a vector of initial probabilties $ boldsymbol{ pi}$ where each $ pi_i = P(q_1 = S_i)$ that is each $ pi_i$ is the probability that the first state of the model is $S_i$. . The $ lambda$ parameter is the triplett consisting of the matrices $ mathbf{A}$, $ mathbf{B}$ and the vector $ boldsymbol{ pi}$ . $$ lambda = { mathbf{A}, mathbf{B}, boldsymbol{ pi} }$$ . For a state set with $N$ states, $ mathbf{A}$ is $N times N$. Likewise for a set $V$ with $M$ symbols, $ mathbf{B}$ is $N times M$. Finally the vector $ boldsymbol{ pi}$ has size $N$. . Typically, when dealing with an HMM we are intersted in the following three problems [1] . Given an HMM i.e. $ lambda$ evaluate the probability of a given observation sequence: | $$P(O| lambda)$$ . Given an HMM and an observation sequence $O$ we want to find the state sequence $Q$ with the highest probability of producing $O$ i.e we want to find $Q$ such that | $$P(Q|O, lambda) ~~ text{is maximum}$$ . Given a training set of observation sequences $ mathbf{X}$ we want to learn the HMM that maximizes the probability of generating $ mathbf{X}$ that is we want to find $ lambda$ so that | $$P( mathbf{X}| lambda)~~ text{is maximum}$$ . Checkout the video below for a motivation about Hidden Markov models . from IPython.display import YouTubeVideo YouTubeVideo(&#39;PAngl8DZ8yk&#39;, width=800, height=300) . The following video explains the Markov property . from IPython.display import YouTubeVideo YouTubeVideo(&#39;J_y5hx_ySCg&#39;, width=800, height=300) . References . Ethem Alpaydin, Introduction To Machine Learning, Second Edition, MIT Press. |",
            "url": "https://pockerman.github.io/qubit_opus/hidden-markov-model/machine-learning/algorithms/numerics/2020/05/21/hidden-markov-model.html",
            "relUrl": "/hidden-markov-model/machine-learning/algorithms/numerics/2020/05/21/hidden-markov-model.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Matrix Approximation",
            "content": "Very often in numerical modeling, the data we need to work with is rather large and therefore not really handy. Moreover, the behavior under investigation can be explained by small number of features. Thus, it is desireable to be able to approximate the matrices. One way to do so is using the singular value decomposition method. . In fact SVD provides an optimal low-rank approaximation to a matrix $A$ (this is the Eckart-Young theorem). We can obtain a hierarchy of low rank matrices by just keeping the leading $k$ singular values and the corresponding eigenvectors. . Image copression is a simple example illustrating matrix approximation using SVD. We can view a grayscale image as matrix $A in mathbb{R}^{n times m}$ where $n, m$ are the number of pixels in the vertical and horizonal directions. . The Python code below computes the full SVD of the matrix representing the loaded image. We then compute approximations of the image using a range of retained singular values. We can see that as the number of retained singular values increases the quality of the image increases. . import numpy as np from matplotlib import image import matplotlib.pyplot as plt from numpy.linalg import matrix_rank . def rgb2gray(rgb): return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]) . A = image.imread(&#39;my_icons/volvo_img.png&#39;) A = np.mean(A, -1)#rgb2gray(rgb=data) # summarize shape of the pixel array print(A.shape) # display the array of pixels as an image plt.imshow(A) plt.show() . (366, 424) . print(&quot;Matrix rank &quot;, matrix_rank(A)) . Matrix rank 307 . U, S, V = np.linalg.svd(A, full_matrices=False) S = np.diag(S) . print(&quot;Shape U &quot;, U.shape) print(&quot;Shape S &quot;, S.shape) print(&quot;Shape V &quot;, V.shape) . Shape U (366, 366) Shape S (366, 366) Shape V (366, 424) . for r in [5, 20, 100]: print(&quot;Working with r&quot;, r) # construct approximate image img_approx = U[:,:r] @ S[0:r,:r] @ V[:r,:] plt.imshow(img_approx) plt.show() . Working with r 5 . Working with r 20 . Working with r 100 . plt.semilogy(np.diag(S)) plt.show() . plt.plot(np.cumsum(np.diag(S))/np.sum(np.diag(S))) plt.show() .",
            "url": "https://pockerman.github.io/qubit_opus/linear-algebra/singular-value-decomposition/matrix-approximation/algorithms/numerics/2020/03/14/matrix-approximation.html",
            "relUrl": "/linear-algebra/singular-value-decomposition/matrix-approximation/algorithms/numerics/2020/03/14/matrix-approximation.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pockerman.github.io/qubit_opus/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://pockerman.github.io/qubit_opus/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Books",
          "content": "Mathematics . Kevin P. Murphy Machine Learning. A Probabilistic Perspective | Thomas Witelski and Mark Bowen Methods of Mathematical Modelling. Continuous Systems and Differential Equations | Steven L. Brunton, J. Nathan Kutz Data-Driven Science and Engineering Machine Learning, Dynamical Systems and Control | Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction | . Physics . Leonard Susskind and Art Friedman, Quantum Mechanics. The Theoretical Minimum | . Computing . Len Bass, Paul Clements, Rick Kazman Software Architecture In Practice | David Mermin Quantum Computer Science. An Introduction | . History . Herodotus The Histories | Max Hastings Catastrophe | . Philosophy . Sun Tzu The Art Of War | Nicolo Machiavelli The Prince | . Literature . Antoine De Saint-Exupery Ο Μικρος Πριγγιπας | Albert Camus The Fall | Albert Camus The Outsider | George Orwell 1984 | Douglas R. Hostadter Godel, Escher, Bach: An Enternal Golden Braid | Fyodor Dostoyevsky Crime And Punishment | Fyodor Dostoyevsky The Idiot | Ernest Hemingway For Whom The Bell Tolls | Ernest Hemingway The Old Man And The Sea | .",
          "url": "https://pockerman.github.io/qubit_opus/books/",
          "relUrl": "/books/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pockerman.github.io/qubit_opus/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}