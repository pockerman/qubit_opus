{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration With Scala 2\n",
    "\n",
    "\n",
    "\n",
    "> \"Scala based implementation of the value iteration algorithm\"\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: false\n",
    "- author: Alexandros Giavaras\n",
    "- categories: [scala, reinforcement-learning, algorithms, dynamic-programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ekf\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a <a href=\"https://pockerman.github.io/qubit_opus/scala/reinforcement-learning/algorithms/dynamic-programming/2021/04/23/scala-value-iteration.html\">previous post</a> we saw how to implement the value iteration algorithm in Scala. This is particularly easy algorithm to implement. In that post, we used a dummy environment to check on the algorithm. In this post we extend our implementation by integrating <a href=\"https://scalapy.dev/\">ScalaPy</a> in our implementation. This way we can interact  with OpenAI Gym library which is written in Python. However, this way we have access to various environments to test our reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ekf\"></a> Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that one drawback of policy iteration is that each of its iterations involves a policy evaluation. This, however, may itself be an iterative computation; therefore requiring multiple sweeps through the state set [1]. Furthermore, if the evaluation is done iteratively, then convergence to $V_{\\pi}$ occurs only in the limit [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Luckily, the policy evaluation step of policy iteration can truncated without loosing the convergence gurantees of the method. Moreover, this can be done in several different ways [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, when policy evaluation\n",
    "is stopped after just one update of each state, the algorithm is called **value iteration**. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_{k+1}(s) = max_{\\alpha}\\sum_{s^*, r}p(s^*, r | s, \\alpha)\\left[r + \\gamma V_{k}(s^*)\\right], ~~ \\forall s \\in \\mathbb{S}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/value_itr_algo.png)\n",
    "*Figure 1: Value iteration algorithm. Image from [1].*\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value iteration is obtained simply by turning the Bellman optimality equation into an update rule [1]. It requires the maximum to be taken over all the actions. Furthermore, the algorithm terminates by checking the amount of change of the value function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"code\"></a> Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym provides a large collection of environments to test on reinforcement learning algorithms. The library is written in Python. Hence, we cannot use here as is. Fortunately, <a href=\"https://scalapy.dev/\">ScalaPy</a> allows us to use Python libraries from Scala code. Check the ScalaPy how to install it on your machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to implement a simple wrapper over the <a hre=\"https://gym.openai.com/envs/FrozenLake-v0/\">FrozenLake-v0</a> environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.mutable.ArrayBuffer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.control.Breaks._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.math.max\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.util.control.Breaks._\n",
    "import scala.math.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package engine.worlds\n",
    "\n",
    "import me.shadaj.scalapy.py\n",
    "\n",
    "\n",
    "class FrozenLake(val version: String) {\n",
    "\n",
    "  // load the gym module\n",
    "  private val gym = py.module(\"gym\")\n",
    "  private var env: me.shadaj.scalapy.py.Dynamic = null\n",
    "\n",
    "  // Make the environment\n",
    "  def make: Unit = this.env = gym.make(this.name)\n",
    "\n",
    "  // Reset the environment\n",
    "  def reset: Int = {\n",
    "\n",
    "    val state = this.env.reset()\n",
    "    state.as[Int]\n",
    "  }\n",
    "\n",
    "  // Returns the name of the environment\n",
    "  def name: String = {\"FrozenLake-\" + version}\n",
    "\n",
    "  // Returns the number of states\n",
    "  def nStates: Int = this.env.observation_space.n.as[Int]\n",
    "\n",
    "  // Returns the number of actions\n",
    "  def nActions: Int = this.env.action_space.n.as[Int];\n",
    "\n",
    "  // \n",
    "  def getDynamics(state: Int, action: Int): Seq[(Double, Int, Double, Boolean)] = {\n",
    "\n",
    "    val P = py.Dynamic.global.dict(this.env.P)\n",
    "    val dynamicsTuple = P.bracketAccess(state)\n",
    "    val result = dynamicsTuple.bracketAccess(action).as[Seq[Tuple4[Double, Int,Double, Boolean]]]\n",
    "    result\n",
    "  }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very simple wrapper suitable though for our needs. It hides all the boilerplate code we need to have  in order to interface with OpenAI Gym. Let's also change the implementation of the ```ValueIteration``` class.\n",
    "The class has two implementation depending on the ```trainMode``` value defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package engine.rl\n",
    "\n",
    "object TrainMode extends Enumeration {\n",
    "\n",
    "  val DEFAULT = Value(0, name = \"DEFAULT\")\n",
    "  val STOCHASTIC = Value(1, name = \"STOCHASTIC\")\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```STOCHASTIC``` mode walks in the environment by randomly selecting an action. The ```DEFAULT``` implements the algorithm as we implemented in the previous related post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package engine.rl\n",
    "\n",
    "\n",
    "import scala.collection.mutable\n",
    "import breeze.linalg.{DenseVector, max}\n",
    "import engine.worlds.DiscreteEnvironment\n",
    "\n",
    "import scala.util.control.Breaks.{break, breakable}\n",
    "\n",
    "\n",
    "class ValueIteration(env: DiscreteEnvironment,\n",
    "                     gamma: Double, maxIterations: Int, tolerance: Double,\n",
    "                     trainMode: TrainMode.Value=TrainMode.DEFAULT) {\n",
    "\n",
    "  val rewards = new mutable.HashMap[Tuple3[Int, Int, Int], Double]()\n",
    "  val transits = new mutable.HashMap[Tuple3[Int, Int, Int], Int]\n",
    "  var stateValues = DenseVector.zeros[Double](env.nStates)\n",
    "  var residual = 1.0\n",
    "\n",
    "  def train:  Unit = {\n",
    "\n",
    "    this.rewards.clear()\n",
    "    this.transits.clear()\n",
    "    this.stateValues = DenseVector.zeros[Double](env.nStates)\n",
    "\n",
    "    breakable {\n",
    "\n",
    "      for(itr <- Range(0, maxIterations)){\n",
    "\n",
    "        println(\"> Learning iteration \" + itr)\n",
    "        println(\"> Learning residual \" + residual)\n",
    "\n",
    "        step\n",
    "\n",
    "        if(residual < tolerance) break;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def step: Unit ={\n",
    "\n",
    "    // stop condition\n",
    "    var  delta = 0.0\n",
    "\n",
    "    // update each state\n",
    "    for( s <- 0 until env.nStates){\n",
    "\n",
    "      // Do a one-step lookahead to find the best action\n",
    "      val value = oneStepLookahead(state=s)\n",
    "      val bestActionValue = breeze.linalg.max(value)\n",
    "      delta = math.max(delta, math.abs(bestActionValue - stateValues(s)))\n",
    "      stateValues(s) = bestActionValue\n",
    "    }\n",
    "\n",
    "    residual = delta\n",
    "  }\n",
    "\n",
    "  def defaultStep: Unit = {\n",
    "\n",
    "  }\n",
    "\n",
    "  def stochasticStep: Unit = {\n",
    "\n",
    "  }\n",
    "\n",
    "  protected  def oneStepLookahead(state: Int): DenseVector[Double] = {\n",
    "\n",
    "    val values = DenseVector.zeros[Double](env.nActions)\n",
    "\n",
    "    for(action <- 0 until env.nActions) {\n",
    "\n",
    "      val dynamics = env.getDynamics(state = state, action = action)\n",
    "\n",
    "      for(item <- dynamics.indices){\n",
    "\n",
    "        val prob = dynamics(item)._1\n",
    "        val next_state = dynamics(item)._2\n",
    "        val reward = dynamics(item)._3\n",
    "        values(action) += prob * (reward + gamma * stateValues(next_state))\n",
    "\n",
    "        if(rewards.contains((state, action, next_state))){\n",
    "          rewards.update((state, action, next_state), reward)\n",
    "          transits.update((state, action, next_state), 1)\n",
    "        }\n",
    "        else{\n",
    "          rewards.addOne((state, action, next_state), reward)\n",
    "          transits.addOne((state, action, next_state), 1)\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    values\n",
    "  }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"refs\"></a> References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Richard S. Sutton and Andrew G. Barto, ```Reinforcement Learning: An Introduction```."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
