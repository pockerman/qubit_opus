{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark with Scala\n",
    "\n",
    " > \"Submit a Scala application to Spark.\"\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: false\n",
    "- author: Alexandros Giavaras\n",
    "- categories: [Spark, Scala, API, data-analysis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, I describe the steps you need to take in order to submit a Scala application to be executed from Spark. The official documentation can be found <a href=\"https://spark.apache.org/docs/latest/quick-start.html\">here</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark with Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will assume that the environment is already set. Meaning,  Scala is already installed, in this post I use version 2.13.3,  Spark is also installed, for this post I use version 3.0.1. Finally, I am using <a href=\"https://www.scala-sbt.org/1.x/docs/index.html\">SBT </a> for building and packaging the application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scala application simply informs us about the version of Spark we are using, the name of the master node and whether we run in local or distributed mode. It is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "//HelloSpark.scala\n",
    "\n",
    "\n",
    "package spark\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "object HelloSpark {\n",
    "  def main(args: Array[String]) {\n",
    "    \n",
    "    val conf = new SparkConf().setAppName(\"Hello Scala Spark\")\n",
    "    val sc = new SparkContext(conf)\n",
    "    println(\"Spark version:            \" + sc.version)\n",
    "    println(\"Spark master:             \" + sc.master)\n",
    "    println(\"Spark running 'locally'?: \" + sc.isLocal)\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that I placed the application under the ```spark``` package. This will be used when submitting the class to Spark. For the moment we need to structure properly our code for SBT to work. In particular, we need a file structure as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "build.sbt\n",
    "+src/\n",
    "    +main/\n",
    "         +scala/\n",
    "               +spark/HelloSpark.scala\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```build.sbt``` script is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "name := \"Hello Spark\"\n",
    "\n",
    "version := \"0.0.1\"\n",
    "\n",
    "scalaVersion := \"2.13.3\"\n",
    "\n",
    "libraryDependencies += \"org.apache.spark\" % \"spark-core_2.12\" % \"3.0.1\"\n",
    "libraryDependencies += \"org.apache.spark\" % \"spark-sql_2.12\" % \"3.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side not, observe that I don't use double percentages. The reason why is explained <a href=\"https://stackoverflow.com/questions/60346192/sbt-package-is-trying-to-download-a-package-whose-path-does-not-exist\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compile our code, call ```sbt``` at the level where the ```build.sbt``` script is located. This will bring up the ```sbt``` console. Once in the console, type ```compile``` to build the project. When the compilation finishes and still in the ```sbt``` console, type ```package``` to create the application ```.jar``` file. The whole process for this application should not take long. Once finished, we can submit our application to Spark for execution. I use the following bash shell script for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "/home/alex/MySoftware/spark-3.0.1-bin-hadoop2.7/bin/spark-submit \\\n",
    "  --class \"spark.HelloSpark\" \\\n",
    "  --master local[4] \\\n",
    "  target/scala-2.13/hello-spark_2.13-0.0.1.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how I specify the class to execute by prefixing it with the package name it belongs to.  Upon execution of the script you should see something similar to what follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Spark version:            3.0.1\n",
    "Spark master:             local[4]\n",
    "Spark running 'locally'?: true\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, I described the steps I need to take in order to build and submit a Scala application to Spark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
