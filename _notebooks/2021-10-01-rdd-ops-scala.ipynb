{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD operations with Scala\n",
    "\n",
    "> 'Basic operationson on Spark RDD with Scala'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of this notebooks is to a large extent edited from [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD operations with Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Spark RDD provides two types of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformations\n",
    "- Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformation operation creates a new RDD from an existing RDD. Moreover, we can apply a chain of\n",
    "transformations once the data is loaded into memory. Some common transformations are ```filter``` and ```map```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark transformations are lazy evaluated. What this means that a transformation is applied only when an action is called. Examples of actions are ```collect``` and ```count```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will review some common RDD transformations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```map(function)```: It returns a new data set by operating on each element of the source RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```flatMap(function)```: Similar to map, but each item can be mapped to zero, one, or more items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```mapPartitions(function)```: Similar to map, but works on the partition level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```mapPartitionsWithIndex(function)```: Similar to ```mapPartitions```, but provides a function with an Int value to indicate the index position of the partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```filter(function)```: It returns a new RDD that contains only elements that satisfy the predicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Subhashini Chellappan, Dharanitharan Ganesan, ```Practical Apache Spark. Using the Scala API```, Apress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
