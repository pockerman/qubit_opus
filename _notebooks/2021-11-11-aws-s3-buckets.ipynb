{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS. S3 Buckets\n",
    "\n",
    "> 'Working with AWS S3 buckets'\n",
    "\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: false\n",
    "- author: Alexandros Giavaras\n",
    "- categories: [aws, s3-buckets, cloud-computing, data-storage, data-engineering, data-storage, boto3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to have a brief view on the <a href=\"https://aws.amazon.com/s3/?nc=sn&loc=0\">AWS S3 storage</a> (a high level overview of the storage types in AWS can be found in <a href=\"https://pockerman.github.io/qubit_opus/aws/aws-storage-services/cloud-computing/data-storage/2022/05/30/aws-storage-services.html\">AWS. Storage services</a> this literally a 2 minutes read). Concretely, we are going to discuss the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to create an AWS S3 Bucket\n",
    "- How to upload and download items  \n",
    "- How to do multi-part file transfer\n",
    "- How to generate pre-signed URLS.\n",
    "- How to set up bucket policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we will work with AWS S3 buckets using the Boto3 Python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS S3 is an object storage system. It is designed to store any type of data. Furthermore, it is a serverless service. Items in S3 are stored in buckets. As we add items to the buckets its size grows. In theory there is no limit on the size a bucket can get. By design, S3  has 11 9's of durability and stores data for millions of applications. S3 files are referred to as objects. You can find more information about S3 <a href=\"https://aws.amazon.com/s3/?p=ft&c=st&z=3\">here</a> and <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items in S3 are stored in buckets. The user/application must create a bucket before storing any data on S3. The bucket name must"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Be unique\n",
    "- Be at least 3 and no more than 63 character long\n",
    "- Cannot be an IP address format\n",
    "- Be lowercase\n",
    "- Must not contain uppercase characters or underscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will use the Boto3 Python package to interact with AWS S3; that is to create a bucket, upload and upload files in the created bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials to be used\n",
    "AWS_ACCESS_KEY_ID = 'Use your own credentials'\n",
    "AWS_SECRET_ACCESS_KEY = 'Use your own credentials'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a client for the resource we will use\n",
    "s3_client = boto3.client('s3', region_name='us-west-2',\n",
    "                       aws_access_key_id=AWS_ACCESS_KEY_ID, \n",
    "                       aws_secret_access_key=AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = {'LocationConstraint': 'us-west-2'}\n",
    "s3_client.create_bucket(Bucket='coursera-s3-bucket',\n",
    "                       CreateBucketConfiguration=location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response of the function all above is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{'ResponseMetadata': {'RequestId': '355VX5QNYSQBTSCM',\n",
    "  'HostId': '7jXN853VP175Fw/il1Zvx8UXkfRsdQRXH3VrAFOcCYZl4y2ZTF6zNPp6tXvwnpBGlmAKTCP9RFA=',\n",
    "  'HTTPStatusCode': 200,\n",
    "  'HTTPHeaders': {'x-amz-id-2': '7jXN853VP175Fw/il1Zvx8UXkfRsdQRXH3VrAFOcCYZl4y2ZTF6zNPp6tXvwnpBGlmAKTCP9RFA=',\n",
    "   'x-amz-request-id': '355VX5QNYSQBTSCM',\n",
    "   'date': 'Thu, 11 Nov 2021 11:01:14 GMT',\n",
    "   'location': 'http://coursera-s3-bucket.s3.amazonaws.com/',\n",
    "   'server': 'AmazonS3',\n",
    "   'content-length': '0'},\n",
    "  'RetryAttempts': 0},\n",
    " 'Location': 'http://coursera-s3-bucket.s3.amazonaws.com/'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 access can be modified via:\n",
    "\n",
    "- IAM policies\n",
    "- Bucket policies\n",
    "- Access control lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload and object to a bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the policies attached to a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 3_client.get_bucket_policy(Bucket='bucket-name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call above fails because by default there are no policies set. A bucket's policy can be set by calling the ```put_bucket_policy``` method. Moreover, a policy is defined in the same JSON format as an IAM policy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Sid (statement ID)** is an optional identifier that you provide for the policy statement. You can assign a Sid value to each statement in a statement array.\n",
    "\n",
    "The **Effect** element is required and specifies whether the statement results in an allow or an explicit deny. Valid values for Effect are Allow and Deny.\n",
    "\n",
    "By default, access to resources is denied. \n",
    "\n",
    "Use the **Principal** element in a policy to specify the principal that is allowed or denied access to a resource.\n",
    "\n",
    "You can specify any of the following principals in a policy:\n",
    "\n",
    "- AWS account and root user\n",
    "- IAM users\n",
    "- Federated users (using web identity or SAML federation)\n",
    "- IAM roles\n",
    "- Assumed-role sessions\n",
    "- AWS services\n",
    "- Anonymous users\n",
    "\n",
    "\n",
    "The **Action** element describes the specific action or actions that will be allowed or denied. \n",
    "\n",
    "We specify a value using a service namespace as an action prefix (iam, ec2, sqs, sns, s3, etc.) followed by the name of the action to allow or deny.\n",
    "\n",
    "The **Resource** element specifies the object or objects that the statement covers. We specify a resource using an ARN. Amazon Resource Names (ARNs) uniquely identify AWS resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORS Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.get_bucket_cors(Bucket=bucket_name)\n",
    "print(response['CORSRules'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cors_configuration = {\n",
    "    'CORSRules':[{'AllowHeaders':['Authorization'],\n",
    "                'AllowedMethods':['GET', 'PUT'],\n",
    "                'AllowedOrigins':['*'],\n",
    "                'ExposeHeaders':['GET', 'PUT'],\n",
    "                 'MaxAgeSeconds':3000}\n",
    "                ]\n",
    "}\n",
    "\n",
    "response = s3_client.put_bucket_cors(Bucket=bucket_name, CORSConfiguration=cors_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glacier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a href=\"https://aws.amazon.com/s3/storage-classes/glacier/\">Glacier</a> storage is the cheapest storage option in AWS. It is used for long term storage of data. Just like S3 is a serverless service. However, it is not as readily accessible as S3. Given this, typically Glacier storage is used for archiving data rather than storing data used on a daily basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It provides three retrieval options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Expedited (1-5 minutes)\n",
    "- Standard (3-5 hours)\n",
    "- Bulk (5-12 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a href=\"https://aws.amazon.com/s3/?p=ft&c=st&z=3\">AWS S3</a>\n",
    "2. <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\">What is Amazon S3?</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
