{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark. Create an RDD with Scala\n",
    "\n",
    "> \"Create an RDDs with Scala\"\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: false\n",
    "- author: Alexandros Giavaras\n",
    "- categories: [spark, scala, big-data, data-engineering, data-analysis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post I want to review how to create Spark RDDs within a Scala applications. There are various methods to do so. The following example is taken for <a href=\"https://sparkbyexamples.com/apache-spark-rdd/how-to-create-an-rdd-using-parallelize/\">Spark by {Examples}</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark RDD with Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Resilient Distributed Datasets, or RDD for short,  is the  fundamental data structure of Spark. An RDD is an immutable collection of objects that can be distributed across a cluster of computers. An RDD collection is divided into a number of partitions so that each node on a Spark cluster  can independently perform computations. Note that modern Spark applications will most likely be using ```DataFrame``` and ```DataSet``` objects which are data structures on top of RDD. This means that RDDs are rather low level and if possible we should avoid it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, there are two main methods available in Spark to create an RDD: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```SparkContext.parallelize``` method\n",
    "- Read from a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method is illustrated in the example below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "package train.spark\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "object CreateRDD {\n",
    "  def main(args: Array[String]) {\n",
    "    \n",
    "    val conf = new SparkConf().setAppName(\"Hello Scala Spark\")\n",
    "    val sc = new SparkContext(conf)\n",
    "    \n",
    "    val data = Array(1,2,3,4,5,6,7,8,9,10)\n",
    "    val rdd = sc.parallelize(data)\n",
    "    rdd.foreach(println)\n",
    "    \n",
    "    println(\"Number of Partitions: \"+rdd.getNumPartitions)\n",
    "    println(\"Action: First element: \"+rdd.first()) \n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the application produces something like the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "3\n",
    "6\n",
    "1\n",
    "8\n",
    "9\n",
    "2\n",
    "7\n",
    "4\n",
    "5\n",
    "10\n",
    "Number of Partitions: 4\n",
    "Action: First element: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the the output may be different as it depends on which thread is accessing  the standard output first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method is to read a file from disk. This is also shown in the snippet below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "package train.spark\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "object CreateRDDFile {\n",
    "  def main(args: Array[String]) {\n",
    "    \n",
    "    val conf = new SparkConf().setAppName(\"Hello Scala Spark\")\n",
    "    val sc = new SparkContext(conf)\n",
    "    \n",
    "    // Should be some file on your system\n",
    "    val csvFile = \"/home/alex/qi3/learn_scala/scripts/spark/data/train.csv\" \n",
    "    val csvRDD = sc.textFile(csvFile)\n",
    "        \n",
    "    println(\"Number of Partitions: \"+csvRDD.getNumPartitions)\n",
    "    println(\"Action: First element: \"+csvRDD.first()) \n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon executing this code, I get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Number of Partitions: 2\n",
    "Action: First element: #Duplicate: 0, Delete: 1, Normal-1: 2, TUF: 3, Normal-2: 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I am interested in converting the contents of the file into floating point numbers so that I can feed them to a machine learning algorithm. I can do this as follows.  I can use the ```map()``` function to convert the ```RDD[String]``` into an ```RDD[Array[Double]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "val doubleRDD = csvRDD.map(line => {line.split(\",\")})\n",
    "                      .map( arrString => {Try(Array(arrString(0).toDouble, arrString(1).toDouble,                                                                   arrString(2).toDouble))})\n",
    "                      .map(_ match {case Success(res) => res\n",
    "                                         case Failure(res) => Array(-100, -100, -100)})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a schema in order to let Spark know the type of the data but this requires that we use a ```DataFrame``` instead and not an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that Spark divides by default data into two partitions and distributes them across a cluster. The number of partitions can be specified while creating an RDD as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "// Should be some file on your system\n",
    "val csvFile = \"/home/alex/qi3/learn_scala/scripts/spark/data/train.csv\" \n",
    "\n",
    "// use four partitions for the data\n",
    "val csvRDD = sc.textFile(csvFile, 4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
