{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark. Create an RDD with Scala\n",
    "\n",
    "> \"Create an RDD with Scala\"\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: false\n",
    "- author: Alexandros Giavaras\n",
    "- categories: [spark, scala, big-data, data-engineering, data-analysis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we will see how to create a Spark RDD within a Scala application.  Although, modern applications most likely will be using the ```DataFrame``` and/or ```DataSet``` APIs, still the RDD data structure is what lies underneath the latter two and therefore always useful to know. As we will see, there are various methods to create an RDD in Spark. The following example is taken for <a href=\"https://sparkbyexamples.com/apache-spark-rdd/how-to-create-an-rdd-using-parallelize/\">Spark by {Examples}</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark RDD with Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Resilient Distributed Datasets, or RDD for short,  is the  fundamental data structure of Spark and underlies both the ```DataFrame``` and ```DataSet``` data structures. An RDD is an immutable collection of objects that can be distributed across a cluster of computers. An RDD collection is divided into a number of partitions so that each node on a Spark cluster  can independently perform computations. . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main methods available in Spark to create an RDD: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```SparkContext.parallelize``` method\n",
    "- Read from a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method is illustrated in the example below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "package train.spark\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "object CreateRDD {\n",
    "  def main(args: Array[String]) {\n",
    "    \n",
    "    val conf = new SparkConf().setAppName(\"Hello Scala Spark\")\n",
    "    val sc = new SparkContext(conf)\n",
    "    \n",
    "    val data = Array(1,2,3,4,5,6,7,8,9,10)\n",
    "    val rdd = sc.parallelize(data)\n",
    "    rdd.foreach(println)\n",
    "    \n",
    "    println(\"Number of Partitions: \"+rdd.getNumPartitions)\n",
    "    println(\"Action: First element: \"+rdd.first()) \n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the application produces something like the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "3\n",
    "6\n",
    "1\n",
    "8\n",
    "9\n",
    "2\n",
    "7\n",
    "4\n",
    "5\n",
    "10\n",
    "Number of Partitions: 4\n",
    "Action: First element: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the the output may be different as it depends on which thread is accessing  the standard output first.\n",
    "Note that the application above has to create a ```SparkContext``` first before we are able to create an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Remark**\n",
    "\n",
    "Creating a ```SparkContext``` is not necessary when we use the Scala Spark shell as one such object is already created for us.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method is to read a file from disk. This is also shown in the snippet below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "package train.spark\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "object CreateRDDFile {\n",
    "  def main(args: Array[String]) {\n",
    "    \n",
    "    val conf = new SparkConf().setAppName(\"Hello Scala Spark\")\n",
    "    val sc = new SparkContext(conf)\n",
    "    \n",
    "    // Should be some file on your system\n",
    "    val csvFile = \"/home/alex/qi3/learn_scala/scripts/spark/data/train.csv\" \n",
    "    val csvRDD = sc.textFile(csvFile)\n",
    "        \n",
    "    println(\"Number of Partitions: \"+csvRDD.getNumPartitions)\n",
    "    println(\"Action: First element: \"+csvRDD.first()) \n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon executing this code, we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Number of Partitions: 2\n",
    "Action: First element: #Duplicate: 0, Delete: 1, Normal-1: 2, TUF: 3, Normal-2: 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we are interested in converting the contents of the file into floating point numbers so that we can feed them to a machine learning algorithm. We can do this as follows.  we can use the ```map()``` function to convert the ```RDD[String]``` into an ```RDD[Array[Double]]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "val doubleRDD = csvRDD.map(line => {line.split(\",\")})\n",
    "                      .map( arrString => {Try(Array(arrString(0).toDouble, arrString(1).toDouble,                                                                   arrString(2).toDouble))})\n",
    "                      .map(_ match {case Success(res) => res\n",
    "                                         case Failure(res) => Array(-100, -100, -100)})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a schema in order to let Spark know the type of the data but this requires that we use a ```DataFrame``` instead and not an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that Spark divides by default data into two partitions and distributes them across a cluster. The number of partitions can be specified while creating an RDD as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "// Should be some file on your system\n",
    "val csvFile = \"/home/alex/qi3/learn_scala/scripts/spark/data/train.csv\" \n",
    "\n",
    "// use four partitions for the data\n",
    "val csvRDD = sc.textFile(csvFile, 4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, we can create an RDD by using the following also:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- JDBC\n",
    "- Cassandra\n",
    "- HBase\n",
    "- Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\">RDD Programming Guide</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
