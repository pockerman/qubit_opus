{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-d trees and k-nearest search Part 1\n",
    "> \"Multidimensional search with k-d trees part 1\"\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: false\n",
    "- author: Alexandros Giavaras\n",
    "- categories: [multidimensional-search, kd-trees, k-nearest-search, algorithms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we take a look into searching in multidimensional data sets. Specifically, we consider k-d trees to facilitate searching in multidimensional data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-d trees and k-nearest search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest neighbor algorithm is one of the simplest algorithms performing both classification and regression modeling on data. Indeed, given a data set $D$ and a set of labels, the k-nearest neighbor algorithm does not explicitly train any particular mathematical model on $D$. Instead, it stores $D$ and every time a query comes in about the class of a point, $P$, the algorithm goes over the points in $D$ computes the $k$ nearest points to $P$ and uses a kind of majority vote in order to determine the class that $P$ should belong to. This is incredibly simple but this means that we need to compare $P$ with every point in $D$. Assuming that the metric we use is $O(1)$ time, the total time complexity is $O(|D|)$. However, this is just an ideal scenario. For a tuple with $N$ elements computing a distance requires $N$ operations. Hence, for one query point the complexity is $O(N|D|)$ and for $D$ query points we get quadratic behavior $O(N|D|^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can we improve this time complexity? The problem we face is that of searching in multidimensional data for the nearest neighbor(s) of a generic point $P$ that possibly is not in the dataset itself. K-d trees pose a way to solve this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting into the details, recall that a balanced binary search tree with $|D|$ nodes has complexity $O(log(|D|)$ and this is the motivation behind k-d trees. The only problem seems to be how to do the splitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A  k-d tree or a k-dimensional tree is a space partitioning data structure for organizing points in a k-dimensional space [1]. Specifically, a k-d tree is a binary search tree where every node in the tree represents a k-dimensional point [2]. We assume also that the coordinates of k-dimensional vector can be compared with each other. A non-leaf node in a k-d tree, implicitly generates a splitting hyperplane that divides the space into two parts, known as half-spaces [1]. Then all points to the left of the splitting hyperplane are represented by the left subtree of that node and points to the right of the hyperplane are represented by the right subtree [1]. The hyperplane direction is chosen by associating  every node in the tree with one of the k dimensions.  K-d trees were proposed by  Jon Louis Bentley in [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above may sound a bit technical and difficult to grasp. So let's go over a simple example. In order to be able to visualize what is happening we will confine ourselves to 2D space i.e. $k=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following points as shown in the figure below. The specific coordinates of the points is immaterial here. We want to construct a k-d tree out of these points. There are many different ways to construct k-d trees [1]. This follows from the fact that there are many possible ways we could choose in order to create axis-aligned splitting planes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, the canonical method of of k-d tree construction is as follows. As we move down the tree, we cycle through the axes used to select the splitting planes. For our example, we could have the root node to have an x-aligned plane, root's children would both have y-aligned planes, the the root's grandchildren would all have again x-aligned planes an so on. This process is illustrated in the images below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second point of concern is how to choose the points to insert. Again in the canonical method, points are inserted by selecting the median of the points being put into the subtree, with respect to their coordinates in the axis being used to create the splitting plane. Of course the latter assumes that we know all points in advance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method leads to a balanced k-d tree, in which each leaf node is approximately the same distance from the root [1]. Note. however,  that it is not required to select the median point; in this case there is no guarantee that the tree will be balanced [1]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid coding a complex $O(n)$ median-finding algorithm or using an $O(n log n)$ sort e.g. heapsort or mergesort to sort all $n$ points, a popular practice is to sort a fixed number of randomly selected points, and use the median of those points to serve as the splitting plane. In practice, this technique often results in nicely balanced trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is as brilliant as it is simple and stems from the considerations that led us\n",
    "this far in chapter 8. If we restrict to 2-D spaces, instead of splitting each region into\n",
    "four sub-regions for each point, we can perform a 2-way split, but alternating splits\n",
    "along vertical lines to split along horizontal lines.\n",
    "For each split, we partition the points in a region into two groups. Then, at the\n",
    "next split, when we choose the next pivot in each of the two sub-regions, we will use a\n",
    "perpendicular direction to do the next partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know conceptually how k-d trees work, let's turn our attention to k-nearest search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nearest neighbor search is the most interesting operation provided by k-d trees. Let's first consider the case of one target point $P_t$. In a brute-force scenario, we would have to compare each point in the data set to $P_t$ and keep track of the smallest one. This is similar to search an element in an unsorted array. However, a k-d tree has structural information we would like to use in order to perform the search faster. Specifically, each node covers one of the half-planes that the $k-$dimensional space was partitioned. If we manage to find the region that contains our point, we can narrow down the search. In fact, the point stored at the node will be among the closest points to $P_t$.\n",
    "\n",
    "\n",
    "As you can see, we check the distance of every intermediate point along the path,\n",
    "because any of them can be closer than the leaf. Even if intermediate points cover\n",
    "larger regions than leaves, inside each region we don’t have any indication of where\n",
    "dataset points might lie. If we refer to figure 9.20, if point A had been at ( 0,0 ) , the\n",
    "tree would have had the same shape, but P would have been closer to A (the root) than\n",
    "G (a leaf).\n",
    "\n",
    "But even that is not enough. After finding the region containing P , we can’t be sure\n",
    "that in neighboring regions there aren’t one or more points even closer than the clos-\n",
    "est point we found during this first traversal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know if a region intersects our current nearest neighbor’s hyper-\n",
    "sphere? That’s simple: each region stems by the partitioning created by split lines.\n",
    "When traversing a path, we go on one side of the split (the one on the same side as P ),\n",
    "but if the distance between a split line and P is less than the distance to our current\n",
    "nearest neighbor, then the hyper-sphere intersects the other partition as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we have a brief overview of k-d trees. These are binary search trees that facilitate searching in multidimensional data sets. In general k-d trees work well for low-to-medium-dimensional spaces but\n",
    "behaves poorly in high-dimensional spaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a $d-$dimensional space, with $d >> 10$ , under certain reasonable assumptions\n",
    "for data distribution, the nearest neighbor problem becomes ill-defined, 9\n",
    "because in high dimensions the ratio between the distance from a target to the\n",
    "nearest and farthest neighbor becomes almost 1. For instance, if points are\n",
    "equally spaced (for example, placed on a grid), then the 2d closest points to any\n",
    "centroid are all at the same distance from it. This means that the nearest neighbor of a point becomes ambiguously defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, k-d trees can also be used to boost the k-means clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to k-d trees is provided by <a href=\"https://en.wikipedia.org/wiki/Ball_tree\">ball-trees</a> but we will look into that in another post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a href=\"https://en.wikipedia.org/wiki/K-d_tree\">k-d tree</a>\n",
    "2. _Advanced algorithms and data structures._, Manning\n",
    "3. Jon Louis Bentley _Multidimensional binary search trees used for associative searching._ Communications of the ACM, 1975, Vol. 18, Issue 9, pp. 509-517"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
